{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfRGAymdTmBA"
      },
      "source": [
        "#**Predicting  Covid19 Deaths Using Deep Learning Models**\n",
        "\n",
        "**Collaborators**: Sebastian Tardieu, Shane Fabbri,  Binod Rimal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m5kjGJtGNbQ"
      },
      "source": [
        "# **Part I:  Data Creation and Exploration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_PmQ0-IjO74"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "import seaborn as sns\n",
        "# sns.set_theme(style=\"whitegrid\")\n",
        "#plt.style.use('ggplot')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9GvUIRynTIp",
        "outputId": "1b22bc3d-e89a-44e5-85da-29d05c83c232"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-Ztn89P6M1A"
      },
      "outputs": [],
      "source": [
        "data_dir= \"/content/drive/MyDrive/Covid_19\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#file = \"covid_19_data.csv\"\n",
        "file='owid-covid-data.csv'\n",
        "file = os.path.join(data_dir, file)\n",
        "df = pd.read_csv(file,index_col=[0])"
      ],
      "metadata": {
        "id": "Z7X9lK1OnfeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WAh56zESBi_",
        "outputId": "a5177b99-f6de-43d5-fc20-34b8316cab21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 358750 entries, AFG to ZWE\n",
            "Data columns (total 66 columns):\n",
            " #   Column                                      Non-Null Count   Dtype  \n",
            "---  ------                                      --------------   -----  \n",
            " 0   continent                                   341666 non-null  object \n",
            " 1   location                                    358750 non-null  object \n",
            " 2   date                                        358750 non-null  object \n",
            " 3   total_cases                                 320628 non-null  float64\n",
            " 4   new_cases                                   349032 non-null  float64\n",
            " 5   new_cases_smoothed                          347773 non-null  float64\n",
            " 6   total_deaths                                299086 non-null  float64\n",
            " 7   new_deaths                                  349089 non-null  float64\n",
            " 8   new_deaths_smoothed                         347859 non-null  float64\n",
            " 9   total_cases_per_million                     320628 non-null  float64\n",
            " 10  new_cases_per_million                       349032 non-null  float64\n",
            " 11  new_cases_smoothed_per_million              347773 non-null  float64\n",
            " 12  total_deaths_per_million                    299086 non-null  float64\n",
            " 13  new_deaths_per_million                      349089 non-null  float64\n",
            " 14  new_deaths_smoothed_per_million             347859 non-null  float64\n",
            " 15  reproduction_rate                           184817 non-null  float64\n",
            " 16  icu_patients                                37834 non-null   float64\n",
            " 17  icu_patients_per_million                    37834 non-null   float64\n",
            " 18  hosp_patients                               39227 non-null   float64\n",
            " 19  hosp_patients_per_million                   39227 non-null   float64\n",
            " 20  weekly_icu_admissions                       10312 non-null   float64\n",
            " 21  weekly_icu_admissions_per_million           10312 non-null   float64\n",
            " 22  weekly_hosp_admissions                      23498 non-null   float64\n",
            " 23  weekly_hosp_admissions_per_million          23498 non-null   float64\n",
            " 24  total_tests                                 79387 non-null   float64\n",
            " 25  new_tests                                   75403 non-null   float64\n",
            " 26  total_tests_per_thousand                    79387 non-null   float64\n",
            " 27  new_tests_per_thousand                      75403 non-null   float64\n",
            " 28  new_tests_smoothed                          103965 non-null  float64\n",
            " 29  new_tests_smoothed_per_thousand             103965 non-null  float64\n",
            " 30  positive_rate                               95927 non-null   float64\n",
            " 31  tests_per_case                              94348 non-null   float64\n",
            " 32  tests_units                                 106788 non-null  object \n",
            " 33  total_vaccinations                          80112 non-null   float64\n",
            " 34  people_vaccinated                           76677 non-null   float64\n",
            " 35  people_fully_vaccinated                     73371 non-null   float64\n",
            " 36  total_boosters                              48311 non-null   float64\n",
            " 37  new_vaccinations                            66074 non-null   float64\n",
            " 38  new_vaccinations_smoothed                   183988 non-null  float64\n",
            " 39  total_vaccinations_per_hundred              80112 non-null   float64\n",
            " 40  people_vaccinated_per_hundred               76677 non-null   float64\n",
            " 41  people_fully_vaccinated_per_hundred         73371 non-null   float64\n",
            " 42  total_boosters_per_hundred                  48311 non-null   float64\n",
            " 43  new_vaccinations_smoothed_per_million       183988 non-null  float64\n",
            " 44  new_people_vaccinated_smoothed              183738 non-null  float64\n",
            " 45  new_people_vaccinated_smoothed_per_hundred  183738 non-null  float64\n",
            " 46  stringency_index                            197651 non-null  float64\n",
            " 47  population_density                          304619 non-null  float64\n",
            " 48  median_age                                  283248 non-null  float64\n",
            " 49  aged_65_older                               273379 non-null  float64\n",
            " 50  aged_70_older                               280408 non-null  float64\n",
            " 51  gdp_per_capita                              277639 non-null  float64\n",
            " 52  extreme_poverty                             178962 non-null  float64\n",
            " 53  cardiovasc_death_rate                       278298 non-null  float64\n",
            " 54  diabetes_prevalence                         292464 non-null  float64\n",
            " 55  female_smokers                              208790 non-null  float64\n",
            " 56  male_smokers                                205950 non-null  float64\n",
            " 57  handwashing_facilities                      136334 non-null  float64\n",
            " 58  hospital_beds_per_thousand                  245710 non-null  float64\n",
            " 59  life_expectancy                             330108 non-null  float64\n",
            " 60  human_development_index                     269739 non-null  float64\n",
            " 61  population                                  358750 non-null  float64\n",
            " 62  excess_mortality_cumulative_absolute        12211 non-null   float64\n",
            " 63  excess_mortality_cumulative                 12211 non-null   float64\n",
            " 64  excess_mortality                            12211 non-null   float64\n",
            " 65  excess_mortality_cumulative_per_million     12211 non-null   float64\n",
            "dtypes: float64(62), object(4)\n",
            "memory usage: 183.4+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#selected_var=['date','new_deaths','new_cases',  'icu_patients', 'hosp_patients','positive_rate','total_vaccinations']\n",
        "selected_var=['date','new_deaths_smoothed','new_cases_smoothed',  'icu_patients', 'hosp_patients','positive_rate','total_vaccinations']\n",
        "df2=df[selected_var]"
      ],
      "metadata": {
        "id": "nC-ZodKrKjDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p31VqWprnhel",
        "outputId": "721c8d2b-d079-4441-82e4-f740315727ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "date                        0\n",
              "new_deaths_smoothed     10891\n",
              "new_cases_smoothed      10977\n",
              "icu_patients           320916\n",
              "hosp_patients          319523\n",
              "positive_rate          262823\n",
              "total_vaccinations     278638\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2=df2.fillna(method='ffill')"
      ],
      "metadata": {
        "id": "Qe907Y0LJckN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LhCtOFTUNob",
        "outputId": "a3e19a6f-48e4-4f07-ce77-e565b9b0b089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "date                       0\n",
              "new_deaths_smoothed        5\n",
              "new_cases_smoothed         5\n",
              "icu_patients            4456\n",
              "hosp_patients          18564\n",
              "positive_rate            764\n",
              "total_vaccinations       416\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2=df2.fillna(method='bfill')"
      ],
      "metadata": {
        "id": "F0dd4tJHURtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eb2dH-pUVZH",
        "outputId": "9ae93e35-ff00-44e8-e1c2-1b2ee77d465d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "date                   0\n",
              "new_deaths_smoothed    0\n",
              "new_cases_smoothed     0\n",
              "icu_patients           0\n",
              "hosp_patients          0\n",
              "positive_rate          0\n",
              "total_vaccinations     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "MkeSk1s8Mja_",
        "outputId": "838817d5-550a-4cb0-fdc5-13904cfdcf73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                date  new_deaths_smoothed  new_cases_smoothed  icu_patients  \\\n",
              "iso_code                                                                      \n",
              "AFG       2020-01-03                  0.0                 0.0          62.0   \n",
              "AFG       2020-01-04                  0.0                 0.0          62.0   \n",
              "AFG       2020-01-05                  0.0                 0.0          62.0   \n",
              "AFG       2020-01-06                  0.0                 0.0          62.0   \n",
              "AFG       2020-01-07                  0.0                 0.0          62.0   \n",
              "\n",
              "          hosp_patients  positive_rate  total_vaccinations  \n",
              "iso_code                                                    \n",
              "AFG               426.0         0.4855                 0.0  \n",
              "AFG               426.0         0.4855                 0.0  \n",
              "AFG               426.0         0.4855                 0.0  \n",
              "AFG               426.0         0.4855                 0.0  \n",
              "AFG               426.0         0.4855                 0.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-26b90844-4710-45aa-a2d7-a0ba9b2ac6cd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>new_deaths_smoothed</th>\n",
              "      <th>new_cases_smoothed</th>\n",
              "      <th>icu_patients</th>\n",
              "      <th>hosp_patients</th>\n",
              "      <th>positive_rate</th>\n",
              "      <th>total_vaccinations</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>iso_code</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>AFG</th>\n",
              "      <td>2020-01-03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>426.0</td>\n",
              "      <td>0.4855</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AFG</th>\n",
              "      <td>2020-01-04</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>426.0</td>\n",
              "      <td>0.4855</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AFG</th>\n",
              "      <td>2020-01-05</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>426.0</td>\n",
              "      <td>0.4855</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AFG</th>\n",
              "      <td>2020-01-06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>426.0</td>\n",
              "      <td>0.4855</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AFG</th>\n",
              "      <td>2020-01-07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>426.0</td>\n",
              "      <td>0.4855</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-26b90844-4710-45aa-a2d7-a0ba9b2ac6cd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-26b90844-4710-45aa-a2d7-a0ba9b2ac6cd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-26b90844-4710-45aa-a2d7-a0ba9b2ac6cd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cd0f3cd5-6c57-4027-9725-2e82c793d283\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cd0f3cd5-6c57-4027-9725-2e82c793d283')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cd0f3cd5-6c57-4027-9725-2e82c793d283 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J1qU9PI4MiRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['date']=pd.to_datetime(df2['date'])\n",
        "df2.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "tf_j_PZ8qy64",
        "outputId": "63673eb6-c77a-4d3f-8531-1f873e6398cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               date  new_deaths_smoothed  new_cases_smoothed  icu_patients  \\\n",
              "iso_code                                                                     \n",
              "AFG      2020-01-03                  0.0                 0.0          62.0   \n",
              "AFG      2020-01-04                  0.0                 0.0          62.0   \n",
              "AFG      2020-01-05                  0.0                 0.0          62.0   \n",
              "AFG      2020-01-06                  0.0                 0.0          62.0   \n",
              "AFG      2020-01-07                  0.0                 0.0          62.0   \n",
              "\n",
              "          hosp_patients  positive_rate  total_vaccinations  \n",
              "iso_code                                                    \n",
              "AFG               426.0         0.4855                 0.0  \n",
              "AFG               426.0         0.4855                 0.0  \n",
              "AFG               426.0         0.4855                 0.0  \n",
              "AFG               426.0         0.4855                 0.0  \n",
              "AFG               426.0         0.4855                 0.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d2fccc9e-da0c-4efc-8b2d-1f057c4608ae\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>new_deaths_smoothed</th>\n",
              "      <th>new_cases_smoothed</th>\n",
              "      <th>icu_patients</th>\n",
              "      <th>hosp_patients</th>\n",
              "      <th>positive_rate</th>\n",
              "      <th>total_vaccinations</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>iso_code</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>AFG</th>\n",
              "      <td>2020-01-03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>426.0</td>\n",
              "      <td>0.4855</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AFG</th>\n",
              "      <td>2020-01-04</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>426.0</td>\n",
              "      <td>0.4855</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AFG</th>\n",
              "      <td>2020-01-05</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>426.0</td>\n",
              "      <td>0.4855</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AFG</th>\n",
              "      <td>2020-01-06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>426.0</td>\n",
              "      <td>0.4855</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AFG</th>\n",
              "      <td>2020-01-07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>426.0</td>\n",
              "      <td>0.4855</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d2fccc9e-da0c-4efc-8b2d-1f057c4608ae')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d2fccc9e-da0c-4efc-8b2d-1f057c4608ae button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d2fccc9e-da0c-4efc-8b2d-1f057c4608ae');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1d84bb60-9727-43e2-a6e1-9299a99bb096\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1d84bb60-9727-43e2-a6e1-9299a99bb096')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1d84bb60-9727-43e2-a6e1-9299a99bb096 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "23E3lP6RJbqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = df2.groupby('date')['new_deaths_smoothed', 'new_cases_smoothed', 'icu_patients', 'hosp_patients',  'total_vaccinations'].sum()\n",
        "data.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "LnPbs2C4nIeN",
        "outputId": "b6264958-17d4-4979-f6c7-c2f6cf9c75a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            new_deaths_smoothed  new_cases_smoothed  icu_patients  \\\n",
              "date                                                                \n",
              "2023-11-23               62.427           10671.858        3511.0   \n",
              "2023-11-24               62.427           10671.858        3415.0   \n",
              "2023-11-25               62.427           10671.858        3389.0   \n",
              "2023-11-26               61.856           10374.715        3387.0   \n",
              "2023-11-27               61.856           10374.715        2835.0   \n",
              "\n",
              "            hosp_patients  total_vaccinations  \n",
              "date                                           \n",
              "2023-11-23        73550.0        4.328761e+10  \n",
              "2023-11-24        60207.0        4.277682e+10  \n",
              "2023-11-25        59568.0        4.273701e+10  \n",
              "2023-11-26        59580.0        4.273242e+10  \n",
              "2023-11-27        45505.0        4.167368e+10  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-75251e33-7348-4e41-9896-a5b1b9253dc0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>new_deaths_smoothed</th>\n",
              "      <th>new_cases_smoothed</th>\n",
              "      <th>icu_patients</th>\n",
              "      <th>hosp_patients</th>\n",
              "      <th>total_vaccinations</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2023-11-23</th>\n",
              "      <td>62.427</td>\n",
              "      <td>10671.858</td>\n",
              "      <td>3511.0</td>\n",
              "      <td>73550.0</td>\n",
              "      <td>4.328761e+10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-11-24</th>\n",
              "      <td>62.427</td>\n",
              "      <td>10671.858</td>\n",
              "      <td>3415.0</td>\n",
              "      <td>60207.0</td>\n",
              "      <td>4.277682e+10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-11-25</th>\n",
              "      <td>62.427</td>\n",
              "      <td>10671.858</td>\n",
              "      <td>3389.0</td>\n",
              "      <td>59568.0</td>\n",
              "      <td>4.273701e+10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-11-26</th>\n",
              "      <td>61.856</td>\n",
              "      <td>10374.715</td>\n",
              "      <td>3387.0</td>\n",
              "      <td>59580.0</td>\n",
              "      <td>4.273242e+10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-11-27</th>\n",
              "      <td>61.856</td>\n",
              "      <td>10374.715</td>\n",
              "      <td>2835.0</td>\n",
              "      <td>45505.0</td>\n",
              "      <td>4.167368e+10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-75251e33-7348-4e41-9896-a5b1b9253dc0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-75251e33-7348-4e41-9896-a5b1b9253dc0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-75251e33-7348-4e41-9896-a5b1b9253dc0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-46d5640d-6fb9-484a-9180-1d45d0a0d634\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-46d5640d-6fb9-484a-9180-1d45d0a0d634')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-46d5640d-6fb9-484a-9180-1d45d0a0d634 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['positive_rate'] = df2.groupby('date')['positive_rate'].mean()\n",
        "\n",
        "data.head(50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tBKWvhw-OPAB",
        "outputId": "1b99a376-a9b1-4d98-c327-9c88c56427b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            new_deaths_smoothed  new_cases_smoothed  icu_patients  \\\n",
              "date                                                                \n",
              "2020-01-01                0.000               0.000           2.0   \n",
              "2020-01-02                0.000               0.000           2.0   \n",
              "2020-01-03              376.284           30210.715       27784.0   \n",
              "2020-01-04              376.284           30210.715       27784.0   \n",
              "2020-01-05              376.284           30210.715       27784.0   \n",
              "2020-01-06              376.284           30210.715       27784.0   \n",
              "2020-01-07              376.284           30210.715       27784.0   \n",
              "2020-01-08                2.145               4.143       27784.0   \n",
              "2020-01-09                2.145               4.286       27784.0   \n",
              "2020-01-10                2.145               4.286       27784.0   \n",
              "2020-01-11                2.145               2.287       27784.0   \n",
              "2020-01-12                0.572              25.716       27784.0   \n",
              "2020-01-13                0.572              24.572       27784.0   \n",
              "2020-01-14                0.572              27.428       27784.0   \n",
              "2020-01-15                0.572              26.856       27784.0   \n",
              "2020-01-16                0.572              26.856       27796.0   \n",
              "2020-01-17                1.144              29.144       27796.0   \n",
              "2020-01-18                1.144              31.429       27796.0   \n",
              "2020-01-19                1.144              53.430       27796.0   \n",
              "2020-01-20                1.716              97.430       27796.0   \n",
              "2020-01-21                2.856             147.715       27796.0   \n",
              "2020-01-22                9.144             232.286       27796.0   \n",
              "2020-01-23                9.716             308.286       27796.0   \n",
              "2020-01-24               13.716             460.858       27796.0   \n",
              "2020-01-25               22.856             728.859       27796.0   \n",
              "2020-01-26               32.429            1086.998       27796.0   \n",
              "2020-01-27               46.716            1492.855       27796.0   \n",
              "2020-01-28               73.573            2456.001       27796.0   \n",
              "2020-01-29               68.429            3215.000       27796.0   \n",
              "2020-01-30               89.573            4143.145       27796.0   \n",
              "2020-01-31              109.001            5140.145       27896.0   \n",
              "2020-02-01              126.714            6077.288       27896.0   \n",
              "2020-02-02              177.571            7173.429       27896.0   \n",
              "2020-02-03              199.286            8342.573       27896.0   \n",
              "2020-02-04              209.573            9184.856       27896.0   \n",
              "2020-02-05              249.573           10590.861       27896.0   \n",
              "2020-02-06              267.858           11711.288       27896.0   \n",
              "2020-02-07              292.430           12365.287       27896.0   \n",
              "2020-02-08              315.286           13098.287       27896.0   \n",
              "2020-02-09              311.428           13143.859       27896.0   \n",
              "2020-02-10              336.000           13233.574       27896.0   \n",
              "2020-02-11              354.856           12793.289       27896.0   \n",
              "2020-02-12              316.572           11705.429       27896.0   \n",
              "2020-02-13              421.144           18249.430       27896.0   \n",
              "2020-02-14              459.428           18768.857       27896.0   \n",
              "2020-02-15              490.856           18349.285       27896.0   \n",
              "2020-02-16              493.859           17973.999       27896.0   \n",
              "2020-02-17              488.715           17442.000       27896.0   \n",
              "2020-02-18              511.001           17114.000       27896.0   \n",
              "2020-02-19              572.715           16980.144       27896.0   \n",
              "\n",
              "            hosp_patients  total_vaccinations  positive_rate  \n",
              "date                                                          \n",
              "2020-01-01          618.0        2.752524e+06       0.002750  \n",
              "2020-01-02          618.0        2.752524e+06       0.002750  \n",
              "2020-01-03       703779.0        5.750226e+10       0.153715  \n",
              "2020-01-04       703779.0        5.750226e+10       0.153715  \n",
              "2020-01-05       703779.0        5.750226e+10       0.153715  \n",
              "2020-01-06       701277.0        5.750226e+10       0.153715  \n",
              "2020-01-07       701277.0        5.750226e+10       0.154376  \n",
              "2020-01-08       701277.0        5.750226e+10       0.154359  \n",
              "2020-01-09       701277.0        5.750226e+10       0.154262  \n",
              "2020-01-10       701277.0        5.750226e+10       0.154115  \n",
              "2020-01-11       701277.0        5.750226e+10       0.154111  \n",
              "2020-01-12       701277.0        5.750226e+10       0.154002  \n",
              "2020-01-13       701277.0        5.750226e+10       0.153896  \n",
              "2020-01-14       701277.0        5.750226e+10       0.153847  \n",
              "2020-01-15       701277.0        5.750226e+10       0.153798  \n",
              "2020-01-16       701659.0        5.750735e+10       0.153197  \n",
              "2020-01-17       701659.0        5.750735e+10       0.153262  \n",
              "2020-01-18       701659.0        5.750735e+10       0.153290  \n",
              "2020-01-19       701659.0        5.750735e+10       0.153541  \n",
              "2020-01-20       701659.0        5.750735e+10       0.153565  \n",
              "2020-01-21       701659.0        5.750735e+10       0.153569  \n",
              "2020-01-22       701659.0        5.750735e+10       0.153990  \n",
              "2020-01-23       701659.0        5.750735e+10       0.154006  \n",
              "2020-01-24       701659.0        5.750735e+10       0.154006  \n",
              "2020-01-25       701659.0        5.750735e+10       0.154006  \n",
              "2020-01-26       701659.0        5.750735e+10       0.154067  \n",
              "2020-01-27       701659.0        5.750735e+10       0.153836  \n",
              "2020-01-28       701659.0        5.750735e+10       0.153582  \n",
              "2020-01-29       701659.0        5.750735e+10       0.153573  \n",
              "2020-01-30       701659.0        5.750735e+10       0.153562  \n",
              "2020-01-31       714808.0        5.752425e+10       0.152950  \n",
              "2020-02-01       714808.0        5.752425e+10       0.152577  \n",
              "2020-02-02       714808.0        5.752425e+10       0.152639  \n",
              "2020-02-03       714808.0        5.752425e+10       0.151469  \n",
              "2020-02-04       714808.0        5.752425e+10       0.151564  \n",
              "2020-02-05       714808.0        5.752425e+10       0.149124  \n",
              "2020-02-06       714808.0        5.752425e+10       0.149131  \n",
              "2020-02-07       714808.0        5.752425e+10       0.149390  \n",
              "2020-02-08       714808.0        5.752425e+10       0.149429  \n",
              "2020-02-09       714808.0        5.752425e+10       0.149246  \n",
              "2020-02-10       714808.0        5.752425e+10       0.151008  \n",
              "2020-02-11       714808.0        5.752425e+10       0.150342  \n",
              "2020-02-12       714808.0        5.752425e+10       0.149102  \n",
              "2020-02-13       714808.0        5.752425e+10       0.149481  \n",
              "2020-02-14       714808.0        5.752425e+10       0.149483  \n",
              "2020-02-15       714808.0        5.752425e+10       0.148987  \n",
              "2020-02-16       714808.0        5.752425e+10       0.148936  \n",
              "2020-02-17       714808.0        5.752425e+10       0.148805  \n",
              "2020-02-18       714808.0        5.752425e+10       0.149076  \n",
              "2020-02-19       714808.0        5.752425e+10       0.149170  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e9a8315a-726b-47ed-adab-16645693859c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>new_deaths_smoothed</th>\n",
              "      <th>new_cases_smoothed</th>\n",
              "      <th>icu_patients</th>\n",
              "      <th>hosp_patients</th>\n",
              "      <th>total_vaccinations</th>\n",
              "      <th>positive_rate</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2020-01-01</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>618.0</td>\n",
              "      <td>2.752524e+06</td>\n",
              "      <td>0.002750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-02</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>618.0</td>\n",
              "      <td>2.752524e+06</td>\n",
              "      <td>0.002750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-03</th>\n",
              "      <td>376.284</td>\n",
              "      <td>30210.715</td>\n",
              "      <td>27784.0</td>\n",
              "      <td>703779.0</td>\n",
              "      <td>5.750226e+10</td>\n",
              "      <td>0.153715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-04</th>\n",
              "      <td>376.284</td>\n",
              "      <td>30210.715</td>\n",
              "      <td>27784.0</td>\n",
              "      <td>703779.0</td>\n",
              "      <td>5.750226e+10</td>\n",
              "      <td>0.153715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-05</th>\n",
              "      <td>376.284</td>\n",
              "      <td>30210.715</td>\n",
              "      <td>27784.0</td>\n",
              "      <td>703779.0</td>\n",
              "      <td>5.750226e+10</td>\n",
              "      <td>0.153715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-06</th>\n",
              "      <td>376.284</td>\n",
              "      <td>30210.715</td>\n",
              "      <td>27784.0</td>\n",
              "      <td>701277.0</td>\n",
              "      <td>5.750226e+10</td>\n",
              "      <td>0.153715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-07</th>\n",
              "      <td>376.284</td>\n",
              "      <td>30210.715</td>\n",
              "      <td>27784.0</td>\n",
              "      <td>701277.0</td>\n",
              "      <td>5.750226e+10</td>\n",
              "      <td>0.154376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-08</th>\n",
              "      <td>2.145</td>\n",
              "      <td>4.143</td>\n",
              "      <td>27784.0</td>\n",
              "      <td>701277.0</td>\n",
              "      <td>5.750226e+10</td>\n",
              "      <td>0.154359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-09</th>\n",
              "      <td>2.145</td>\n",
              "      <td>4.286</td>\n",
              "      <td>27784.0</td>\n",
              "      <td>701277.0</td>\n",
              "      <td>5.750226e+10</td>\n",
              "      <td>0.154262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-10</th>\n",
              "      <td>2.145</td>\n",
              "      <td>4.286</td>\n",
              "      <td>27784.0</td>\n",
              "      <td>701277.0</td>\n",
              "      <td>5.750226e+10</td>\n",
              "      <td>0.154115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-11</th>\n",
              "      <td>2.145</td>\n",
              "      <td>2.287</td>\n",
              "      <td>27784.0</td>\n",
              "      <td>701277.0</td>\n",
              "      <td>5.750226e+10</td>\n",
              "      <td>0.154111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-12</th>\n",
              "      <td>0.572</td>\n",
              "      <td>25.716</td>\n",
              "      <td>27784.0</td>\n",
              "      <td>701277.0</td>\n",
              "      <td>5.750226e+10</td>\n",
              "      <td>0.154002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-13</th>\n",
              "      <td>0.572</td>\n",
              "      <td>24.572</td>\n",
              "      <td>27784.0</td>\n",
              "      <td>701277.0</td>\n",
              "      <td>5.750226e+10</td>\n",
              "      <td>0.153896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-14</th>\n",
              "      <td>0.572</td>\n",
              "      <td>27.428</td>\n",
              "      <td>27784.0</td>\n",
              "      <td>701277.0</td>\n",
              "      <td>5.750226e+10</td>\n",
              "      <td>0.153847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-15</th>\n",
              "      <td>0.572</td>\n",
              "      <td>26.856</td>\n",
              "      <td>27784.0</td>\n",
              "      <td>701277.0</td>\n",
              "      <td>5.750226e+10</td>\n",
              "      <td>0.153798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-16</th>\n",
              "      <td>0.572</td>\n",
              "      <td>26.856</td>\n",
              "      <td>27796.0</td>\n",
              "      <td>701659.0</td>\n",
              "      <td>5.750735e+10</td>\n",
              "      <td>0.153197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-17</th>\n",
              "      <td>1.144</td>\n",
              "      <td>29.144</td>\n",
              "      <td>27796.0</td>\n",
              "      <td>701659.0</td>\n",
              "      <td>5.750735e+10</td>\n",
              "      <td>0.153262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-18</th>\n",
              "      <td>1.144</td>\n",
              "      <td>31.429</td>\n",
              "      <td>27796.0</td>\n",
              "      <td>701659.0</td>\n",
              "      <td>5.750735e+10</td>\n",
              "      <td>0.153290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-19</th>\n",
              "      <td>1.144</td>\n",
              "      <td>53.430</td>\n",
              "      <td>27796.0</td>\n",
              "      <td>701659.0</td>\n",
              "      <td>5.750735e+10</td>\n",
              "      <td>0.153541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-20</th>\n",
              "      <td>1.716</td>\n",
              "      <td>97.430</td>\n",
              "      <td>27796.0</td>\n",
              "      <td>701659.0</td>\n",
              "      <td>5.750735e+10</td>\n",
              "      <td>0.153565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-21</th>\n",
              "      <td>2.856</td>\n",
              "      <td>147.715</td>\n",
              "      <td>27796.0</td>\n",
              "      <td>701659.0</td>\n",
              "      <td>5.750735e+10</td>\n",
              "      <td>0.153569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-22</th>\n",
              "      <td>9.144</td>\n",
              "      <td>232.286</td>\n",
              "      <td>27796.0</td>\n",
              "      <td>701659.0</td>\n",
              "      <td>5.750735e+10</td>\n",
              "      <td>0.153990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-23</th>\n",
              "      <td>9.716</td>\n",
              "      <td>308.286</td>\n",
              "      <td>27796.0</td>\n",
              "      <td>701659.0</td>\n",
              "      <td>5.750735e+10</td>\n",
              "      <td>0.154006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-24</th>\n",
              "      <td>13.716</td>\n",
              "      <td>460.858</td>\n",
              "      <td>27796.0</td>\n",
              "      <td>701659.0</td>\n",
              "      <td>5.750735e+10</td>\n",
              "      <td>0.154006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-25</th>\n",
              "      <td>22.856</td>\n",
              "      <td>728.859</td>\n",
              "      <td>27796.0</td>\n",
              "      <td>701659.0</td>\n",
              "      <td>5.750735e+10</td>\n",
              "      <td>0.154006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-26</th>\n",
              "      <td>32.429</td>\n",
              "      <td>1086.998</td>\n",
              "      <td>27796.0</td>\n",
              "      <td>701659.0</td>\n",
              "      <td>5.750735e+10</td>\n",
              "      <td>0.154067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-27</th>\n",
              "      <td>46.716</td>\n",
              "      <td>1492.855</td>\n",
              "      <td>27796.0</td>\n",
              "      <td>701659.0</td>\n",
              "      <td>5.750735e+10</td>\n",
              "      <td>0.153836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-28</th>\n",
              "      <td>73.573</td>\n",
              "      <td>2456.001</td>\n",
              "      <td>27796.0</td>\n",
              "      <td>701659.0</td>\n",
              "      <td>5.750735e+10</td>\n",
              "      <td>0.153582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-29</th>\n",
              "      <td>68.429</td>\n",
              "      <td>3215.000</td>\n",
              "      <td>27796.0</td>\n",
              "      <td>701659.0</td>\n",
              "      <td>5.750735e+10</td>\n",
              "      <td>0.153573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-30</th>\n",
              "      <td>89.573</td>\n",
              "      <td>4143.145</td>\n",
              "      <td>27796.0</td>\n",
              "      <td>701659.0</td>\n",
              "      <td>5.750735e+10</td>\n",
              "      <td>0.153562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-31</th>\n",
              "      <td>109.001</td>\n",
              "      <td>5140.145</td>\n",
              "      <td>27896.0</td>\n",
              "      <td>714808.0</td>\n",
              "      <td>5.752425e+10</td>\n",
              "      <td>0.152950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-02-01</th>\n",
              "      <td>126.714</td>\n",
              "      <td>6077.288</td>\n",
              "      <td>27896.0</td>\n",
              "      <td>714808.0</td>\n",
              "      <td>5.752425e+10</td>\n",
              "      <td>0.152577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-02-02</th>\n",
              "      <td>177.571</td>\n",
              "      <td>7173.429</td>\n",
              "      <td>27896.0</td>\n",
              "      <td>714808.0</td>\n",
              "      <td>5.752425e+10</td>\n",
              "      <td>0.152639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-02-03</th>\n",
              "      <td>199.286</td>\n",
              "      <td>8342.573</td>\n",
              "      <td>27896.0</td>\n",
              "      <td>714808.0</td>\n",
              "      <td>5.752425e+10</td>\n",
              "      <td>0.151469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-02-04</th>\n",
              "      <td>209.573</td>\n",
              "      <td>9184.856</td>\n",
              "      <td>27896.0</td>\n",
              "      <td>714808.0</td>\n",
              "      <td>5.752425e+10</td>\n",
              "      <td>0.151564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-02-05</th>\n",
              "      <td>249.573</td>\n",
              "      <td>10590.861</td>\n",
              "      <td>27896.0</td>\n",
              "      <td>714808.0</td>\n",
              "      <td>5.752425e+10</td>\n",
              "      <td>0.149124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-02-06</th>\n",
              "      <td>267.858</td>\n",
              "      <td>11711.288</td>\n",
              "      <td>27896.0</td>\n",
              "      <td>714808.0</td>\n",
              "      <td>5.752425e+10</td>\n",
              "      <td>0.149131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-02-07</th>\n",
              "      <td>292.430</td>\n",
              "      <td>12365.287</td>\n",
              "      <td>27896.0</td>\n",
              "      <td>714808.0</td>\n",
              "      <td>5.752425e+10</td>\n",
              "      <td>0.149390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-02-08</th>\n",
              "      <td>315.286</td>\n",
              "      <td>13098.287</td>\n",
              "      <td>27896.0</td>\n",
              "      <td>714808.0</td>\n",
              "      <td>5.752425e+10</td>\n",
              "      <td>0.149429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-02-09</th>\n",
              "      <td>311.428</td>\n",
              "      <td>13143.859</td>\n",
              "      <td>27896.0</td>\n",
              "      <td>714808.0</td>\n",
              "      <td>5.752425e+10</td>\n",
              "      <td>0.149246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-02-10</th>\n",
              "      <td>336.000</td>\n",
              "      <td>13233.574</td>\n",
              "      <td>27896.0</td>\n",
              "      <td>714808.0</td>\n",
              "      <td>5.752425e+10</td>\n",
              "      <td>0.151008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-02-11</th>\n",
              "      <td>354.856</td>\n",
              "      <td>12793.289</td>\n",
              "      <td>27896.0</td>\n",
              "      <td>714808.0</td>\n",
              "      <td>5.752425e+10</td>\n",
              "      <td>0.150342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-02-12</th>\n",
              "      <td>316.572</td>\n",
              "      <td>11705.429</td>\n",
              "      <td>27896.0</td>\n",
              "      <td>714808.0</td>\n",
              "      <td>5.752425e+10</td>\n",
              "      <td>0.149102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-02-13</th>\n",
              "      <td>421.144</td>\n",
              "      <td>18249.430</td>\n",
              "      <td>27896.0</td>\n",
              "      <td>714808.0</td>\n",
              "      <td>5.752425e+10</td>\n",
              "      <td>0.149481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-02-14</th>\n",
              "      <td>459.428</td>\n",
              "      <td>18768.857</td>\n",
              "      <td>27896.0</td>\n",
              "      <td>714808.0</td>\n",
              "      <td>5.752425e+10</td>\n",
              "      <td>0.149483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-02-15</th>\n",
              "      <td>490.856</td>\n",
              "      <td>18349.285</td>\n",
              "      <td>27896.0</td>\n",
              "      <td>714808.0</td>\n",
              "      <td>5.752425e+10</td>\n",
              "      <td>0.148987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-02-16</th>\n",
              "      <td>493.859</td>\n",
              "      <td>17973.999</td>\n",
              "      <td>27896.0</td>\n",
              "      <td>714808.0</td>\n",
              "      <td>5.752425e+10</td>\n",
              "      <td>0.148936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-02-17</th>\n",
              "      <td>488.715</td>\n",
              "      <td>17442.000</td>\n",
              "      <td>27896.0</td>\n",
              "      <td>714808.0</td>\n",
              "      <td>5.752425e+10</td>\n",
              "      <td>0.148805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-02-18</th>\n",
              "      <td>511.001</td>\n",
              "      <td>17114.000</td>\n",
              "      <td>27896.0</td>\n",
              "      <td>714808.0</td>\n",
              "      <td>5.752425e+10</td>\n",
              "      <td>0.149076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-02-19</th>\n",
              "      <td>572.715</td>\n",
              "      <td>16980.144</td>\n",
              "      <td>27896.0</td>\n",
              "      <td>714808.0</td>\n",
              "      <td>5.752425e+10</td>\n",
              "      <td>0.149170</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e9a8315a-726b-47ed-adab-16645693859c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e9a8315a-726b-47ed-adab-16645693859c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e9a8315a-726b-47ed-adab-16645693859c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7b77b742-81a9-4984-bff8-b6b2879e2645\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7b77b742-81a9-4984-bff8-b6b2879e2645')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7b77b742-81a9-4984-bff8-b6b2879e2645 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=data['2020-01-22':'2022-12-31']\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "dlfRRcKzVPN7",
        "outputId": "384d142d-42a3-4242-adb1-ae691dfa4274"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            new_deaths_smoothed  new_cases_smoothed  icu_patients  \\\n",
              "date                                                                \n",
              "2020-01-22                9.144             232.286       27796.0   \n",
              "2020-01-23                9.716             308.286       27796.0   \n",
              "2020-01-24               13.716             460.858       27796.0   \n",
              "2020-01-25               22.856             728.859       27796.0   \n",
              "2020-01-26               32.429            1086.998       27796.0   \n",
              "\n",
              "            hosp_patients  total_vaccinations  positive_rate  \n",
              "date                                                          \n",
              "2020-01-22       701659.0        5.750735e+10       0.153990  \n",
              "2020-01-23       701659.0        5.750735e+10       0.154006  \n",
              "2020-01-24       701659.0        5.750735e+10       0.154006  \n",
              "2020-01-25       701659.0        5.750735e+10       0.154006  \n",
              "2020-01-26       701659.0        5.750735e+10       0.154067  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9fc8c897-b9f1-4176-b956-0a500de8ef1a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>new_deaths_smoothed</th>\n",
              "      <th>new_cases_smoothed</th>\n",
              "      <th>icu_patients</th>\n",
              "      <th>hosp_patients</th>\n",
              "      <th>total_vaccinations</th>\n",
              "      <th>positive_rate</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2020-01-22</th>\n",
              "      <td>9.144</td>\n",
              "      <td>232.286</td>\n",
              "      <td>27796.0</td>\n",
              "      <td>701659.0</td>\n",
              "      <td>5.750735e+10</td>\n",
              "      <td>0.153990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-23</th>\n",
              "      <td>9.716</td>\n",
              "      <td>308.286</td>\n",
              "      <td>27796.0</td>\n",
              "      <td>701659.0</td>\n",
              "      <td>5.750735e+10</td>\n",
              "      <td>0.154006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-24</th>\n",
              "      <td>13.716</td>\n",
              "      <td>460.858</td>\n",
              "      <td>27796.0</td>\n",
              "      <td>701659.0</td>\n",
              "      <td>5.750735e+10</td>\n",
              "      <td>0.154006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-25</th>\n",
              "      <td>22.856</td>\n",
              "      <td>728.859</td>\n",
              "      <td>27796.0</td>\n",
              "      <td>701659.0</td>\n",
              "      <td>5.750735e+10</td>\n",
              "      <td>0.154006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-26</th>\n",
              "      <td>32.429</td>\n",
              "      <td>1086.998</td>\n",
              "      <td>27796.0</td>\n",
              "      <td>701659.0</td>\n",
              "      <td>5.750735e+10</td>\n",
              "      <td>0.154067</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9fc8c897-b9f1-4176-b956-0a500de8ef1a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9fc8c897-b9f1-4176-b956-0a500de8ef1a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9fc8c897-b9f1-4176-b956-0a500de8ef1a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-93d605a8-caf2-4873-a9d5-1b87fdf19c18\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-93d605a8-caf2-4873-a9d5-1b87fdf19c18')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-93d605a8-caf2-4873-a9d5-1b87fdf19c18 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxtnCCRrqChk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "efb944a5-f4cd-4ff5-f49f-b27982322019"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            new_deaths_smoothed  new_cases_smoothed  icu_patients  \\\n",
              "2020-01-22                9.144             232.286       27796.0   \n",
              "2020-01-23                9.716             308.286       27796.0   \n",
              "2020-01-24               13.716             460.858       27796.0   \n",
              "2020-01-25               22.856             728.859       27796.0   \n",
              "2020-01-26               32.429            1086.998       27796.0   \n",
              "\n",
              "            hosp_patients  total_vaccinations  positive_rate  \n",
              "2020-01-22       701659.0        5.750735e+10       0.153990  \n",
              "2020-01-23       701659.0        5.750735e+10       0.154006  \n",
              "2020-01-24       701659.0        5.750735e+10       0.154006  \n",
              "2020-01-25       701659.0        5.750735e+10       0.154006  \n",
              "2020-01-26       701659.0        5.750735e+10       0.154067  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d21d08b0-28dd-4e30-b31b-8c2c059d5217\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>new_deaths_smoothed</th>\n",
              "      <th>new_cases_smoothed</th>\n",
              "      <th>icu_patients</th>\n",
              "      <th>hosp_patients</th>\n",
              "      <th>total_vaccinations</th>\n",
              "      <th>positive_rate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2020-01-22</th>\n",
              "      <td>9.144</td>\n",
              "      <td>232.286</td>\n",
              "      <td>27796.0</td>\n",
              "      <td>701659.0</td>\n",
              "      <td>5.750735e+10</td>\n",
              "      <td>0.153990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-23</th>\n",
              "      <td>9.716</td>\n",
              "      <td>308.286</td>\n",
              "      <td>27796.0</td>\n",
              "      <td>701659.0</td>\n",
              "      <td>5.750735e+10</td>\n",
              "      <td>0.154006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-24</th>\n",
              "      <td>13.716</td>\n",
              "      <td>460.858</td>\n",
              "      <td>27796.0</td>\n",
              "      <td>701659.0</td>\n",
              "      <td>5.750735e+10</td>\n",
              "      <td>0.154006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-25</th>\n",
              "      <td>22.856</td>\n",
              "      <td>728.859</td>\n",
              "      <td>27796.0</td>\n",
              "      <td>701659.0</td>\n",
              "      <td>5.750735e+10</td>\n",
              "      <td>0.154006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-26</th>\n",
              "      <td>32.429</td>\n",
              "      <td>1086.998</td>\n",
              "      <td>27796.0</td>\n",
              "      <td>701659.0</td>\n",
              "      <td>5.750735e+10</td>\n",
              "      <td>0.154067</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d21d08b0-28dd-4e30-b31b-8c2c059d5217')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d21d08b0-28dd-4e30-b31b-8c2c059d5217 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d21d08b0-28dd-4e30-b31b-8c2c059d5217');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-458acde1-8bf7-4409-a21d-0e1a77da3649\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-458acde1-8bf7-4409-a21d-0e1a77da3649')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-458acde1-8bf7-4409-a21d-0e1a77da3649 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "#data=df.set_index('ObservationDate')\n",
        "data.index = pd.to_datetime(data.index)\n",
        "data.index.name = None\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aajxY08TrDIQ",
        "outputId": "4864a33f-afd5-430f-8a8a-98ca70e09a89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1075, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "columns=['new_deaths', 'new_cases', 'icu_patients', 'hosp_patients',  'total_vaccinations', 'positive_rate']\n",
        "data.columns=columns\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "MCAhr761Xaug",
        "outputId": "6278b90a-c628-4985-9bc8-fee609c3cd83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            new_deaths  new_cases  icu_patients  hosp_patients  \\\n",
              "2020-01-22       9.144    232.286       27796.0       701659.0   \n",
              "2020-01-23       9.716    308.286       27796.0       701659.0   \n",
              "2020-01-24      13.716    460.858       27796.0       701659.0   \n",
              "2020-01-25      22.856    728.859       27796.0       701659.0   \n",
              "2020-01-26      32.429   1086.998       27796.0       701659.0   \n",
              "\n",
              "            total_vaccinations  positive_rate  \n",
              "2020-01-22        5.750735e+10       0.153990  \n",
              "2020-01-23        5.750735e+10       0.154006  \n",
              "2020-01-24        5.750735e+10       0.154006  \n",
              "2020-01-25        5.750735e+10       0.154006  \n",
              "2020-01-26        5.750735e+10       0.154067  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-90d8653c-b999-492b-a1d0-697bc89a99d8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>new_deaths</th>\n",
              "      <th>new_cases</th>\n",
              "      <th>icu_patients</th>\n",
              "      <th>hosp_patients</th>\n",
              "      <th>total_vaccinations</th>\n",
              "      <th>positive_rate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2020-01-22</th>\n",
              "      <td>9.144</td>\n",
              "      <td>232.286</td>\n",
              "      <td>27796.0</td>\n",
              "      <td>701659.0</td>\n",
              "      <td>5.750735e+10</td>\n",
              "      <td>0.153990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-23</th>\n",
              "      <td>9.716</td>\n",
              "      <td>308.286</td>\n",
              "      <td>27796.0</td>\n",
              "      <td>701659.0</td>\n",
              "      <td>5.750735e+10</td>\n",
              "      <td>0.154006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-24</th>\n",
              "      <td>13.716</td>\n",
              "      <td>460.858</td>\n",
              "      <td>27796.0</td>\n",
              "      <td>701659.0</td>\n",
              "      <td>5.750735e+10</td>\n",
              "      <td>0.154006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-25</th>\n",
              "      <td>22.856</td>\n",
              "      <td>728.859</td>\n",
              "      <td>27796.0</td>\n",
              "      <td>701659.0</td>\n",
              "      <td>5.750735e+10</td>\n",
              "      <td>0.154006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-26</th>\n",
              "      <td>32.429</td>\n",
              "      <td>1086.998</td>\n",
              "      <td>27796.0</td>\n",
              "      <td>701659.0</td>\n",
              "      <td>5.750735e+10</td>\n",
              "      <td>0.154067</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-90d8653c-b999-492b-a1d0-697bc89a99d8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-90d8653c-b999-492b-a1d0-697bc89a99d8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-90d8653c-b999-492b-a1d0-697bc89a99d8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d8743bb1-3002-4992-b526-05602de2012a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d8743bb1-3002-4992-b526-05602de2012a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d8743bb1-3002-4992-b526-05602de2012a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KF0uDMU1I6xJ"
      },
      "outputs": [],
      "source": [
        "output_dir_path = \"/content/drive/MyDrive/Covid_19/Results/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SBN1ixjC_Pm"
      },
      "source": [
        "## **Visualization of Response Variable**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "4Sdqn-OODFZz",
        "outputId": "596e6e87-0f5d-407d-d305-7baf27d24c58"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x400 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "fig = plt.figure(figsize = (15,4))\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(10,5))\n",
        "#fig.set(facecolor = \"white\")\n",
        "plt.plot(data['new_deaths'],'k', label='COVID-19 Deaths')\n",
        "plt.legend(['COVID-19 Deaths'], loc='upper right')\n",
        "plt.title('Time series plot for number of deaths')\n",
        "plt.xlabel('Time (year-month)')\n",
        "plt.ylabel('New Deaths')\n",
        "fig.savefig(output_dir_path+'original_data_plot.png',dpi=600)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-l1vOwyU28Jt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDjzpVypfMdW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-Vap-SkxQjy"
      },
      "source": [
        "####  **Correlation heatmap**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a lower triangular mask\n",
        "mask = np.triu(np.ones_like(data.corr(), dtype=bool))"
      ],
      "metadata": {
        "id": "qhlgO8vbjc9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data = nepse_data.iloc[:, 4:]\n",
        "sns.set_style(\"white\")  # Set the style to plain white background\n",
        "sns.set(style=\"white\", rc={\"axes.grid\": False})\n",
        "fig = plt.figure(figsize= (10,5))\n",
        "sns.heatmap(data.corr(), annot=True, mask=mask, cbar = False)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "1iM29B_TjeXi",
        "outputId": "d01a2854-723f-41ab-d0a0-30e2f34a4d8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Fv9RQuWqbzk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 869
        },
        "id": "pcaukJvvNTPN",
        "outputId": "43e4faa9-733d-47d4-b7a3-d1df75c1a631"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1000 with 6 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "fig = plt.figure(figsize = (20, 12))\n",
        "#fig.axes.get_yaxis().set_visible(False)\n",
        "data.plot(subplots = True, figsize = (12,10),grid=False)\n",
        "sns.set_style(\"whitegrid\")\n",
        "fig.savefig(output_dir_path+\"timeseries.png\",dpi=600)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "lpHdLNaUvDja",
        "outputId": "7c6f7c0c-f53f-408a-b29b-02d497851a5b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         new_deaths     new_cases  icu_patients  hosp_patients  \\\n",
              "count   1075.000000  1.075000e+03   1075.000000   1.075000e+03   \n",
              "mean   26074.468087  2.837561e+06  49990.128372   8.198857e+05   \n",
              "std    16185.721663  3.241578e+06  16540.309195   8.157766e+04   \n",
              "min        9.144000  2.322860e+02  27796.000000   7.016590e+05   \n",
              "25%     9697.146000  1.451314e+06  34863.500000   7.578905e+05   \n",
              "50%    26055.719000  2.128732e+06  46711.000000   8.105800e+05   \n",
              "75%    38309.361500  2.771227e+06  63738.500000   8.608305e+05   \n",
              "max    62789.997000  2.572646e+07  91750.000000   1.049419e+06   \n",
              "\n",
              "       total_vaccinations  positive_rate  \n",
              "count        1.075000e+03    1075.000000  \n",
              "mean         4.501539e+10       0.145116  \n",
              "std          1.485772e+10       0.016264  \n",
              "min          9.191042e+09       0.124980  \n",
              "25%          3.380068e+10       0.134804  \n",
              "50%          5.161593e+10       0.140962  \n",
              "75%          5.759321e+10       0.153660  \n",
              "max          5.787143e+10       0.218572  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8e00b524-be88-4d73-9bd4-690506f81e70\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>new_deaths</th>\n",
              "      <th>new_cases</th>\n",
              "      <th>icu_patients</th>\n",
              "      <th>hosp_patients</th>\n",
              "      <th>total_vaccinations</th>\n",
              "      <th>positive_rate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1075.000000</td>\n",
              "      <td>1.075000e+03</td>\n",
              "      <td>1075.000000</td>\n",
              "      <td>1.075000e+03</td>\n",
              "      <td>1.075000e+03</td>\n",
              "      <td>1075.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>26074.468087</td>\n",
              "      <td>2.837561e+06</td>\n",
              "      <td>49990.128372</td>\n",
              "      <td>8.198857e+05</td>\n",
              "      <td>4.501539e+10</td>\n",
              "      <td>0.145116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>16185.721663</td>\n",
              "      <td>3.241578e+06</td>\n",
              "      <td>16540.309195</td>\n",
              "      <td>8.157766e+04</td>\n",
              "      <td>1.485772e+10</td>\n",
              "      <td>0.016264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>9.144000</td>\n",
              "      <td>2.322860e+02</td>\n",
              "      <td>27796.000000</td>\n",
              "      <td>7.016590e+05</td>\n",
              "      <td>9.191042e+09</td>\n",
              "      <td>0.124980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>9697.146000</td>\n",
              "      <td>1.451314e+06</td>\n",
              "      <td>34863.500000</td>\n",
              "      <td>7.578905e+05</td>\n",
              "      <td>3.380068e+10</td>\n",
              "      <td>0.134804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>26055.719000</td>\n",
              "      <td>2.128732e+06</td>\n",
              "      <td>46711.000000</td>\n",
              "      <td>8.105800e+05</td>\n",
              "      <td>5.161593e+10</td>\n",
              "      <td>0.140962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>38309.361500</td>\n",
              "      <td>2.771227e+06</td>\n",
              "      <td>63738.500000</td>\n",
              "      <td>8.608305e+05</td>\n",
              "      <td>5.759321e+10</td>\n",
              "      <td>0.153660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>62789.997000</td>\n",
              "      <td>2.572646e+07</td>\n",
              "      <td>91750.000000</td>\n",
              "      <td>1.049419e+06</td>\n",
              "      <td>5.787143e+10</td>\n",
              "      <td>0.218572</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8e00b524-be88-4d73-9bd4-690506f81e70')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8e00b524-be88-4d73-9bd4-690506f81e70 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8e00b524-be88-4d73-9bd4-690506f81e70');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-260cd270-9139-4940-a408-d9a21d431d2f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-260cd270-9139-4940-a408-d9a21d431d2f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-260cd270-9139-4940-a408-d9a21d431d2f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1HR-3y8m8xQ",
        "outputId": "6daf2e6f-0f9b-4e7c-9d46-3b06da69e289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 1075 entries, 2020-01-22 to 2022-12-31\n",
            "Data columns (total 6 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   new_deaths          1075 non-null   float64\n",
            " 1   new_cases           1075 non-null   float64\n",
            " 2   icu_patients        1075 non-null   float64\n",
            " 3   hosp_patients       1075 non-null   float64\n",
            " 4   total_vaccinations  1075 non-null   float64\n",
            " 5   positive_rate       1075 non-null   float64\n",
            "dtypes: float64(6)\n",
            "memory usage: 58.8 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7CN1Pi07Nzi"
      },
      "source": [
        "### **Machine learning libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fi67Y5BNKB1k"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import GRU\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras import optimizers\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import r2_score\n",
        "import time\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifiXXSC7KluW"
      },
      "source": [
        "### **Supporting functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikplCNqpJ8CN"
      },
      "outputs": [],
      "source": [
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    return (np.mean(np.abs((y_true - y_pred)/(y_true))*100)) #some issues with zero denominator\n",
        "\n",
        "def calculate_scores(y_true, y_pred):\n",
        "  rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
        "  #R2_score = r2_score(y_true, y_pred)\n",
        "  R = np.corrcoef(y_true, y_pred)\n",
        "  #mae = mean_absolute_error(y_true, y_pred)\n",
        "  mape = mean_absolute_percentage_error(y_true, y_pred)\n",
        "  #dic = {'rmse':rmse, 'R2_score': R2_score, 'R':R[0,1], 'mae': mae, 'mape': mape}\n",
        "  dic = {'rmse':rmse, 'R': R[0,1], 'mape': mape}\n",
        "  return (dic)\n",
        "\n",
        "\n",
        "\n",
        "def DatasetCreation(dataset, time_step = 1):  ##defining a function that gives a dataset and a time step, which then returns the input and output data\n",
        "   DataX, DataY = [], []\n",
        "   for i in range(len(dataset)- time_step -1):\n",
        "         a = dataset[i:(i+ time_step), ]\n",
        "         DataX.append(a)\n",
        "         DataY.append(dataset[i + time_step, 0]) #ydata consists deaths\n",
        "   return np.array(DataX), np.array(DataY)\n",
        "\n",
        "\n",
        "def data_split(data, split = 0.2):\n",
        "  #======= creating training and test data===#\n",
        "  l1   = int(len(data) * (1- split))\n",
        "  l2    = len(data) - l1\n",
        "  data1  = data.iloc[0:l1,:]\n",
        "  data2   = data.iloc[l1:len(data),:]\n",
        "  return data1, data2\n",
        "\n",
        "def min_max_transform(data, feature_range=(0, 1)):\n",
        "   scaler = MinMaxScaler(feature_range)\n",
        "   return scaler.fit_transform(data)\n",
        "\n",
        "def min_max_inverse_transform(data_scaled, min_original, max_original):\n",
        "    return min_original +  data_scaled*(max_original - min_original)\n",
        "\n",
        "\n",
        "\n",
        "def write_dic_to_file(dic_name, file_name):\n",
        "  file = open(file_name, 'w')\n",
        "  file.write(str(dic_name))\n",
        "  file.close()\n",
        "\n",
        "import ast\n",
        "def read_dic_from_file(file_name):\n",
        "  file = open(file_name, \"r\")\n",
        "  contents = file.read()\n",
        "  dictionary = ast.literal_eval(contents)\n",
        "  file.close()\n",
        "  return dictionary\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFioIufGUClb"
      },
      "source": [
        "#**Building Deep Learning Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxCt6qNmURgn"
      },
      "source": [
        "## **Model 1: Build and Compile LSTM Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8PHScCoHoUn"
      },
      "outputs": [],
      "source": [
        "def Build_LSTM_Model(layers, time_step, num_features, optimizer = 'Adam', learning_rate = 0.001, verbose = 1):\n",
        "\n",
        "  model = Sequential()\n",
        "\n",
        "  for i in range(len(layers)):\n",
        "    if len(layers)==1:\n",
        "      model.add(LSTM(np.int(layers[i]), input_shape = (time_step, num_features)))\n",
        "    else:\n",
        "      if i < len(layers)-1:\n",
        "        if i == 0:\n",
        "          model.add(LSTM(np.int(layers[i]), input_shape=(time_step, num_features), return_sequences= True))\n",
        "          #model.add(Dropout(0.10))\n",
        "        else:\n",
        "          model.add(LSTM(np.int(layers[i]), return_sequences=True))\n",
        "          #model.add(Dropout(0.10))\n",
        "      else:\n",
        "        model.add(LSTM(np.int(layers[i])))\n",
        "        #model.add(Dropout(0.10))\n",
        "\n",
        "  model.add(Dense(16, activation='relu'))\n",
        "  model.add(Dense(1, activation = 'linear'))\n",
        "\n",
        "  if optimizer == 'Adam':\n",
        "    opt = optimizers.Adam(learning_rate = learning_rate)\n",
        "  elif optimizer == 'Adagrad':\n",
        "    opt = optimizers.Adagrad(learning_rate = learning_rate)\n",
        "  elif optimizer == 'Nadam':\n",
        "    opt = optimizers.Nadam(learning_rate = learning_rate)\n",
        "  elif optimizer == 'Adadelta':\n",
        "    opt = optimizers.Adadelta(learning_rate= learning_rate)\n",
        "  elif optimizer == 'RMSprop':\n",
        "    opt = optimizers.RMSprop(learning_rate= learning_rate)\n",
        "  else:\n",
        "    print(\"No optimizer found in the list(['Adam', 'Adagrad','Nadam', 'Adadelta', 'RMSprop'])! Please apply your optimizer manually...\")\n",
        "\n",
        "  model.compile(loss='mean_squared_error', optimizer= opt)\n",
        "\n",
        "  if verbose == 1:\n",
        "    print(model.summary())\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yfl0jy6aphJ"
      },
      "source": [
        "### **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnKUJFHVULBX",
        "outputId": "7f894262-5e18-46f1-cd82-6de8ac07da4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 20)                2480      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                336       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2833 (11.07 KB)\n",
            "Trainable params: 2833 (11.07 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.engine.sequential.Sequential at 0x784f9fc2b820>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "#optimizers_names = ['Adam', 'Adagrad', 'Nadam']\n",
        "optimizers_names = ['Adam']\n",
        "time_step = 5\n",
        "num_features = 10\n",
        "learning_rate = 0.001\n",
        "verbose = 1\n",
        "layers = [20]\n",
        "\n",
        "Build_LSTM_Model(layers, time_step, num_features, optimizer =  optimizers_names[0], learning_rate= learning_rate, verbose = verbose)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHze4bCCUMYp"
      },
      "source": [
        "## **Model 2: Build and Compile GRU Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnKn7oSUUUgi"
      },
      "outputs": [],
      "source": [
        "def Build_GRU_Model(layers, time_step, num_features, optimizer = 'Adam', learning_rate = 0.001, verbose = 1):\n",
        "\n",
        "  model = Sequential()\n",
        "\n",
        "  for i in range(len(layers)):\n",
        "    if len(layers)==1:\n",
        "      model.add(GRU(np.int(layers[i]), input_shape = (time_step, num_features)))\n",
        "    else:\n",
        "      if i < len(layers)-1:\n",
        "        if i == 0:\n",
        "          model.add(GRU(np.int(layers[i]), input_shape=(time_step, num_features), return_sequences= True))\n",
        "          #model.add(Dropout(0.10))\n",
        "        else:\n",
        "          model.add(GRU(np.int(layers[i]), return_sequences=True))\n",
        "          #model.add(Dropout(0.10))\n",
        "      else:\n",
        "        model.add(GRU(np.int(layers[i])))\n",
        "        #model.add(Dropout(0.10))\n",
        "  model.add(Dense(16, activation = 'relu'))\n",
        "  model.add(Dense(1, activation = 'linear'))\n",
        "\n",
        "  if optimizer == 'Adam':\n",
        "    opt = optimizers.Adam(learning_rate = learning_rate)\n",
        "  elif optimizer == 'Adagrad':\n",
        "    opt = optimizers.Adagrad(learning_rate = learning_rate)\n",
        "  elif optimizer == 'Nadam':\n",
        "    opt = optimizers.Nadam(learning_rate = learning_rate)\n",
        "  elif optimizer == 'Adadelta':\n",
        "    opt = optimizers.Adadelta(learning_rate= learning_rate)\n",
        "  elif optimizer == 'RMSprop':\n",
        "    opt = optimizers.RMSprop(learning_rate= learning_rate)\n",
        "  else:\n",
        "    print(\"No optimizer found in the list(['Adam', 'Adagrad','Nadam', 'Adadelta', 'RMSprop'])! Please apply your optimizer manually...\")\n",
        "\n",
        "  model.compile(loss='mean_squared_error', optimizer= opt)\n",
        "\n",
        "  if verbose == 1:\n",
        "    print(model.summary())\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXgQS60Sbx3s"
      },
      "source": [
        "### **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aogwyjm7bzov",
        "outputId": "3421dee3-62cb-42c8-8e77-1cdd45724a94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " gru_1 (GRU)                 (None, 20)                1860      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1881 (7.35 KB)\n",
            "Trainable params: 1881 (7.35 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.engine.sequential.Sequential at 0x7af95cb91f60>"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "optimizers_names = ['Adam', 'Adagrad', 'Nadam']\n",
        "time_step = 5\n",
        "num_features = 9\n",
        "learning_rate = 0.001\n",
        "verbose = 1\n",
        "layers = [20]\n",
        "\n",
        "Build_GRU_Model(layers, time_step, num_features, optimizer =  optimizers_names[0], learning_rate= learning_rate, verbose = verbose)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPrAqyLfY26G"
      },
      "source": [
        "# **Tuning Hyperparameters of Deep Learning Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_K-2oyoi02-"
      },
      "source": [
        "## **Model 1: Tuning Hyper Parameters of LSTM Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_uHVktrIj6K"
      },
      "outputs": [],
      "source": [
        "def LSTM_Hyper_Parameter_Tuning(layers, data, time_step, split, optimizers_names, learning_rates, batch_sizes, epochs, num_replicates = 2):\n",
        "  #======= creating training and test data===#\n",
        "  train_data, val_data = data_split(data, split)\n",
        "\n",
        "  num_features = train_data.shape[1]\n",
        "\n",
        "  min_train, max_train  = train_data[\"new_deaths\"].min(), train_data[\"new_deaths\"].max()\n",
        "  min_val, max_val   =    val_data[\"new_deaths\"].min(), val_data[\"new_deaths\"].max()\n",
        "\n",
        "  train_data_scaled  =  min_max_transform(train_data)\n",
        "  val_data_scaled    = min_max_transform(val_data)\n",
        "\n",
        "  X_train, y_train =   DatasetCreation(train_data_scaled, time_step)\n",
        "  X_val, y_val     =   DatasetCreation(val_data_scaled, time_step)\n",
        "\n",
        "  #========dealing with time series=========#\n",
        "\n",
        "  best_avg_rmse = 99999999999\n",
        "\n",
        "  collect_rmse = []\n",
        "\n",
        "  all_avg_rmse = np.zeros((len(optimizers_names), len(learning_rates), len(batch_sizes)))\n",
        "\n",
        "  best_hyper_parameters = {\"model\": layers, \"optimizer\": None, \"learning_rate\": None, \"batch_size\": None,\"best_avg_rmse\": None}\n",
        "\n",
        "  #hist_csv_file = 'model_history'+ str(time.time())+'.csv'\n",
        "\n",
        "  for opt in range(len(optimizers_names)):\n",
        "\n",
        "    for lr in range(len(learning_rates)):\n",
        "\n",
        "      for batch_size in range(len(batch_sizes)):\n",
        "\n",
        "        for i in range(num_replicates):\n",
        "\n",
        "          print(\"Running for \" + optimizers_names[opt] + \" optimizer \" + str(learning_rates[lr]) +  \" learning_rate \" +  str(batch_sizes[batch_size]) + \" batch_size and \" + str(i) +  \" replicate \" +  \"\\n\")\n",
        "\n",
        "          model = Build_LSTM_Model(layers, time_step, num_features, optimizers_names[opt], learning_rate = learning_rates[lr], verbose = 0)\n",
        "\n",
        "          callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience= 5)\n",
        "\n",
        "          history = model.fit(X_train, y_train, batch_size = batch_sizes[batch_size], epochs= epochs, validation_data = (X_val, y_val), callbacks=[callback], verbose = 1)\n",
        "\n",
        "          #hist_df = pd.DataFrame(history.history)\n",
        "          #hist_df.to_csv(\"history\")\n",
        "          #with open(\"hist_csv_file.csv\", mode='w') as f:\n",
        "          #  hist_df.to_csv(f)\n",
        "\n",
        "          #==============Making predictions in original scale ==========\n",
        "          #train_pred  =  min_max_inverse_transform(model.predict(X_train).ravel(), min_train, max_train)\n",
        "          val_pred    =  min_max_inverse_transform(model.predict(X_val).ravel(), min_val, max_val)\n",
        "\n",
        "          #train_scores =  calculate_scores(min_max_inverse_transform(y_train, min_train, max_train),train_pred)\n",
        "          #scores =   calculate_scores(min_max_inverse_transform(y_val, min_val, max_val),val_pred)\n",
        "\n",
        "          collect_rmse.append(math.sqrt(mean_squared_error(min_max_inverse_transform(y_val, min_val, max_val),val_pred)))\n",
        "\n",
        "        avg_rmse = np.mean(np.array(collect_rmse))\n",
        "        all_avg_rmse[opt][lr][batch_size] = avg_rmse\n",
        "\n",
        "        if avg_rmse < best_avg_rmse:\n",
        "          best_avg_rmse = avg_rmse\n",
        "          best_hyper_parameters = {\"model\": layers,  \"optimizer\": optimizers_names[opt], \"learning_rate\": learning_rates[lr], \"batch_size\": batch_sizes[batch_size], \"best_avg_rmse\": best_avg_rmse}\n",
        "\n",
        "\n",
        "  output_dictionary = {\n",
        "      \"best_hyper_parameters\":  best_hyper_parameters,\n",
        "      \"all_avg_rmse\": all_avg_rmse\n",
        "       }\n",
        "\n",
        "  #writing output dictionary in the file\n",
        "\n",
        "  file_name = output_dir_path+ \"lstm-\" + str(layers[0])+ \"N-hyperparameter_tuning__results\" + \".txt\"\n",
        "  write_dic_to_file(output_dictionary, file_name)\n",
        "\n",
        "  print(\"Best_hyper_parameters(LSTM): \\n\", output_dictionary['best_hyper_parameters'])\n",
        "  print(\"all_avg_rmse(LSTM): \\n\", output_dictionary['all_avg_rmse'])\n",
        "\n",
        "  return output_dictionary['best_hyper_parameters']\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hgrGSsZaZhH"
      },
      "source": [
        "### **Case I: 8N-LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvI-r_JGajeW",
        "outputId": "29f1b4fb-e88b-4097-b4f3-58f5056ec24b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running for Adam optimizer 0.1 learning_rate 4 batch_size and 0 replicate \n",
            "\n",
            "Epoch 1/50\n",
            "171/171 [==============================] - 4s 8ms/step - loss: 0.0158 - val_loss: 0.0127\n",
            "Epoch 2/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 0.0015 - val_loss: 0.0096\n",
            "Epoch 3/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 0.0016 - val_loss: 0.0071\n",
            "Epoch 4/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 0.0018 - val_loss: 0.0063\n",
            "Epoch 5/50\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0012 - val_loss: 0.0087\n",
            "Epoch 6/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0011 - val_loss: 0.0041\n",
            "Epoch 7/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0016 - val_loss: 0.0042\n",
            "Epoch 8/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 6.6992e-04 - val_loss: 0.0045\n",
            "Epoch 9/50\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0101 - val_loss: 0.0069\n",
            "Epoch 10/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 5.8833e-04 - val_loss: 0.0034\n",
            "Epoch 11/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 0.0012 - val_loss: 0.0025\n",
            "Epoch 12/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 7.1129e-04 - val_loss: 0.0017\n",
            "Epoch 13/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 9.1084e-04 - val_loss: 0.0013\n",
            "Epoch 14/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 6.9928e-04 - val_loss: 0.0050\n",
            "Epoch 15/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 9.3266e-04 - val_loss: 0.0035\n",
            "Epoch 16/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 0.0023 - val_loss: 0.0025\n",
            "Epoch 17/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 0.0026 - val_loss: 0.0021\n",
            "Epoch 18/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 5.7782e-04 - val_loss: 0.0015\n",
            "6/6 [==============================] - 0s 4ms/step\n",
            "Running for Adam optimizer 0.1 learning_rate 4 batch_size and 1 replicate \n",
            "\n",
            "Epoch 1/50\n",
            "171/171 [==============================] - 5s 10ms/step - loss: 0.0106 - val_loss: 0.0060\n",
            "Epoch 2/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 0.0027 - val_loss: 0.0061\n",
            "Epoch 3/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 0.0025 - val_loss: 0.0042\n",
            "Epoch 4/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 0.0031 - val_loss: 0.0065\n",
            "Epoch 5/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 0.0011 - val_loss: 0.0049\n",
            "Epoch 6/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 0.0022 - val_loss: 0.0392\n",
            "Epoch 7/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 0.0035 - val_loss: 0.0031\n",
            "Epoch 8/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 5.9232e-04 - val_loss: 0.0030\n",
            "Epoch 9/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 0.0025 - val_loss: 0.0028\n",
            "Epoch 10/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 0.0036 - val_loss: 0.0423\n",
            "Epoch 11/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 0.0038 - val_loss: 0.0019\n",
            "Epoch 12/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 4.4309e-04 - val_loss: 0.0012\n",
            "Epoch 13/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 8.1830e-04 - val_loss: 0.0013\n",
            "Epoch 14/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0016 - val_loss: 0.0021\n",
            "Epoch 15/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 5.5242e-04 - val_loss: 0.0016\n",
            "Epoch 16/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 5.1767e-04 - val_loss: 0.0021\n",
            "Epoch 17/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 6.5593e-04 - val_loss: 0.0010\n",
            "Epoch 18/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 5.1644e-04 - val_loss: 7.8173e-04\n",
            "Epoch 19/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 5.7311e-04 - val_loss: 0.0044\n",
            "Epoch 20/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 0.0017 - val_loss: 0.0029\n",
            "Epoch 21/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 0.0011 - val_loss: 3.8731e-04\n",
            "Epoch 22/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 4.8158e-04 - val_loss: 4.7124e-04\n",
            "Epoch 23/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 9.6242e-04 - val_loss: 6.3432e-04\n",
            "Epoch 24/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 7.4448e-04 - val_loss: 6.3656e-04\n",
            "Epoch 25/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 0.0013 - val_loss: 5.9470e-04\n",
            "Epoch 26/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 6.1862e-04 - val_loss: 0.0088\n",
            "6/6 [==============================] - 0s 3ms/step\n",
            "Running for Adam optimizer 0.1 learning_rate 8 batch_size and 0 replicate \n",
            "\n",
            "Epoch 1/50\n",
            "86/86 [==============================] - 4s 14ms/step - loss: 0.0243 - val_loss: 0.0119\n",
            "Epoch 2/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0014 - val_loss: 0.0080\n",
            "Epoch 3/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0011 - val_loss: 0.0060\n",
            "Epoch 4/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 0.0012 - val_loss: 0.0055\n",
            "Epoch 5/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 5.8358e-04 - val_loss: 0.0044\n",
            "Epoch 6/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 7.4458e-04 - val_loss: 0.0055\n",
            "Epoch 7/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 4.5691e-04 - val_loss: 0.0038\n",
            "Epoch 8/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 5.9358e-04 - val_loss: 0.0074\n",
            "Epoch 9/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 4.6239e-04 - val_loss: 0.0063\n",
            "Epoch 10/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 0.0031 - val_loss: 0.0107\n",
            "Epoch 11/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 0.0015 - val_loss: 0.0075\n",
            "Epoch 12/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 0.0017 - val_loss: 0.0037\n",
            "Epoch 13/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 4.4288e-04 - val_loss: 0.0032\n",
            "Epoch 14/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 5.4512e-04 - val_loss: 0.0033\n",
            "Epoch 15/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 3.2393e-04 - val_loss: 0.0042\n",
            "Epoch 16/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 4.1531e-04 - val_loss: 0.0019\n",
            "Epoch 17/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 3.5909e-04 - val_loss: 0.0026\n",
            "Epoch 18/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 3.3430e-04 - val_loss: 0.0015\n",
            "Epoch 19/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 4.1793e-04 - val_loss: 0.0019\n",
            "Epoch 20/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 5.0847e-04 - val_loss: 0.0040\n",
            "Epoch 21/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 7.8169e-04 - val_loss: 0.0017\n",
            "Epoch 22/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 3.5240e-04 - val_loss: 0.0017\n",
            "Epoch 23/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 6.2168e-04 - val_loss: 0.0020\n",
            "6/6 [==============================] - 0s 3ms/step\n",
            "Running for Adam optimizer 0.1 learning_rate 8 batch_size and 1 replicate \n",
            "\n",
            "Epoch 1/50\n",
            "86/86 [==============================] - 3s 11ms/step - loss: 0.0164 - val_loss: 0.0129\n",
            "Epoch 2/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 0.0012 - val_loss: 0.0083\n",
            "Epoch 3/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 0.0011 - val_loss: 0.0080\n",
            "Epoch 4/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 0.0017 - val_loss: 0.0099\n",
            "Epoch 5/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0014 - val_loss: 0.0048\n",
            "Epoch 6/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 7.8713e-04 - val_loss: 0.0059\n",
            "Epoch 7/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0017 - val_loss: 0.0046\n",
            "Epoch 8/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0012 - val_loss: 0.0036\n",
            "Epoch 9/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 4.0817e-04 - val_loss: 0.0033\n",
            "Epoch 10/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 4.6101e-04 - val_loss: 0.0032\n",
            "Epoch 11/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 3.5695e-04 - val_loss: 0.0025\n",
            "Epoch 12/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 3.6701e-04 - val_loss: 0.0024\n",
            "Epoch 13/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 2.6584e-04 - val_loss: 0.0021\n",
            "Epoch 14/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 0.0017 - val_loss: 0.0054\n",
            "Epoch 15/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 0.0015 - val_loss: 0.0021\n",
            "Epoch 16/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 0.0012 - val_loss: 0.0019\n",
            "Epoch 17/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 3.5932e-04 - val_loss: 0.0028\n",
            "Epoch 18/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 4.0789e-04 - val_loss: 0.0036\n",
            "Epoch 19/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 3.0131e-04 - val_loss: 0.0016\n",
            "Epoch 20/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.9517e-04 - val_loss: 0.0030\n",
            "Epoch 21/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 0.0014 - val_loss: 0.0021\n",
            "Epoch 22/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 3.7243e-04 - val_loss: 0.0019\n",
            "Epoch 23/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 5.2500e-04 - val_loss: 0.0016\n",
            "Epoch 24/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 5.0957e-04 - val_loss: 0.0024\n",
            "Epoch 25/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 3.0777e-04 - val_loss: 0.0023\n",
            "Epoch 26/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 3.4777e-04 - val_loss: 0.0019\n",
            "Epoch 27/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 3.5513e-04 - val_loss: 0.0017\n",
            "Epoch 28/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 6.2212e-04 - val_loss: 0.0014\n",
            "Epoch 29/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 5.9894e-04 - val_loss: 0.0012\n",
            "Epoch 30/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 6.1376e-04 - val_loss: 0.0012\n",
            "Epoch 31/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 0.0056 - val_loss: 0.0042\n",
            "Epoch 32/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 6.7518e-04 - val_loss: 0.0016\n",
            "Epoch 33/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 8.4965e-04 - val_loss: 0.0021\n",
            "Epoch 34/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 3.0842e-04 - val_loss: 0.0017\n",
            "Epoch 35/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0013 - val_loss: 0.0016\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adam optimizer 0.1 learning_rate 16 batch_size and 0 replicate \n",
            "\n",
            "Epoch 1/50\n",
            "43/43 [==============================] - 4s 16ms/step - loss: 0.1333 - val_loss: 0.0196\n",
            "Epoch 2/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0058 - val_loss: 0.0200\n",
            "Epoch 3/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0013 - val_loss: 0.0166\n",
            "Epoch 4/50\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 9.3492e-04 - val_loss: 0.0141\n",
            "Epoch 5/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 9.0548e-04 - val_loss: 0.0125\n",
            "Epoch 6/50\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 7.2373e-04 - val_loss: 0.0107\n",
            "Epoch 7/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 6.3175e-04 - val_loss: 0.0092\n",
            "Epoch 8/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 4.7436e-04 - val_loss: 0.0083\n",
            "Epoch 9/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 5.4789e-04 - val_loss: 0.0080\n",
            "Epoch 10/50\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 5.7672e-04 - val_loss: 0.0074\n",
            "Epoch 11/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 4.2187e-04 - val_loss: 0.0065\n",
            "Epoch 12/50\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 4.7366e-04 - val_loss: 0.0052\n",
            "Epoch 13/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 5.0728e-04 - val_loss: 0.0051\n",
            "Epoch 14/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 3.5623e-04 - val_loss: 0.0041\n",
            "Epoch 15/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 2.4182e-04 - val_loss: 0.0039\n",
            "Epoch 16/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 4.1365e-04 - val_loss: 0.0035\n",
            "Epoch 17/50\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 2.6892e-04 - val_loss: 0.0026\n",
            "Epoch 18/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 2.0211e-04 - val_loss: 0.0022\n",
            "Epoch 19/50\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 2.4906e-04 - val_loss: 0.0021\n",
            "Epoch 20/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 2.2372e-04 - val_loss: 0.0016\n",
            "Epoch 21/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 1.8187e-04 - val_loss: 0.0014\n",
            "Epoch 22/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 1.2889e-04 - val_loss: 0.0012\n",
            "Epoch 23/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 1.4321e-04 - val_loss: 0.0013\n",
            "Epoch 24/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 1.4521e-04 - val_loss: 0.0014\n",
            "Epoch 25/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 1.7582e-04 - val_loss: 8.2021e-04\n",
            "Epoch 26/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 1.5310e-04 - val_loss: 0.0010\n",
            "Epoch 27/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 3.6671e-04 - val_loss: 0.0012\n",
            "Epoch 28/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 1.3417e-04 - val_loss: 0.0011\n",
            "Epoch 29/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 1.9009e-04 - val_loss: 0.0011\n",
            "Epoch 30/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 2.2395e-04 - val_loss: 9.4690e-04\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adam optimizer 0.1 learning_rate 16 batch_size and 1 replicate \n",
            "\n",
            "Epoch 1/50\n",
            "43/43 [==============================] - 4s 19ms/step - loss: 0.0512 - val_loss: 0.0172\n",
            "Epoch 2/50\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 0.0025 - val_loss: 0.0122\n",
            "Epoch 3/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 8.8084e-04 - val_loss: 0.0078\n",
            "Epoch 4/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 3.9810e-04 - val_loss: 0.0071\n",
            "Epoch 5/50\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 6.6456e-04 - val_loss: 0.0068\n",
            "Epoch 6/50\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 5.0445e-04 - val_loss: 0.0061\n",
            "Epoch 7/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 2.9895e-04 - val_loss: 0.0049\n",
            "Epoch 8/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 2.8762e-04 - val_loss: 0.0057\n",
            "Epoch 9/50\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 3.1957e-04 - val_loss: 0.0050\n",
            "Epoch 10/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 2.7248e-04 - val_loss: 0.0040\n",
            "Epoch 11/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 1.8685e-04 - val_loss: 0.0038\n",
            "Epoch 12/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 1.9468e-04 - val_loss: 0.0043\n",
            "Epoch 13/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 2.3448e-04 - val_loss: 0.0045\n",
            "Epoch 14/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 1.9399e-04 - val_loss: 0.0039\n",
            "Epoch 15/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 2.5675e-04 - val_loss: 0.0058\n",
            "Epoch 16/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 4.4272e-04 - val_loss: 0.0039\n",
            "6/6 [==============================] - 0s 3ms/step\n",
            "Running for Adam optimizer 0.01 learning_rate 4 batch_size and 0 replicate \n",
            "\n",
            "Epoch 1/50\n",
            "171/171 [==============================] - 5s 11ms/step - loss: 0.0122 - val_loss: 0.0114\n",
            "Epoch 2/50\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 6.8855e-04 - val_loss: 0.0081\n",
            "Epoch 3/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 6.7028e-04 - val_loss: 0.0064\n",
            "Epoch 4/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 8.2770e-04 - val_loss: 0.0067\n",
            "Epoch 5/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 5.2989e-04 - val_loss: 0.0058\n",
            "Epoch 6/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 3.5784e-04 - val_loss: 0.0049\n",
            "Epoch 7/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 3.1522e-04 - val_loss: 0.0046\n",
            "Epoch 8/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 3.6224e-04 - val_loss: 0.0033\n",
            "Epoch 9/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 3.8743e-04 - val_loss: 0.0043\n",
            "Epoch 10/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 3.4891e-04 - val_loss: 0.0040\n",
            "Epoch 11/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 2.1054e-04 - val_loss: 0.0032\n",
            "Epoch 12/50\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 5.3957e-04 - val_loss: 0.0036\n",
            "Epoch 13/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 2.3120e-04 - val_loss: 0.0032\n",
            "Epoch 14/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 2.2035e-04 - val_loss: 0.0017\n",
            "Epoch 15/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 2.6713e-04 - val_loss: 0.0023\n",
            "Epoch 16/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 2.1570e-04 - val_loss: 0.0027\n",
            "Epoch 17/50\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 3.9753e-04 - val_loss: 0.0023\n",
            "Epoch 18/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 4.8808e-04 - val_loss: 0.0026\n",
            "Epoch 19/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 1.9874e-04 - val_loss: 0.0023\n",
            "6/6 [==============================] - 0s 3ms/step\n",
            "Running for Adam optimizer 0.01 learning_rate 4 batch_size and 1 replicate \n",
            "\n",
            "Epoch 1/50\n",
            "171/171 [==============================] - 3s 7ms/step - loss: 0.0069 - val_loss: 0.0139\n",
            "Epoch 2/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 0.0012 - val_loss: 0.0112\n",
            "Epoch 3/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 8.6708e-04 - val_loss: 0.0094\n",
            "Epoch 4/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 5.3977e-04 - val_loss: 0.0062\n",
            "Epoch 5/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 7.1291e-04 - val_loss: 0.0054\n",
            "Epoch 6/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 3.4909e-04 - val_loss: 0.0040\n",
            "Epoch 7/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 3.1747e-04 - val_loss: 0.0037\n",
            "Epoch 8/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 2.6955e-04 - val_loss: 0.0034\n",
            "Epoch 9/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 2.5930e-04 - val_loss: 0.0060\n",
            "Epoch 10/50\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 9.3965e-04 - val_loss: 0.0034\n",
            "Epoch 11/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 2.1868e-04 - val_loss: 0.0033\n",
            "Epoch 12/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 3.8131e-04 - val_loss: 0.0030\n",
            "Epoch 13/50\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 2.4470e-04 - val_loss: 0.0028\n",
            "Epoch 14/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 2.1285e-04 - val_loss: 0.0027\n",
            "Epoch 15/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 1.9331e-04 - val_loss: 0.0018\n",
            "Epoch 16/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 2.0920e-04 - val_loss: 0.0018\n",
            "Epoch 17/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 1.8480e-04 - val_loss: 0.0017\n",
            "Epoch 18/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 2.0846e-04 - val_loss: 0.0015\n",
            "Epoch 19/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 3.2882e-04 - val_loss: 0.0019\n",
            "Epoch 20/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 1.6813e-04 - val_loss: 0.0012\n",
            "Epoch 21/50\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 4.7931e-04 - val_loss: 0.0019\n",
            "Epoch 22/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 3.6522e-04 - val_loss: 0.0018\n",
            "Epoch 23/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 1.5264e-04 - val_loss: 0.0011\n",
            "Epoch 24/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 1.3947e-04 - val_loss: 0.0010\n",
            "Epoch 25/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 1.3566e-04 - val_loss: 5.6739e-04\n",
            "Epoch 26/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 3.2150e-04 - val_loss: 0.0014\n",
            "Epoch 27/50\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 1.9280e-04 - val_loss: 0.0010\n",
            "Epoch 28/50\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 1.6597e-04 - val_loss: 8.0093e-04\n",
            "Epoch 29/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 1.4235e-04 - val_loss: 6.4349e-04\n",
            "Epoch 30/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 1.2947e-04 - val_loss: 5.6799e-04\n",
            "6/6 [==============================] - 0s 3ms/step\n",
            "Running for Adam optimizer 0.01 learning_rate 8 batch_size and 0 replicate \n",
            "\n",
            "Epoch 1/50\n",
            "86/86 [==============================] - 3s 10ms/step - loss: 0.0095 - val_loss: 0.0274\n",
            "Epoch 2/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 0.0013 - val_loss: 0.0218\n",
            "Epoch 3/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 8.0070e-04 - val_loss: 0.0170\n",
            "Epoch 4/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 5.6907e-04 - val_loss: 0.0147\n",
            "Epoch 5/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 6.3989e-04 - val_loss: 0.0148\n",
            "Epoch 6/50\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 4.3710e-04 - val_loss: 0.0112\n",
            "Epoch 7/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 3.9619e-04 - val_loss: 0.0123\n",
            "Epoch 8/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 3.5375e-04 - val_loss: 0.0079\n",
            "Epoch 9/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 2.9992e-04 - val_loss: 0.0083\n",
            "Epoch 10/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 3.5512e-04 - val_loss: 0.0078\n",
            "Epoch 11/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 2.4644e-04 - val_loss: 0.0070\n",
            "Epoch 12/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 3.6948e-04 - val_loss: 0.0071\n",
            "Epoch 13/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 2.0097e-04 - val_loss: 0.0062\n",
            "Epoch 14/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 2.4501e-04 - val_loss: 0.0060\n",
            "Epoch 15/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 2.5528e-04 - val_loss: 0.0063\n",
            "Epoch 16/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.1951e-04 - val_loss: 0.0062\n",
            "Epoch 17/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.1804e-04 - val_loss: 0.0047\n",
            "Epoch 18/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.3752e-04 - val_loss: 0.0063\n",
            "Epoch 19/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 1.7025e-04 - val_loss: 0.0061\n",
            "Epoch 20/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 4.3072e-04 - val_loss: 0.0045\n",
            "Epoch 21/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 1.9450e-04 - val_loss: 0.0054\n",
            "Epoch 22/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.2817e-04 - val_loss: 0.0047\n",
            "Epoch 23/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 1.7892e-04 - val_loss: 0.0050\n",
            "Epoch 24/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.2629e-04 - val_loss: 0.0054\n",
            "Epoch 25/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 1.6426e-04 - val_loss: 0.0042\n",
            "Epoch 26/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 4.0176e-04 - val_loss: 0.0063\n",
            "Epoch 27/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.9325e-04 - val_loss: 0.0033\n",
            "Epoch 28/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.7384e-04 - val_loss: 0.0041\n",
            "Epoch 29/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 2.8524e-04 - val_loss: 0.0038\n",
            "Epoch 30/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 1.4427e-04 - val_loss: 0.0035\n",
            "Epoch 31/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 1.3904e-04 - val_loss: 0.0036\n",
            "Epoch 32/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 1.3182e-04 - val_loss: 0.0033\n",
            "Epoch 33/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 1.7503e-04 - val_loss: 0.0027\n",
            "Epoch 34/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 2.1159e-04 - val_loss: 0.0032\n",
            "Epoch 35/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 2.1993e-04 - val_loss: 0.0038\n",
            "Epoch 36/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 2.2602e-04 - val_loss: 0.0028\n",
            "Epoch 37/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 2.4318e-04 - val_loss: 0.0029\n",
            "Epoch 38/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 2.2083e-04 - val_loss: 0.0031\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adam optimizer 0.01 learning_rate 8 batch_size and 1 replicate \n",
            "\n",
            "Epoch 1/50\n",
            "86/86 [==============================] - 3s 11ms/step - loss: 0.0078 - val_loss: 0.0122\n",
            "Epoch 2/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 5.0041e-04 - val_loss: 0.0096\n",
            "Epoch 3/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 4.0335e-04 - val_loss: 0.0091\n",
            "Epoch 4/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 4.1988e-04 - val_loss: 0.0079\n",
            "Epoch 5/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 3.6267e-04 - val_loss: 0.0086\n",
            "Epoch 6/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.3316e-04 - val_loss: 0.0071\n",
            "Epoch 7/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 3.0659e-04 - val_loss: 0.0063\n",
            "Epoch 8/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.7673e-04 - val_loss: 0.0061\n",
            "Epoch 9/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 2.6039e-04 - val_loss: 0.0070\n",
            "Epoch 10/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 2.6032e-04 - val_loss: 0.0051\n",
            "Epoch 11/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 2.2070e-04 - val_loss: 0.0060\n",
            "Epoch 12/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.6598e-04 - val_loss: 0.0050\n",
            "Epoch 13/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.5609e-04 - val_loss: 0.0058\n",
            "Epoch 14/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.9764e-04 - val_loss: 0.0047\n",
            "Epoch 15/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 1.7971e-04 - val_loss: 0.0048\n",
            "Epoch 16/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 2.7094e-04 - val_loss: 0.0044\n",
            "Epoch 17/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 3.2525e-04 - val_loss: 0.0047\n",
            "Epoch 18/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 3.4262e-04 - val_loss: 0.0057\n",
            "Epoch 19/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 2.2045e-04 - val_loss: 0.0049\n",
            "Epoch 20/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 2.1274e-04 - val_loss: 0.0043\n",
            "Epoch 21/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 1.7736e-04 - val_loss: 0.0047\n",
            "Epoch 22/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 1.8930e-04 - val_loss: 0.0042\n",
            "Epoch 23/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 2.9737e-04 - val_loss: 0.0046\n",
            "Epoch 24/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 1.9677e-04 - val_loss: 0.0044\n",
            "Epoch 25/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 1.7976e-04 - val_loss: 0.0038\n",
            "Epoch 26/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 2.1229e-04 - val_loss: 0.0038\n",
            "Epoch 27/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 3.1266e-04 - val_loss: 0.0041\n",
            "Epoch 28/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.0320e-04 - val_loss: 0.0036\n",
            "Epoch 29/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 1.7298e-04 - val_loss: 0.0038\n",
            "Epoch 30/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 1.7497e-04 - val_loss: 0.0029\n",
            "Epoch 31/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 1.6678e-04 - val_loss: 0.0037\n",
            "Epoch 32/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.0809e-04 - val_loss: 0.0033\n",
            "Epoch 33/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 1.6939e-04 - val_loss: 0.0027\n",
            "Epoch 34/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 3.4888e-04 - val_loss: 0.0029\n",
            "Epoch 35/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 2.7899e-04 - val_loss: 0.0029\n",
            "Epoch 36/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 1.2947e-04 - val_loss: 0.0039\n",
            "Epoch 37/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 1.8079e-04 - val_loss: 0.0030\n",
            "Epoch 38/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 1.6591e-04 - val_loss: 0.0028\n",
            "6/6 [==============================] - 0s 5ms/step\n",
            "Running for Adam optimizer 0.01 learning_rate 16 batch_size and 0 replicate \n",
            "\n",
            "Epoch 1/50\n",
            "43/43 [==============================] - 3s 24ms/step - loss: 0.0340 - val_loss: 0.0191\n",
            "Epoch 2/50\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0171\n",
            "Epoch 3/50\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 8.6075e-04 - val_loss: 0.0165\n",
            "Epoch 4/50\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 6.2919e-04 - val_loss: 0.0164\n",
            "Epoch 5/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 6.2267e-04 - val_loss: 0.0146\n",
            "Epoch 6/50\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.7012e-04 - val_loss: 0.0128\n",
            "Epoch 7/50\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.2948e-04 - val_loss: 0.0126\n",
            "Epoch 8/50\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 3.9340e-04 - val_loss: 0.0117\n",
            "Epoch 9/50\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.4641e-04 - val_loss: 0.0111\n",
            "Epoch 10/50\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.5546e-04 - val_loss: 0.0101\n",
            "Epoch 11/50\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 3.1335e-04 - val_loss: 0.0091\n",
            "Epoch 12/50\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 3.0891e-04 - val_loss: 0.0088\n",
            "Epoch 13/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 2.5505e-04 - val_loss: 0.0079\n",
            "Epoch 14/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 2.7457e-04 - val_loss: 0.0081\n",
            "Epoch 15/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 2.5442e-04 - val_loss: 0.0069\n",
            "Epoch 16/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 2.4820e-04 - val_loss: 0.0067\n",
            "Epoch 17/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 2.2584e-04 - val_loss: 0.0058\n",
            "Epoch 18/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 2.9308e-04 - val_loss: 0.0048\n",
            "Epoch 19/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 2.4095e-04 - val_loss: 0.0049\n",
            "Epoch 20/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 2.1693e-04 - val_loss: 0.0048\n",
            "Epoch 21/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 2.1379e-04 - val_loss: 0.0046\n",
            "Epoch 22/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 2.1998e-04 - val_loss: 0.0042\n",
            "Epoch 23/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 1.7139e-04 - val_loss: 0.0042\n",
            "Epoch 24/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 1.6790e-04 - val_loss: 0.0039\n",
            "Epoch 25/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 1.9559e-04 - val_loss: 0.0038\n",
            "Epoch 26/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 1.6954e-04 - val_loss: 0.0032\n",
            "Epoch 27/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 1.3593e-04 - val_loss: 0.0029\n",
            "Epoch 28/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 2.1062e-04 - val_loss: 0.0034\n",
            "Epoch 29/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 2.1518e-04 - val_loss: 0.0029\n",
            "Epoch 30/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 2.2990e-04 - val_loss: 0.0031\n",
            "Epoch 31/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 1.7656e-04 - val_loss: 0.0030\n",
            "Epoch 32/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 1.3088e-04 - val_loss: 0.0026\n",
            "Epoch 33/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 1.3346e-04 - val_loss: 0.0024\n",
            "Epoch 34/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 1.3238e-04 - val_loss: 0.0023\n",
            "Epoch 35/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 1.5587e-04 - val_loss: 0.0023\n",
            "Epoch 36/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 1.7956e-04 - val_loss: 0.0023\n",
            "Epoch 37/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 1.2158e-04 - val_loss: 0.0023\n",
            "Epoch 38/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 1.3260e-04 - val_loss: 0.0024\n",
            "Epoch 39/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 3.4517e-04 - val_loss: 0.0021\n",
            "Epoch 40/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 1.6050e-04 - val_loss: 0.0030\n",
            "Epoch 41/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 1.2018e-04 - val_loss: 0.0024\n",
            "Epoch 42/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 1.1558e-04 - val_loss: 0.0020\n",
            "Epoch 43/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 2.0211e-04 - val_loss: 0.0022\n",
            "Epoch 44/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 1.9342e-04 - val_loss: 0.0025\n",
            "Epoch 45/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 1.1489e-04 - val_loss: 0.0020\n",
            "Epoch 46/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 1.1840e-04 - val_loss: 0.0022\n",
            "Epoch 47/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 1.2805e-04 - val_loss: 0.0020\n",
            "6/6 [==============================] - 0s 3ms/step\n",
            "Running for Adam optimizer 0.01 learning_rate 16 batch_size and 1 replicate \n",
            "\n",
            "Epoch 1/50\n",
            "43/43 [==============================] - 3s 16ms/step - loss: 0.0159 - val_loss: 0.0157\n",
            "Epoch 2/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0012 - val_loss: 0.0135\n",
            "Epoch 3/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 6.7538e-04 - val_loss: 0.0125\n",
            "Epoch 4/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 4.7269e-04 - val_loss: 0.0098\n",
            "Epoch 5/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 3.6731e-04 - val_loss: 0.0093\n",
            "Epoch 6/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 3.2568e-04 - val_loss: 0.0092\n",
            "Epoch 7/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 3.1597e-04 - val_loss: 0.0082\n",
            "Epoch 8/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 3.7657e-04 - val_loss: 0.0078\n",
            "Epoch 9/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 2.1742e-04 - val_loss: 0.0074\n",
            "Epoch 10/50\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 3.0891e-04 - val_loss: 0.0064\n",
            "Epoch 11/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 2.1587e-04 - val_loss: 0.0065\n",
            "Epoch 12/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 2.9590e-04 - val_loss: 0.0064\n",
            "Epoch 13/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 3.0289e-04 - val_loss: 0.0062\n",
            "Epoch 14/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 2.2527e-04 - val_loss: 0.0052\n",
            "Epoch 15/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 3.1558e-04 - val_loss: 0.0055\n",
            "Epoch 16/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 2.1604e-04 - val_loss: 0.0053\n",
            "Epoch 17/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 2.6996e-04 - val_loss: 0.0052\n",
            "Epoch 18/50\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 2.0170e-04 - val_loss: 0.0053\n",
            "Epoch 19/50\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 1.8649e-04 - val_loss: 0.0050\n",
            "Epoch 20/50\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 2.1876e-04 - val_loss: 0.0049\n",
            "Epoch 21/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 2.0419e-04 - val_loss: 0.0060\n",
            "Epoch 22/50\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 2.4866e-04 - val_loss: 0.0045\n",
            "Epoch 23/50\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 2.2727e-04 - val_loss: 0.0042\n",
            "Epoch 24/50\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 3.1398e-04 - val_loss: 0.0048\n",
            "Epoch 25/50\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 2.3296e-04 - val_loss: 0.0047\n",
            "Epoch 26/50\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 1.9984e-04 - val_loss: 0.0044\n",
            "Epoch 27/50\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 2.1360e-04 - val_loss: 0.0044\n",
            "Epoch 28/50\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 2.9647e-04 - val_loss: 0.0048\n",
            "6/6 [==============================] - 0s 4ms/step\n",
            "Running for Adam optimizer 0.001 learning_rate 4 batch_size and 0 replicate \n",
            "\n",
            "Epoch 1/50\n",
            "171/171 [==============================] - 3s 8ms/step - loss: 0.0256 - val_loss: 0.0215\n",
            "Epoch 2/50\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0020 - val_loss: 0.0170\n",
            "Epoch 3/50\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0010 - val_loss: 0.0146\n",
            "Epoch 4/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 7.3354e-04 - val_loss: 0.0138\n",
            "Epoch 5/50\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 6.4612e-04 - val_loss: 0.0143\n",
            "Epoch 6/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 4.9885e-04 - val_loss: 0.0136\n",
            "Epoch 7/50\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 4.4243e-04 - val_loss: 0.0129\n",
            "Epoch 8/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 4.1606e-04 - val_loss: 0.0125\n",
            "Epoch 9/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 4.1576e-04 - val_loss: 0.0119\n",
            "Epoch 10/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 3.8547e-04 - val_loss: 0.0114\n",
            "Epoch 11/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 3.3976e-04 - val_loss: 0.0110\n",
            "Epoch 12/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 3.7429e-04 - val_loss: 0.0099\n",
            "Epoch 13/50\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 2.7770e-04 - val_loss: 0.0098\n",
            "Epoch 14/50\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 3.1416e-04 - val_loss: 0.0094\n",
            "Epoch 15/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 2.7600e-04 - val_loss: 0.0092\n",
            "Epoch 16/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 2.9841e-04 - val_loss: 0.0093\n",
            "Epoch 17/50\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 2.6472e-04 - val_loss: 0.0080\n",
            "Epoch 18/50\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 2.3437e-04 - val_loss: 0.0080\n",
            "Epoch 19/50\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 2.4976e-04 - val_loss: 0.0079\n",
            "Epoch 20/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 2.4646e-04 - val_loss: 0.0076\n",
            "Epoch 21/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 2.4755e-04 - val_loss: 0.0075\n",
            "Epoch 22/50\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 2.4182e-04 - val_loss: 0.0082\n",
            "Epoch 23/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 2.1573e-04 - val_loss: 0.0072\n",
            "Epoch 24/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 2.2252e-04 - val_loss: 0.0071\n",
            "Epoch 25/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 2.4933e-04 - val_loss: 0.0066\n",
            "Epoch 26/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 2.5056e-04 - val_loss: 0.0069\n",
            "Epoch 27/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 2.1769e-04 - val_loss: 0.0064\n",
            "Epoch 28/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 1.8988e-04 - val_loss: 0.0063\n",
            "Epoch 29/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 1.8424e-04 - val_loss: 0.0061\n",
            "Epoch 30/50\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 2.0350e-04 - val_loss: 0.0064\n",
            "Epoch 31/50\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 1.7655e-04 - val_loss: 0.0060\n",
            "Epoch 32/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 1.7292e-04 - val_loss: 0.0060\n",
            "Epoch 33/50\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 1.8367e-04 - val_loss: 0.0060\n",
            "Epoch 34/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 1.8927e-04 - val_loss: 0.0060\n",
            "Epoch 35/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 2.0463e-04 - val_loss: 0.0059\n",
            "Epoch 36/50\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 2.0335e-04 - val_loss: 0.0060\n",
            "Epoch 37/50\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 2.1250e-04 - val_loss: 0.0060\n",
            "Epoch 38/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 1.7328e-04 - val_loss: 0.0060\n",
            "Epoch 39/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 1.9005e-04 - val_loss: 0.0061\n",
            "Epoch 40/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 1.5858e-04 - val_loss: 0.0064\n",
            "6/6 [==============================] - 0s 3ms/step\n",
            "Running for Adam optimizer 0.001 learning_rate 4 batch_size and 1 replicate \n",
            "\n",
            "Epoch 1/50\n",
            "171/171 [==============================] - 3s 8ms/step - loss: 0.1037 - val_loss: 0.0311\n",
            "Epoch 2/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 0.0049 - val_loss: 0.0239\n",
            "Epoch 3/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 0.0014 - val_loss: 0.0208\n",
            "Epoch 4/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 9.4520e-04 - val_loss: 0.0184\n",
            "Epoch 5/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 7.6236e-04 - val_loss: 0.0172\n",
            "Epoch 6/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 6.8391e-04 - val_loss: 0.0147\n",
            "Epoch 7/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 5.7621e-04 - val_loss: 0.0131\n",
            "Epoch 8/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 5.5811e-04 - val_loss: 0.0124\n",
            "Epoch 9/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 5.0166e-04 - val_loss: 0.0121\n",
            "Epoch 10/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 4.2837e-04 - val_loss: 0.0108\n",
            "Epoch 11/50\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 3.9246e-04 - val_loss: 0.0104\n",
            "Epoch 12/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 3.4036e-04 - val_loss: 0.0094\n",
            "Epoch 13/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 3.4102e-04 - val_loss: 0.0091\n",
            "Epoch 14/50\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 2.9268e-04 - val_loss: 0.0084\n",
            "Epoch 15/50\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 2.7427e-04 - val_loss: 0.0081\n",
            "Epoch 16/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 2.8789e-04 - val_loss: 0.0078\n",
            "Epoch 17/50\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 2.4589e-04 - val_loss: 0.0072\n",
            "Epoch 18/50\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 2.6214e-04 - val_loss: 0.0074\n",
            "Epoch 19/50\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 2.2243e-04 - val_loss: 0.0069\n",
            "Epoch 20/50\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 2.2510e-04 - val_loss: 0.0074\n",
            "Epoch 21/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 2.1619e-04 - val_loss: 0.0068\n",
            "Epoch 22/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 1.9105e-04 - val_loss: 0.0065\n",
            "Epoch 23/50\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 2.0614e-04 - val_loss: 0.0062\n",
            "Epoch 24/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 1.9039e-04 - val_loss: 0.0059\n",
            "Epoch 25/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 1.8782e-04 - val_loss: 0.0062\n",
            "Epoch 26/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 2.5204e-04 - val_loss: 0.0058\n",
            "Epoch 27/50\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 2.2643e-04 - val_loss: 0.0056\n",
            "Epoch 28/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 1.8717e-04 - val_loss: 0.0059\n",
            "Epoch 29/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 1.6669e-04 - val_loss: 0.0061\n",
            "Epoch 30/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 1.9557e-04 - val_loss: 0.0057\n",
            "Epoch 31/50\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 1.5561e-04 - val_loss: 0.0059\n",
            "Epoch 32/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 1.8365e-04 - val_loss: 0.0053\n",
            "Epoch 33/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 1.5958e-04 - val_loss: 0.0056\n",
            "Epoch 34/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 1.8384e-04 - val_loss: 0.0057\n",
            "Epoch 35/50\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 1.7255e-04 - val_loss: 0.0058\n",
            "Epoch 36/50\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 1.5046e-04 - val_loss: 0.0058\n",
            "Epoch 37/50\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 1.7465e-04 - val_loss: 0.0056\n",
            "6/6 [==============================] - 0s 4ms/step\n",
            "Running for Adam optimizer 0.001 learning_rate 8 batch_size and 0 replicate \n",
            "\n",
            "Epoch 1/50\n",
            "86/86 [==============================] - 9s 12ms/step - loss: 0.0486 - val_loss: 0.0338\n",
            "Epoch 2/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0044 - val_loss: 0.0179\n",
            "Epoch 3/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0018 - val_loss: 0.0157\n",
            "Epoch 4/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0010 - val_loss: 0.0140\n",
            "Epoch 5/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 7.3089e-04 - val_loss: 0.0131\n",
            "Epoch 6/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 5.4066e-04 - val_loss: 0.0121\n",
            "Epoch 7/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 4.6899e-04 - val_loss: 0.0114\n",
            "Epoch 8/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 4.1625e-04 - val_loss: 0.0111\n",
            "Epoch 9/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 3.7941e-04 - val_loss: 0.0096\n",
            "Epoch 10/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 3.3441e-04 - val_loss: 0.0093\n",
            "Epoch 11/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 3.0781e-04 - val_loss: 0.0090\n",
            "Epoch 12/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 2.8987e-04 - val_loss: 0.0087\n",
            "Epoch 13/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 2.7804e-04 - val_loss: 0.0080\n",
            "Epoch 14/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 2.7260e-04 - val_loss: 0.0079\n",
            "Epoch 15/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 2.6418e-04 - val_loss: 0.0077\n",
            "Epoch 16/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 2.5009e-04 - val_loss: 0.0072\n",
            "Epoch 17/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 2.3590e-04 - val_loss: 0.0072\n",
            "Epoch 18/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 2.8151e-04 - val_loss: 0.0076\n",
            "Epoch 19/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 2.6320e-04 - val_loss: 0.0070\n",
            "Epoch 20/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 2.3619e-04 - val_loss: 0.0070\n",
            "Epoch 21/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 2.2871e-04 - val_loss: 0.0070\n",
            "Epoch 22/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 2.3581e-04 - val_loss: 0.0067\n",
            "Epoch 23/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 2.3656e-04 - val_loss: 0.0066\n",
            "Epoch 24/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 2.1101e-04 - val_loss: 0.0065\n",
            "Epoch 25/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 1.9770e-04 - val_loss: 0.0059\n",
            "Epoch 26/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 2.1597e-04 - val_loss: 0.0063\n",
            "Epoch 27/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 2.0001e-04 - val_loss: 0.0061\n",
            "Epoch 28/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 2.0150e-04 - val_loss: 0.0058\n",
            "Epoch 29/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 1.9592e-04 - val_loss: 0.0057\n",
            "Epoch 30/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 1.7866e-04 - val_loss: 0.0057\n",
            "Epoch 31/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 1.7357e-04 - val_loss: 0.0062\n",
            "Epoch 32/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 2.1517e-04 - val_loss: 0.0052\n",
            "Epoch 33/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 1.9007e-04 - val_loss: 0.0053\n",
            "Epoch 34/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 1.8988e-04 - val_loss: 0.0055\n",
            "Epoch 35/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 1.6707e-04 - val_loss: 0.0052\n",
            "Epoch 36/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 1.9362e-04 - val_loss: 0.0056\n",
            "Epoch 37/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 1.7441e-04 - val_loss: 0.0054\n",
            "Epoch 38/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 1.7082e-04 - val_loss: 0.0053\n",
            "Epoch 39/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 1.6537e-04 - val_loss: 0.0052\n",
            "Epoch 40/50\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 1.6934e-04 - val_loss: 0.0050\n",
            "Epoch 41/50\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 1.6116e-04 - val_loss: 0.0051\n",
            "Epoch 42/50\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 1.6314e-04 - val_loss: 0.0047\n",
            "Epoch 43/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 1.7423e-04 - val_loss: 0.0052\n",
            "Epoch 44/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 1.5587e-04 - val_loss: 0.0050\n",
            "Epoch 45/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 1.7107e-04 - val_loss: 0.0047\n",
            "Epoch 46/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 1.7069e-04 - val_loss: 0.0048\n",
            "Epoch 47/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 1.4163e-04 - val_loss: 0.0050\n",
            "Epoch 48/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 1.4616e-04 - val_loss: 0.0048\n",
            "Epoch 49/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 1.3662e-04 - val_loss: 0.0050\n",
            "Epoch 50/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 1.5165e-04 - val_loss: 0.0048\n",
            "6/6 [==============================] - 0s 4ms/step\n",
            "Running for Adam optimizer 0.001 learning_rate 8 batch_size and 1 replicate \n",
            "\n",
            "Epoch 1/50\n",
            "86/86 [==============================] - 3s 11ms/step - loss: 0.0953 - val_loss: 0.1422\n",
            "Epoch 2/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 0.0188 - val_loss: 0.0716\n",
            "Epoch 3/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 0.0022 - val_loss: 0.0534\n",
            "Epoch 4/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 0.0013 - val_loss: 0.0514\n",
            "Epoch 5/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0011 - val_loss: 0.0468\n",
            "Epoch 6/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 9.9058e-04 - val_loss: 0.0457\n",
            "Epoch 7/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 8.8079e-04 - val_loss: 0.0434\n",
            "Epoch 8/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 7.9346e-04 - val_loss: 0.0402\n",
            "Epoch 9/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 7.3905e-04 - val_loss: 0.0384\n",
            "Epoch 10/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 6.9647e-04 - val_loss: 0.0386\n",
            "Epoch 11/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 6.3410e-04 - val_loss: 0.0354\n",
            "Epoch 12/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 5.8770e-04 - val_loss: 0.0345\n",
            "Epoch 13/50\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 6.1357e-04 - val_loss: 0.0336\n",
            "Epoch 14/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 5.3298e-04 - val_loss: 0.0330\n",
            "Epoch 15/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 4.8869e-04 - val_loss: 0.0323\n",
            "Epoch 16/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 4.9878e-04 - val_loss: 0.0303\n",
            "Epoch 17/50\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 4.6675e-04 - val_loss: 0.0302\n",
            "Epoch 18/50\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 4.2688e-04 - val_loss: 0.0296\n",
            "Epoch 19/50\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 4.2244e-04 - val_loss: 0.0291\n",
            "Epoch 20/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 3.9890e-04 - val_loss: 0.0294\n",
            "Epoch 21/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 3.7390e-04 - val_loss: 0.0283\n",
            "Epoch 22/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 3.3692e-04 - val_loss: 0.0275\n",
            "Epoch 23/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 3.4055e-04 - val_loss: 0.0267\n",
            "Epoch 24/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 3.3410e-04 - val_loss: 0.0264\n",
            "Epoch 25/50\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 3.2326e-04 - val_loss: 0.0259\n",
            "Epoch 26/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 3.2696e-04 - val_loss: 0.0256\n",
            "Epoch 27/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 2.9732e-04 - val_loss: 0.0252\n",
            "Epoch 28/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 2.8001e-04 - val_loss: 0.0243\n",
            "Epoch 29/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 2.7044e-04 - val_loss: 0.0241\n",
            "Epoch 30/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 2.7252e-04 - val_loss: 0.0241\n",
            "Epoch 31/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 2.5220e-04 - val_loss: 0.0234\n",
            "Epoch 32/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 2.6398e-04 - val_loss: 0.0230\n",
            "Epoch 33/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 2.2824e-04 - val_loss: 0.0227\n",
            "Epoch 34/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 2.4507e-04 - val_loss: 0.0221\n",
            "Epoch 35/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 2.1414e-04 - val_loss: 0.0218\n",
            "Epoch 36/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 2.1102e-04 - val_loss: 0.0216\n",
            "Epoch 37/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 2.0123e-04 - val_loss: 0.0212\n",
            "Epoch 38/50\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 1.9838e-04 - val_loss: 0.0212\n",
            "Epoch 39/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 2.0298e-04 - val_loss: 0.0216\n",
            "Epoch 40/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 2.0671e-04 - val_loss: 0.0212\n",
            "Epoch 41/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 1.9900e-04 - val_loss: 0.0217\n",
            "Epoch 42/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 2.0352e-04 - val_loss: 0.0207\n",
            "Epoch 43/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 1.8109e-04 - val_loss: 0.0210\n",
            "Epoch 44/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 1.9623e-04 - val_loss: 0.0210\n",
            "Epoch 45/50\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 1.9012e-04 - val_loss: 0.0207\n",
            "Epoch 46/50\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 2.2949e-04 - val_loss: 0.0211\n",
            "Epoch 47/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 1.8850e-04 - val_loss: 0.0215\n",
            "Epoch 48/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 1.6896e-04 - val_loss: 0.0222\n",
            "Epoch 49/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 1.8981e-04 - val_loss: 0.0215\n",
            "Epoch 50/50\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 1.7808e-04 - val_loss: 0.0209\n",
            "6/6 [==============================] - 0s 4ms/step\n",
            "Running for Adam optimizer 0.001 learning_rate 16 batch_size and 0 replicate \n",
            "\n",
            "Epoch 1/50\n",
            "43/43 [==============================] - 3s 17ms/step - loss: 0.0642 - val_loss: 0.0568\n",
            "Epoch 2/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0122 - val_loss: 0.0258\n",
            "Epoch 3/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0048 - val_loss: 0.0230\n",
            "Epoch 4/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0028 - val_loss: 0.0220\n",
            "Epoch 5/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0017 - val_loss: 0.0211\n",
            "Epoch 6/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0013 - val_loss: 0.0206\n",
            "Epoch 7/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0010 - val_loss: 0.0207\n",
            "Epoch 8/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 9.0178e-04 - val_loss: 0.0206\n",
            "Epoch 9/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 8.5199e-04 - val_loss: 0.0202\n",
            "Epoch 10/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 8.0374e-04 - val_loss: 0.0198\n",
            "Epoch 11/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 7.8857e-04 - val_loss: 0.0201\n",
            "Epoch 12/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 7.5381e-04 - val_loss: 0.0199\n",
            "Epoch 13/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 7.3176e-04 - val_loss: 0.0190\n",
            "Epoch 14/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 7.0564e-04 - val_loss: 0.0191\n",
            "Epoch 15/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 6.8409e-04 - val_loss: 0.0185\n",
            "Epoch 16/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 6.5389e-04 - val_loss: 0.0180\n",
            "Epoch 17/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 6.6190e-04 - val_loss: 0.0179\n",
            "Epoch 18/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 6.1070e-04 - val_loss: 0.0174\n",
            "Epoch 19/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 5.8839e-04 - val_loss: 0.0169\n",
            "Epoch 20/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 5.6958e-04 - val_loss: 0.0169\n",
            "Epoch 21/50\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 5.6951e-04 - val_loss: 0.0166\n",
            "Epoch 22/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 5.3834e-04 - val_loss: 0.0166\n",
            "Epoch 23/50\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5.1558e-04 - val_loss: 0.0158\n",
            "Epoch 24/50\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 5.2998e-04 - val_loss: 0.0156\n",
            "Epoch 25/50\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 4.8795e-04 - val_loss: 0.0152\n",
            "Epoch 26/50\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.6450e-04 - val_loss: 0.0151\n",
            "Epoch 27/50\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 4.6559e-04 - val_loss: 0.0147\n",
            "Epoch 28/50\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 4.4683e-04 - val_loss: 0.0143\n",
            "Epoch 29/50\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 4.4140e-04 - val_loss: 0.0141\n",
            "Epoch 30/50\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.4418e-04 - val_loss: 0.0139\n",
            "Epoch 31/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 4.1049e-04 - val_loss: 0.0137\n",
            "Epoch 32/50\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 3.9284e-04 - val_loss: 0.0133\n",
            "Epoch 33/50\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 3.7881e-04 - val_loss: 0.0129\n",
            "Epoch 34/50\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 3.6076e-04 - val_loss: 0.0126\n",
            "Epoch 35/50\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 3.5571e-04 - val_loss: 0.0124\n",
            "Epoch 36/50\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 3.4988e-04 - val_loss: 0.0124\n",
            "Epoch 37/50\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 3.2071e-04 - val_loss: 0.0127\n",
            "Epoch 38/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 3.2375e-04 - val_loss: 0.0124\n",
            "Epoch 39/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 3.0826e-04 - val_loss: 0.0119\n",
            "Epoch 40/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 3.0267e-04 - val_loss: 0.0119\n",
            "Epoch 41/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 3.0095e-04 - val_loss: 0.0118\n",
            "Epoch 42/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 2.7843e-04 - val_loss: 0.0115\n",
            "Epoch 43/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 2.7398e-04 - val_loss: 0.0112\n",
            "Epoch 44/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 2.5684e-04 - val_loss: 0.0111\n",
            "Epoch 45/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 2.5145e-04 - val_loss: 0.0109\n",
            "Epoch 46/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 2.3953e-04 - val_loss: 0.0109\n",
            "Epoch 47/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 2.5276e-04 - val_loss: 0.0108\n",
            "Epoch 48/50\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 2.2895e-04 - val_loss: 0.0106\n",
            "Epoch 49/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 2.3305e-04 - val_loss: 0.0101\n",
            "Epoch 50/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 2.3660e-04 - val_loss: 0.0103\n",
            "6/6 [==============================] - 0s 4ms/step\n",
            "Running for Adam optimizer 0.001 learning_rate 16 batch_size and 1 replicate \n",
            "\n",
            "Epoch 1/50\n",
            "43/43 [==============================] - 3s 17ms/step - loss: 0.0858 - val_loss: 0.0978\n",
            "Epoch 2/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0187 - val_loss: 0.0456\n",
            "Epoch 3/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0067 - val_loss: 0.0345\n",
            "Epoch 4/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0039 - val_loss: 0.0314\n",
            "Epoch 5/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0027 - val_loss: 0.0306\n",
            "Epoch 6/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0021 - val_loss: 0.0301\n",
            "Epoch 7/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0018 - val_loss: 0.0287\n",
            "Epoch 8/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0016 - val_loss: 0.0280\n",
            "Epoch 9/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0014 - val_loss: 0.0271\n",
            "Epoch 10/50\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0254\n",
            "Epoch 11/50\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0011 - val_loss: 0.0248\n",
            "Epoch 12/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0010 - val_loss: 0.0239\n",
            "Epoch 13/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 9.3804e-04 - val_loss: 0.0233\n",
            "Epoch 14/50\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 8.9286e-04 - val_loss: 0.0226\n",
            "Epoch 15/50\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 7.9210e-04 - val_loss: 0.0220\n",
            "Epoch 16/50\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 7.2130e-04 - val_loss: 0.0212\n",
            "Epoch 17/50\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 7.1569e-04 - val_loss: 0.0203\n",
            "Epoch 18/50\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 6.6757e-04 - val_loss: 0.0202\n",
            "Epoch 19/50\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5.9964e-04 - val_loss: 0.0196\n",
            "Epoch 20/50\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 5.5979e-04 - val_loss: 0.0189\n",
            "Epoch 21/50\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5.2463e-04 - val_loss: 0.0182\n",
            "Epoch 22/50\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5.0729e-04 - val_loss: 0.0176\n",
            "Epoch 23/50\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5.0662e-04 - val_loss: 0.0170\n",
            "Epoch 24/50\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.9521e-04 - val_loss: 0.0166\n",
            "Epoch 25/50\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 4.7518e-04 - val_loss: 0.0162\n",
            "Epoch 26/50\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 4.4305e-04 - val_loss: 0.0161\n",
            "Epoch 27/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 4.2997e-04 - val_loss: 0.0153\n",
            "Epoch 28/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 4.1994e-04 - val_loss: 0.0152\n",
            "Epoch 29/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 4.0229e-04 - val_loss: 0.0149\n",
            "Epoch 30/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 3.9300e-04 - val_loss: 0.0146\n",
            "Epoch 31/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 3.7360e-04 - val_loss: 0.0141\n",
            "Epoch 32/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 3.6766e-04 - val_loss: 0.0142\n",
            "Epoch 33/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 3.6228e-04 - val_loss: 0.0132\n",
            "Epoch 34/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 3.4979e-04 - val_loss: 0.0132\n",
            "Epoch 35/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 3.2414e-04 - val_loss: 0.0129\n",
            "Epoch 36/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 3.2380e-04 - val_loss: 0.0127\n",
            "Epoch 37/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 3.0323e-04 - val_loss: 0.0120\n",
            "Epoch 38/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 3.2955e-04 - val_loss: 0.0121\n",
            "Epoch 39/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 2.8712e-04 - val_loss: 0.0125\n",
            "Epoch 40/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 2.9727e-04 - val_loss: 0.0115\n",
            "Epoch 41/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 2.7743e-04 - val_loss: 0.0110\n",
            "Epoch 42/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 2.6412e-04 - val_loss: 0.0107\n",
            "Epoch 43/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 2.6596e-04 - val_loss: 0.0105\n",
            "Epoch 44/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 2.6004e-04 - val_loss: 0.0097\n",
            "Epoch 45/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 2.7076e-04 - val_loss: 0.0098\n",
            "Epoch 46/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 2.7784e-04 - val_loss: 0.0099\n",
            "Epoch 47/50\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 2.6002e-04 - val_loss: 0.0102\n",
            "Epoch 48/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 2.2820e-04 - val_loss: 0.0098\n",
            "Epoch 49/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 2.4105e-04 - val_loss: 0.0091\n",
            "Epoch 50/50\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 2.2099e-04 - val_loss: 0.0091\n",
            "6/6 [==============================] - 0s 5ms/step\n",
            "Best_hyper_parameters(LSTM): \n",
            " {'model': [8], 'optimizer': 'Adam', 'learning_rate': 0.01, 'batch_size': 4, 'best_avg_rmse': 1908.2096132704578}\n",
            "all_avg_rmse(LSTM): \n",
            " [[[2634.48839573 2164.00788846 2065.30184846]\n",
            "  [1908.20961327 1957.78524958 2012.28742085]\n",
            "  [2167.27785806 2430.05691096 2597.2127592 ]]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model': [8],\n",
              " 'optimizer': 'Adam',\n",
              " 'learning_rate': 0.01,\n",
              " 'batch_size': 4,\n",
              " 'best_avg_rmse': 1908.2096132704578}"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ],
      "source": [
        "layers = [8]\n",
        "time_step = 5\n",
        "optimizers_names = ['Adam', 'Adagrad']\n",
        "#optimizers_names = ['Adam']\n",
        "learning_rates =  [0.1, 0.01, 0.001]\n",
        "batch_sizes =  [4, 8, 16]\n",
        "epochs = 75\n",
        "num_replicates = 10\n",
        "test_split = 0.2\n",
        "val_split = 0.2\n",
        "\n",
        "train_data, test_data = data_split(data, test_split)\n",
        "\n",
        "lstm_N8_best_hyper_parameters = LSTM_Hyper_Parameter_Tuning(layers, train_data, time_step, val_split, optimizers_names, learning_rates, batch_sizes, epochs = epochs, num_replicates = num_replicates)\n",
        "lstm_N8_best_hyper_parameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Case II: 16N-LSTM**"
      ],
      "metadata": {
        "id": "golVoBNXwUye"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eOdLEi2AbCdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layers = [16]\n",
        "time_step = 5\n",
        "optimizers_names = ['Adam', 'Adagrad']\n",
        "learning_rates =  [0.1, 0.01, 0.001]\n",
        "batch_sizes =  [4, 8, 16]\n",
        "epochs = 75\n",
        "num_replicates = 10\n",
        "test_split = 0.2\n",
        "val_split = 0.2\n",
        "\n",
        "train_data, test_data = data_split(data, test_split)\n",
        "\n",
        "lstm_N16_best_hyper_parameters = LSTM_Hyper_Parameter_Tuning(layers, train_data, time_step, val_split, optimizers_names, learning_rates, batch_sizes, epochs = epochs, num_replicates = num_replicates)\n",
        "lstm_N16_best_hyper_parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1u1fZ2VwUTB",
        "outputId": "6405b425-a49d-422e-a38d-a07c998fac7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 8.5264e-04 - val_loss: 0.0055\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 8.3261e-04 - val_loss: 0.0051\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 8.1250e-04 - val_loss: 0.0052\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 7.9375e-04 - val_loss: 0.0047\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 7.7849e-04 - val_loss: 0.0048\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 7.5924e-04 - val_loss: 0.0046\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 7.3985e-04 - val_loss: 0.0047\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 7.1965e-04 - val_loss: 0.0047\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 7.0300e-04 - val_loss: 0.0042\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 6.9467e-04 - val_loss: 0.0045\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.7542e-04 - val_loss: 0.0042\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.5898e-04 - val_loss: 0.0041\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.4372e-04 - val_loss: 0.0041\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 6.2768e-04 - val_loss: 0.0040\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.1954e-04 - val_loss: 0.0039\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.0840e-04 - val_loss: 0.0039\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 6.0062e-04 - val_loss: 0.0039\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 5.8744e-04 - val_loss: 0.0036\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 5.8320e-04 - val_loss: 0.0036\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 5.6454e-04 - val_loss: 0.0036\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 5.5298e-04 - val_loss: 0.0036\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 5.4806e-04 - val_loss: 0.0034\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 5.3952e-04 - val_loss: 0.0034\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 5.3270e-04 - val_loss: 0.0035\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 5.2550e-04 - val_loss: 0.0035\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 5.1203e-04 - val_loss: 0.0035\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 5.1316e-04 - val_loss: 0.0034\n",
            "6/6 [==============================] - 0s 6ms/step\n",
            "Running for Adagrad optimizer 0.01 learning_rate 16 batch_size and 2 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 2s 16ms/step - loss: 0.0752 - val_loss: 0.0430\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0200 - val_loss: 0.0271\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0114 - val_loss: 0.0215\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0086 - val_loss: 0.0209\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0074 - val_loss: 0.0201\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0068 - val_loss: 0.0200\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0063 - val_loss: 0.0201\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0059 - val_loss: 0.0189\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0055 - val_loss: 0.0183\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0052 - val_loss: 0.0190\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0049 - val_loss: 0.0178\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0046 - val_loss: 0.0184\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0044 - val_loss: 0.0186\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0041 - val_loss: 0.0172\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0039 - val_loss: 0.0180\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0037 - val_loss: 0.0179\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0035 - val_loss: 0.0171\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0033 - val_loss: 0.0178\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0032 - val_loss: 0.0176\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0030 - val_loss: 0.0157\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0166\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0028 - val_loss: 0.0171\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0159\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0025 - val_loss: 0.0154\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0024 - val_loss: 0.0150\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0023 - val_loss: 0.0145\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0022 - val_loss: 0.0145\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0021 - val_loss: 0.0143\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0141\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0019 - val_loss: 0.0133\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0019 - val_loss: 0.0132\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0144\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0128\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0017 - val_loss: 0.0127\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0125\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0121\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0016 - val_loss: 0.0123\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0116\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0015 - val_loss: 0.0112\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0110\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0014 - val_loss: 0.0114\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0013 - val_loss: 0.0104\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0103\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0013 - val_loss: 0.0101\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0104\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0012 - val_loss: 0.0102\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0096\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0011 - val_loss: 0.0094\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0011 - val_loss: 0.0092\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0011 - val_loss: 0.0087\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0010 - val_loss: 0.0086\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0010 - val_loss: 0.0084\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 9.7657e-04 - val_loss: 0.0085\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 9.4627e-04 - val_loss: 0.0081\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 9.2268e-04 - val_loss: 0.0082\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 8.9057e-04 - val_loss: 0.0078\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 8.7262e-04 - val_loss: 0.0077\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 8.4966e-04 - val_loss: 0.0075\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 8.3171e-04 - val_loss: 0.0074\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 8.0927e-04 - val_loss: 0.0073\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 7.8669e-04 - val_loss: 0.0072\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 7.7157e-04 - val_loss: 0.0072\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 7.5279e-04 - val_loss: 0.0072\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 7.3678e-04 - val_loss: 0.0068\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 7.1383e-04 - val_loss: 0.0066\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 6.9689e-04 - val_loss: 0.0064\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.8275e-04 - val_loss: 0.0064\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 6.7087e-04 - val_loss: 0.0066\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.4839e-04 - val_loss: 0.0064\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.3927e-04 - val_loss: 0.0062\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.2671e-04 - val_loss: 0.0063\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.1362e-04 - val_loss: 0.0063\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5.9675e-04 - val_loss: 0.0060\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 5.8698e-04 - val_loss: 0.0062\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 5.7553e-04 - val_loss: 0.0058\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adagrad optimizer 0.01 learning_rate 16 batch_size and 3 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 3s 16ms/step - loss: 0.0978 - val_loss: 0.0379\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0352 - val_loss: 0.0219\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0236 - val_loss: 0.0155\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0162 - val_loss: 0.0105\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0116 - val_loss: 0.0083\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0090 - val_loss: 0.0064\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0074 - val_loss: 0.0055\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0064 - val_loss: 0.0048\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0055 - val_loss: 0.0046\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0049 - val_loss: 0.0043\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0043 - val_loss: 0.0043\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0039 - val_loss: 0.0042\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0042\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0033 - val_loss: 0.0042\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0030 - val_loss: 0.0042\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0028 - val_loss: 0.0042\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0026 - val_loss: 0.0041\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0024 - val_loss: 0.0041\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0023 - val_loss: 0.0042\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0021 - val_loss: 0.0041\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0042\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0019 - val_loss: 0.0041\n",
            "6/6 [==============================] - 0s 3ms/step\n",
            "Running for Adagrad optimizer 0.01 learning_rate 16 batch_size and 4 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 6s 95ms/step - loss: 0.0410 - val_loss: 0.0297\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0190 - val_loss: 0.0222\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0132 - val_loss: 0.0170\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0088 - val_loss: 0.0132\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0063 - val_loss: 0.0117\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0052 - val_loss: 0.0108\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0047 - val_loss: 0.0108\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0044 - val_loss: 0.0104\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0042 - val_loss: 0.0104\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0040 - val_loss: 0.0105\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0038 - val_loss: 0.0102\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0037 - val_loss: 0.0102\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0036 - val_loss: 0.0104\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0034 - val_loss: 0.0101\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0033 - val_loss: 0.0101\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0032 - val_loss: 0.0102\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0031 - val_loss: 0.0103\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0030 - val_loss: 0.0101\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0029 - val_loss: 0.0100\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0028 - val_loss: 0.0097\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0027 - val_loss: 0.0097\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0026 - val_loss: 0.0098\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0026 - val_loss: 0.0098\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0025 - val_loss: 0.0095\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0024 - val_loss: 0.0095\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0024 - val_loss: 0.0096\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0023 - val_loss: 0.0094\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0022 - val_loss: 0.0091\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0022 - val_loss: 0.0090\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0094\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0088\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0020 - val_loss: 0.0086\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0020 - val_loss: 0.0089\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0019 - val_loss: 0.0088\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0019 - val_loss: 0.0088\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0086\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0018 - val_loss: 0.0085\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0018 - val_loss: 0.0083\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0080\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0080\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0079\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0016 - val_loss: 0.0077\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0077\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0016 - val_loss: 0.0075\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0015 - val_loss: 0.0076\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 0.0074\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0015 - val_loss: 0.0075\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 12ms/step - loss: 0.0014 - val_loss: 0.0070\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0069\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0069\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0069\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0013 - val_loss: 0.0066\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0013 - val_loss: 0.0064\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0065\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0064\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0065\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0061\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0012 - val_loss: 0.0060\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0012 - val_loss: 0.0061\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0011 - val_loss: 0.0061\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0011 - val_loss: 0.0058\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0011 - val_loss: 0.0056\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0011 - val_loss: 0.0057\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0011 - val_loss: 0.0055\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0011 - val_loss: 0.0055\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0010 - val_loss: 0.0054\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0010 - val_loss: 0.0052\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 9.8537e-04 - val_loss: 0.0054\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 9.7755e-04 - val_loss: 0.0051\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 9.5269e-04 - val_loss: 0.0050\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 9.3893e-04 - val_loss: 0.0050\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 9.2553e-04 - val_loss: 0.0050\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 9.0578e-04 - val_loss: 0.0049\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 8.9007e-04 - val_loss: 0.0048\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 8.7129e-04 - val_loss: 0.0046\n",
            "6/6 [==============================] - 0s 7ms/step\n",
            "Running for Adagrad optimizer 0.01 learning_rate 16 batch_size and 5 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 3s 17ms/step - loss: 0.1239 - val_loss: 0.0668\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0276 - val_loss: 0.0454\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0143 - val_loss: 0.0317\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0083 - val_loss: 0.0237\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0057 - val_loss: 0.0194\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0046 - val_loss: 0.0172\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0040 - val_loss: 0.0162\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0037 - val_loss: 0.0151\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0035 - val_loss: 0.0147\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0034 - val_loss: 0.0144\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0032 - val_loss: 0.0141\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0031 - val_loss: 0.0139\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0030 - val_loss: 0.0132\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0030 - val_loss: 0.0132\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0029 - val_loss: 0.0130\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0028 - val_loss: 0.0127\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0027 - val_loss: 0.0122\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0027 - val_loss: 0.0122\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0122\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0116\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0025 - val_loss: 0.0118\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0024 - val_loss: 0.0112\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0024 - val_loss: 0.0110\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0023 - val_loss: 0.0108\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0023 - val_loss: 0.0107\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0022 - val_loss: 0.0108\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0022 - val_loss: 0.0104\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0021 - val_loss: 0.0102\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0021 - val_loss: 0.0102\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0020 - val_loss: 0.0099\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0098\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0019 - val_loss: 0.0099\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0094\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0094\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0090\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0088\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0087\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0017 - val_loss: 0.0086\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0017 - val_loss: 0.0084\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0016 - val_loss: 0.0084\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0016 - val_loss: 0.0081\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0016 - val_loss: 0.0079\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0015 - val_loss: 0.0078\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0015 - val_loss: 0.0077\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0014 - val_loss: 0.0076\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0014 - val_loss: 0.0073\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0072\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0072\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0013 - val_loss: 0.0070\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0013 - val_loss: 0.0069\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0013 - val_loss: 0.0067\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0012 - val_loss: 0.0066\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0012 - val_loss: 0.0064\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0012 - val_loss: 0.0063\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0062\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0011 - val_loss: 0.0061\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0011 - val_loss: 0.0059\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0011 - val_loss: 0.0058\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0011 - val_loss: 0.0057\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0010 - val_loss: 0.0057\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0010 - val_loss: 0.0055\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 9.9249e-04 - val_loss: 0.0055\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 9.6664e-04 - val_loss: 0.0054\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 9.5144e-04 - val_loss: 0.0052\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 9.2786e-04 - val_loss: 0.0052\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 9.0524e-04 - val_loss: 0.0050\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 8.9482e-04 - val_loss: 0.0050\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 8.7085e-04 - val_loss: 0.0049\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 8.4713e-04 - val_loss: 0.0048\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 8.3192e-04 - val_loss: 0.0048\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 8.1570e-04 - val_loss: 0.0046\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 7.9448e-04 - val_loss: 0.0046\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 7.8275e-04 - val_loss: 0.0045\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 7.6873e-04 - val_loss: 0.0043\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 7.5091e-04 - val_loss: 0.0044\n",
            "6/6 [==============================] - 1s 3ms/step\n",
            "Running for Adagrad optimizer 0.01 learning_rate 16 batch_size and 6 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 3s 16ms/step - loss: 0.0835 - val_loss: 0.0638\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0187 - val_loss: 0.0427\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0102 - val_loss: 0.0306\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0067 - val_loss: 0.0239\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0051 - val_loss: 0.0199\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0043 - val_loss: 0.0176\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0038 - val_loss: 0.0162\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0035 - val_loss: 0.0152\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0033 - val_loss: 0.0145\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0031 - val_loss: 0.0139\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0029 - val_loss: 0.0134\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0028 - val_loss: 0.0130\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0027 - val_loss: 0.0127\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0026 - val_loss: 0.0124\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0025 - val_loss: 0.0121\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0024 - val_loss: 0.0120\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0024 - val_loss: 0.0115\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0023 - val_loss: 0.0114\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0022 - val_loss: 0.0110\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0022 - val_loss: 0.0111\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0021 - val_loss: 0.0107\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0021 - val_loss: 0.0106\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0104\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0020 - val_loss: 0.0102\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0100\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0019 - val_loss: 0.0097\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0018 - val_loss: 0.0096\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0094\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0093\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0017 - val_loss: 0.0090\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0087\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0016 - val_loss: 0.0088\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0084\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0015 - val_loss: 0.0083\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0015 - val_loss: 0.0083\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0080\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0078\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0014 - val_loss: 0.0076\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 0.0076\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0014 - val_loss: 0.0073\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0072\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0013 - val_loss: 0.0071\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0013 - val_loss: 0.0070\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0068\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0012 - val_loss: 0.0068\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0067\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0012 - val_loss: 0.0065\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0012 - val_loss: 0.0064\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0011 - val_loss: 0.0062\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 0.0062\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0011 - val_loss: 0.0061\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0011 - val_loss: 0.0059\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0010 - val_loss: 0.0058\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0010 - val_loss: 0.0057\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0010 - val_loss: 0.0057\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 9.9004e-04 - val_loss: 0.0056\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 9.6902e-04 - val_loss: 0.0054\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 9.5665e-04 - val_loss: 0.0053\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 9.4028e-04 - val_loss: 0.0053\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 9.2633e-04 - val_loss: 0.0052\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 9.0924e-04 - val_loss: 0.0051\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 8.9447e-04 - val_loss: 0.0050\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 8.7955e-04 - val_loss: 0.0049\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 8.6078e-04 - val_loss: 0.0048\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 8.4937e-04 - val_loss: 0.0048\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 8.3524e-04 - val_loss: 0.0048\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 8.2331e-04 - val_loss: 0.0046\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 8.0617e-04 - val_loss: 0.0046\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 7.9374e-04 - val_loss: 0.0045\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 7.8259e-04 - val_loss: 0.0044\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 7.7215e-04 - val_loss: 0.0044\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 7.5792e-04 - val_loss: 0.0043\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 7.4750e-04 - val_loss: 0.0041\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 7.3793e-04 - val_loss: 0.0041\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 7.2156e-04 - val_loss: 0.0041\n",
            "6/6 [==============================] - 0s 5ms/step\n",
            "Running for Adagrad optimizer 0.01 learning_rate 16 batch_size and 7 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 3s 23ms/step - loss: 0.0373 - val_loss: 0.0452\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0104 - val_loss: 0.0317\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0064 - val_loss: 0.0257\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0052 - val_loss: 0.0232\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0048 - val_loss: 0.0219\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0046 - val_loss: 0.0215\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0044 - val_loss: 0.0215\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0043 - val_loss: 0.0201\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0042 - val_loss: 0.0199\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0041 - val_loss: 0.0198\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0040 - val_loss: 0.0195\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0039 - val_loss: 0.0194\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0038 - val_loss: 0.0193\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0037 - val_loss: 0.0193\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0036 - val_loss: 0.0187\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0036 - val_loss: 0.0183\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0035 - val_loss: 0.0176\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0034 - val_loss: 0.0181\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0033 - val_loss: 0.0172\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0032 - val_loss: 0.0172\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0031 - val_loss: 0.0167\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0031 - val_loss: 0.0163\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0030 - val_loss: 0.0165\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0030 - val_loss: 0.0158\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0029 - val_loss: 0.0158\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0028 - val_loss: 0.0154\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0028 - val_loss: 0.0152\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0027 - val_loss: 0.0149\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0027 - val_loss: 0.0147\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0026 - val_loss: 0.0148\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0025 - val_loss: 0.0144\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0025 - val_loss: 0.0143\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0024 - val_loss: 0.0139\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0024 - val_loss: 0.0136\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0023 - val_loss: 0.0130\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0023 - val_loss: 0.0131\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0022 - val_loss: 0.0128\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0022 - val_loss: 0.0125\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0021 - val_loss: 0.0122\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0021 - val_loss: 0.0124\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0020 - val_loss: 0.0116\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0020 - val_loss: 0.0120\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0116\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0019 - val_loss: 0.0112\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0019 - val_loss: 0.0113\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0108\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0106\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0105\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0017 - val_loss: 0.0102\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0017 - val_loss: 0.0100\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0016 - val_loss: 0.0099\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0016 - val_loss: 0.0100\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0097\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0015 - val_loss: 0.0095\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0093\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0015 - val_loss: 0.0093\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0089\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 0.0091\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0014 - val_loss: 0.0086\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0085\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0083\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0013 - val_loss: 0.0082\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0012 - val_loss: 0.0080\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0080\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 0.0077\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0075\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 0.0073\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0011 - val_loss: 0.0072\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0011 - val_loss: 0.0070\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0011 - val_loss: 0.0071\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0010 - val_loss: 0.0067\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0010 - val_loss: 0.0069\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 9.8776e-04 - val_loss: 0.0066\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 9.7087e-04 - val_loss: 0.0063\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 9.4495e-04 - val_loss: 0.0063\n",
            "6/6 [==============================] - 0s 3ms/step\n",
            "Running for Adagrad optimizer 0.01 learning_rate 16 batch_size and 8 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 2s 16ms/step - loss: 0.1075 - val_loss: 0.0297\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0164 - val_loss: 0.0172\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0113 - val_loss: 0.0139\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0094 - val_loss: 0.0124\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0086 - val_loss: 0.0116\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0081 - val_loss: 0.0112\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0078 - val_loss: 0.0110\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0075 - val_loss: 0.0109\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0072 - val_loss: 0.0108\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0069 - val_loss: 0.0107\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0067 - val_loss: 0.0107\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0065 - val_loss: 0.0108\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0062 - val_loss: 0.0108\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0060 - val_loss: 0.0109\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0058 - val_loss: 0.0109\n",
            "6/6 [==============================] - 0s 5ms/step\n",
            "Running for Adagrad optimizer 0.01 learning_rate 16 batch_size and 9 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 2s 16ms/step - loss: 0.0539 - val_loss: 0.0324\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0135 - val_loss: 0.0217\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0079 - val_loss: 0.0160\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0057 - val_loss: 0.0139\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0048 - val_loss: 0.0124\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0043 - val_loss: 0.0120\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0040 - val_loss: 0.0117\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0037 - val_loss: 0.0110\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0035 - val_loss: 0.0116\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0034 - val_loss: 0.0113\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0032 - val_loss: 0.0116\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0031 - val_loss: 0.0112\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0029 - val_loss: 0.0111\n",
            "6/6 [==============================] - 0s 6ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 0 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 4s 11ms/step - loss: 0.1653 - val_loss: 0.1020\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0730 - val_loss: 0.0541\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0378 - val_loss: 0.0333\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0220 - val_loss: 0.0242\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0148 - val_loss: 0.0203\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0114 - val_loss: 0.0188\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0099 - val_loss: 0.0180\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0091 - val_loss: 0.0176\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0086 - val_loss: 0.0172\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0082 - val_loss: 0.0169\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0079 - val_loss: 0.0166\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0077 - val_loss: 0.0163\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0075 - val_loss: 0.0161\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0074 - val_loss: 0.0160\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0072 - val_loss: 0.0159\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0071 - val_loss: 0.0158\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0069 - val_loss: 0.0157\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0068 - val_loss: 0.0156\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0067 - val_loss: 0.0155\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0066 - val_loss: 0.0155\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0065 - val_loss: 0.0154\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0064 - val_loss: 0.0154\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0063 - val_loss: 0.0153\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0062 - val_loss: 0.0153\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0061 - val_loss: 0.0153\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0060 - val_loss: 0.0152\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0060 - val_loss: 0.0151\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0059 - val_loss: 0.0151\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0058 - val_loss: 0.0151\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0058 - val_loss: 0.0150\n",
            "Epoch 31/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0057 - val_loss: 0.0150\n",
            "Epoch 32/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0056 - val_loss: 0.0150\n",
            "Epoch 33/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0056 - val_loss: 0.0150\n",
            "Epoch 34/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0055 - val_loss: 0.0150\n",
            "Epoch 35/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0055 - val_loss: 0.0150\n",
            "Epoch 36/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0054 - val_loss: 0.0150\n",
            "Epoch 37/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0054 - val_loss: 0.0150\n",
            "Epoch 38/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0053 - val_loss: 0.0150\n",
            "Epoch 39/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0053 - val_loss: 0.0150\n",
            "Epoch 40/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0052 - val_loss: 0.0150\n",
            "Epoch 41/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0052 - val_loss: 0.0150\n",
            "Epoch 42/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0051 - val_loss: 0.0150\n",
            "Epoch 43/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0051 - val_loss: 0.0150\n",
            "Epoch 44/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0051 - val_loss: 0.0151\n",
            "Epoch 45/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0050 - val_loss: 0.0151\n",
            "6/6 [==============================] - 0s 3ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 1 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 3s 8ms/step - loss: 0.1392 - val_loss: 0.0840\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0657 - val_loss: 0.0483\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0375 - val_loss: 0.0349\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0258 - val_loss: 0.0295\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0203 - val_loss: 0.0267\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0171 - val_loss: 0.0248\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0149 - val_loss: 0.0232\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0133 - val_loss: 0.0218\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0119 - val_loss: 0.0205\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0108 - val_loss: 0.0194\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0098 - val_loss: 0.0184\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0090 - val_loss: 0.0175\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0084 - val_loss: 0.0168\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0078 - val_loss: 0.0162\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0074 - val_loss: 0.0156\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0069 - val_loss: 0.0150\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0066 - val_loss: 0.0145\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0062 - val_loss: 0.0140\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0059 - val_loss: 0.0137\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0057 - val_loss: 0.0133\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0055 - val_loss: 0.0131\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0053 - val_loss: 0.0129\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0051 - val_loss: 0.0126\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0050 - val_loss: 0.0125\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0049 - val_loss: 0.0123\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0047 - val_loss: 0.0122\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0046 - val_loss: 0.0121\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0045 - val_loss: 0.0120\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0045 - val_loss: 0.0119\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0044 - val_loss: 0.0118\n",
            "Epoch 31/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0043 - val_loss: 0.0117\n",
            "Epoch 32/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0042 - val_loss: 0.0117\n",
            "Epoch 33/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0042 - val_loss: 0.0117\n",
            "Epoch 34/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0041 - val_loss: 0.0116\n",
            "Epoch 35/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0041 - val_loss: 0.0116\n",
            "Epoch 36/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0040 - val_loss: 0.0115\n",
            "Epoch 37/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0114\n",
            "Epoch 38/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0039 - val_loss: 0.0114\n",
            "Epoch 39/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0039 - val_loss: 0.0114\n",
            "Epoch 40/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0038 - val_loss: 0.0113\n",
            "Epoch 41/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0038 - val_loss: 0.0113\n",
            "Epoch 42/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0113\n",
            "Epoch 43/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0037 - val_loss: 0.0112\n",
            "Epoch 44/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0036 - val_loss: 0.0112\n",
            "Epoch 45/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0036 - val_loss: 0.0112\n",
            "Epoch 46/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0036 - val_loss: 0.0112\n",
            "Epoch 47/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0035 - val_loss: 0.0111\n",
            "Epoch 48/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0035 - val_loss: 0.0111\n",
            "Epoch 49/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0035 - val_loss: 0.0111\n",
            "Epoch 50/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0034 - val_loss: 0.0111\n",
            "Epoch 51/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0034 - val_loss: 0.0111\n",
            "Epoch 52/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0034 - val_loss: 0.0110\n",
            "Epoch 53/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0110\n",
            "Epoch 54/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0110\n",
            "Epoch 55/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0110\n",
            "Epoch 56/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0110\n",
            "Epoch 57/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0032 - val_loss: 0.0109\n",
            "Epoch 58/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0109\n",
            "Epoch 59/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0109\n",
            "Epoch 60/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0031 - val_loss: 0.0109\n",
            "Epoch 61/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0109\n",
            "Epoch 62/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0031 - val_loss: 0.0109\n",
            "Epoch 63/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0031 - val_loss: 0.0109\n",
            "Epoch 64/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0109\n",
            "Epoch 65/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0030 - val_loss: 0.0109\n",
            "Epoch 66/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0030 - val_loss: 0.0108\n",
            "Epoch 67/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0030 - val_loss: 0.0108\n",
            "Epoch 68/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0029 - val_loss: 0.0108\n",
            "Epoch 69/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0108\n",
            "Epoch 70/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0108\n",
            "Epoch 71/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0108\n",
            "Epoch 72/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0107\n",
            "Epoch 73/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0028 - val_loss: 0.0107\n",
            "Epoch 74/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0107\n",
            "Epoch 75/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0107\n",
            "6/6 [==============================] - 1s 3ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 2 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 4s 8ms/step - loss: 0.4220 - val_loss: 0.2394\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.1680 - val_loss: 0.1282\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0917 - val_loss: 0.0809\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0593 - val_loss: 0.0573\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0437 - val_loss: 0.0443\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0353 - val_loss: 0.0364\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0301 - val_loss: 0.0311\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0266 - val_loss: 0.0274\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0240 - val_loss: 0.0245\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0218 - val_loss: 0.0222\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0200 - val_loss: 0.0203\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0185 - val_loss: 0.0188\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0173 - val_loss: 0.0175\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0162 - val_loss: 0.0164\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0153 - val_loss: 0.0155\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0145 - val_loss: 0.0147\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0139 - val_loss: 0.0141\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0133 - val_loss: 0.0136\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0128 - val_loss: 0.0131\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0124 - val_loss: 0.0128\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0120 - val_loss: 0.0124\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0116 - val_loss: 0.0122\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0113 - val_loss: 0.0119\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0110 - val_loss: 0.0118\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0107 - val_loss: 0.0117\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0105 - val_loss: 0.0116\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0102 - val_loss: 0.0116\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0100 - val_loss: 0.0116\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0098 - val_loss: 0.0115\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0096 - val_loss: 0.0115\n",
            "Epoch 31/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0094 - val_loss: 0.0115\n",
            "Epoch 32/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0092 - val_loss: 0.0115\n",
            "Epoch 33/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0090 - val_loss: 0.0115\n",
            "Epoch 34/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0089 - val_loss: 0.0116\n",
            "Epoch 35/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0087 - val_loss: 0.0116\n",
            "Epoch 36/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0086 - val_loss: 0.0116\n",
            "6/6 [==============================] - 0s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 3 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 3s 8ms/step - loss: 0.1744 - val_loss: 0.1520\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.1065 - val_loss: 0.1132\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0749 - val_loss: 0.0922\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0573 - val_loss: 0.0801\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0470 - val_loss: 0.0726\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0405 - val_loss: 0.0675\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0361 - val_loss: 0.0637\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0328 - val_loss: 0.0606\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0301 - val_loss: 0.0577\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0278 - val_loss: 0.0551\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0257 - val_loss: 0.0525\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0239 - val_loss: 0.0502\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0222 - val_loss: 0.0479\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0207 - val_loss: 0.0457\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0192 - val_loss: 0.0436\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0179 - val_loss: 0.0416\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0166 - val_loss: 0.0398\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0155 - val_loss: 0.0380\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0144 - val_loss: 0.0363\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0134 - val_loss: 0.0347\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0125 - val_loss: 0.0332\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0117 - val_loss: 0.0318\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0110 - val_loss: 0.0305\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0103 - val_loss: 0.0293\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0097 - val_loss: 0.0282\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0092 - val_loss: 0.0272\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0087 - val_loss: 0.0263\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0083 - val_loss: 0.0254\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0079 - val_loss: 0.0247\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0076 - val_loss: 0.0240\n",
            "Epoch 31/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0073 - val_loss: 0.0234\n",
            "Epoch 32/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0071 - val_loss: 0.0228\n",
            "Epoch 33/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0069 - val_loss: 0.0223\n",
            "Epoch 34/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0067 - val_loss: 0.0218\n",
            "Epoch 35/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0065 - val_loss: 0.0213\n",
            "Epoch 36/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0064 - val_loss: 0.0209\n",
            "Epoch 37/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0063 - val_loss: 0.0205\n",
            "Epoch 38/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0062 - val_loss: 0.0202\n",
            "Epoch 39/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0061 - val_loss: 0.0199\n",
            "Epoch 40/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0060 - val_loss: 0.0196\n",
            "Epoch 41/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0059 - val_loss: 0.0194\n",
            "Epoch 42/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0058 - val_loss: 0.0191\n",
            "Epoch 43/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0058 - val_loss: 0.0189\n",
            "Epoch 44/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0057 - val_loss: 0.0187\n",
            "Epoch 45/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0057 - val_loss: 0.0185\n",
            "Epoch 46/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0056 - val_loss: 0.0184\n",
            "Epoch 47/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0056 - val_loss: 0.0182\n",
            "Epoch 48/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0056 - val_loss: 0.0181\n",
            "Epoch 49/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0055 - val_loss: 0.0179\n",
            "Epoch 50/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0055 - val_loss: 0.0178\n",
            "Epoch 51/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0055 - val_loss: 0.0177\n",
            "Epoch 52/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0054 - val_loss: 0.0176\n",
            "Epoch 53/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0054 - val_loss: 0.0175\n",
            "Epoch 54/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0054 - val_loss: 0.0174\n",
            "Epoch 55/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0053 - val_loss: 0.0174\n",
            "Epoch 56/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0053 - val_loss: 0.0173\n",
            "Epoch 57/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0053 - val_loss: 0.0172\n",
            "Epoch 58/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0053 - val_loss: 0.0171\n",
            "Epoch 59/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0053 - val_loss: 0.0171\n",
            "Epoch 60/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0052 - val_loss: 0.0170\n",
            "Epoch 61/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0052 - val_loss: 0.0170\n",
            "Epoch 62/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0052 - val_loss: 0.0169\n",
            "Epoch 63/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0052 - val_loss: 0.0169\n",
            "Epoch 64/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0052 - val_loss: 0.0168\n",
            "Epoch 65/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0051 - val_loss: 0.0168\n",
            "Epoch 66/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0051 - val_loss: 0.0168\n",
            "Epoch 67/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0051 - val_loss: 0.0167\n",
            "Epoch 68/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0051 - val_loss: 0.0167\n",
            "Epoch 69/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0051 - val_loss: 0.0167\n",
            "Epoch 70/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0050 - val_loss: 0.0167\n",
            "Epoch 71/75\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 0.0050 - val_loss: 0.0166\n",
            "Epoch 72/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0050 - val_loss: 0.0166\n",
            "Epoch 73/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0050 - val_loss: 0.0166\n",
            "Epoch 74/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0050 - val_loss: 0.0166\n",
            "Epoch 75/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0050 - val_loss: 0.0166\n",
            "6/6 [==============================] - 1s 3ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 4 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 4s 9ms/step - loss: 0.1611 - val_loss: 0.1023\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0745 - val_loss: 0.0574\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0458 - val_loss: 0.0391\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0334 - val_loss: 0.0305\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0273 - val_loss: 0.0258\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0239 - val_loss: 0.0230\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0215 - val_loss: 0.0211\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0197 - val_loss: 0.0197\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0183 - val_loss: 0.0186\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0171 - val_loss: 0.0176\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0161 - val_loss: 0.0169\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0152 - val_loss: 0.0162\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0145 - val_loss: 0.0157\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0139 - val_loss: 0.0152\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0133 - val_loss: 0.0149\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0128 - val_loss: 0.0146\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0124 - val_loss: 0.0144\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0120 - val_loss: 0.0142\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0117 - val_loss: 0.0140\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0114 - val_loss: 0.0138\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0111 - val_loss: 0.0136\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0108 - val_loss: 0.0135\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0106 - val_loss: 0.0134\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0104 - val_loss: 0.0133\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0102 - val_loss: 0.0132\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0100 - val_loss: 0.0131\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0099 - val_loss: 0.0130\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0097 - val_loss: 0.0129\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0096 - val_loss: 0.0129\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0094 - val_loss: 0.0128\n",
            "Epoch 31/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0093 - val_loss: 0.0127\n",
            "Epoch 32/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0092 - val_loss: 0.0127\n",
            "Epoch 33/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0091 - val_loss: 0.0126\n",
            "Epoch 34/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0090 - val_loss: 0.0126\n",
            "Epoch 35/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0089 - val_loss: 0.0125\n",
            "Epoch 36/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0088 - val_loss: 0.0125\n",
            "Epoch 37/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0087 - val_loss: 0.0124\n",
            "Epoch 38/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0086 - val_loss: 0.0124\n",
            "Epoch 39/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0085 - val_loss: 0.0124\n",
            "Epoch 40/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0084 - val_loss: 0.0123\n",
            "Epoch 41/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0084 - val_loss: 0.0123\n",
            "Epoch 42/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0083 - val_loss: 0.0122\n",
            "Epoch 43/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0082 - val_loss: 0.0122\n",
            "Epoch 44/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0081 - val_loss: 0.0121\n",
            "Epoch 45/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0081 - val_loss: 0.0121\n",
            "Epoch 46/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0080 - val_loss: 0.0121\n",
            "Epoch 47/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0079 - val_loss: 0.0120\n",
            "Epoch 48/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0079 - val_loss: 0.0120\n",
            "Epoch 49/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0078 - val_loss: 0.0120\n",
            "Epoch 50/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0077 - val_loss: 0.0120\n",
            "Epoch 51/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0077 - val_loss: 0.0119\n",
            "Epoch 52/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0076 - val_loss: 0.0119\n",
            "Epoch 53/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0076 - val_loss: 0.0119\n",
            "Epoch 54/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0075 - val_loss: 0.0119\n",
            "Epoch 55/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0075 - val_loss: 0.0118\n",
            "Epoch 56/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0074 - val_loss: 0.0118\n",
            "Epoch 57/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0073 - val_loss: 0.0118\n",
            "Epoch 58/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0073 - val_loss: 0.0117\n",
            "Epoch 59/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0072 - val_loss: 0.0117\n",
            "Epoch 60/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0072 - val_loss: 0.0117\n",
            "Epoch 61/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0071 - val_loss: 0.0116\n",
            "Epoch 62/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0071 - val_loss: 0.0116\n",
            "Epoch 63/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0070 - val_loss: 0.0116\n",
            "Epoch 64/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0070 - val_loss: 0.0116\n",
            "Epoch 65/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0070 - val_loss: 0.0115\n",
            "Epoch 66/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0069 - val_loss: 0.0115\n",
            "Epoch 67/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0069 - val_loss: 0.0115\n",
            "Epoch 68/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0068 - val_loss: 0.0115\n",
            "Epoch 69/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0068 - val_loss: 0.0115\n",
            "Epoch 70/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0067 - val_loss: 0.0114\n",
            "Epoch 71/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0067 - val_loss: 0.0114\n",
            "Epoch 72/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0066 - val_loss: 0.0114\n",
            "Epoch 73/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0066 - val_loss: 0.0114\n",
            "Epoch 74/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0066 - val_loss: 0.0113\n",
            "Epoch 75/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0065 - val_loss: 0.0113\n",
            "6/6 [==============================] - 0s 6ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 5 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 4s 11ms/step - loss: 0.1375 - val_loss: 0.1135\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0752 - val_loss: 0.0834\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0559 - val_loss: 0.0690\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0454 - val_loss: 0.0597\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0381 - val_loss: 0.0528\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0326 - val_loss: 0.0474\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.0430\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0246 - val_loss: 0.0394\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0216 - val_loss: 0.0364\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0192 - val_loss: 0.0339\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0171 - val_loss: 0.0317\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0154 - val_loss: 0.0298\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0139 - val_loss: 0.0282\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0127 - val_loss: 0.0268\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0116 - val_loss: 0.0256\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0107 - val_loss: 0.0246\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0099 - val_loss: 0.0237\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0093 - val_loss: 0.0229\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0087 - val_loss: 0.0222\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0082 - val_loss: 0.0217\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0078 - val_loss: 0.0211\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0074 - val_loss: 0.0207\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0071 - val_loss: 0.0203\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0068 - val_loss: 0.0199\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0066 - val_loss: 0.0197\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0064 - val_loss: 0.0194\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0062 - val_loss: 0.0192\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0060 - val_loss: 0.0189\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0058 - val_loss: 0.0188\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0057 - val_loss: 0.0186\n",
            "Epoch 31/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0056 - val_loss: 0.0184\n",
            "Epoch 32/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0055 - val_loss: 0.0183\n",
            "Epoch 33/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0054 - val_loss: 0.0182\n",
            "Epoch 34/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0053 - val_loss: 0.0181\n",
            "Epoch 35/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0052 - val_loss: 0.0180\n",
            "Epoch 36/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0051 - val_loss: 0.0179\n",
            "Epoch 37/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0050 - val_loss: 0.0179\n",
            "Epoch 38/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0050 - val_loss: 0.0178\n",
            "Epoch 39/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0049 - val_loss: 0.0177\n",
            "Epoch 40/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0048 - val_loss: 0.0177\n",
            "Epoch 41/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0048 - val_loss: 0.0176\n",
            "Epoch 42/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0047 - val_loss: 0.0175\n",
            "Epoch 43/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0047 - val_loss: 0.0175\n",
            "Epoch 44/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0046 - val_loss: 0.0174\n",
            "Epoch 45/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0046 - val_loss: 0.0174\n",
            "Epoch 46/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0045 - val_loss: 0.0174\n",
            "Epoch 47/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0045 - val_loss: 0.0173\n",
            "Epoch 48/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0044 - val_loss: 0.0173\n",
            "Epoch 49/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0044 - val_loss: 0.0173\n",
            "Epoch 50/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0044 - val_loss: 0.0173\n",
            "Epoch 51/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0043 - val_loss: 0.0172\n",
            "Epoch 52/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0043 - val_loss: 0.0172\n",
            "Epoch 53/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0042 - val_loss: 0.0172\n",
            "Epoch 54/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0042 - val_loss: 0.0172\n",
            "Epoch 55/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0042 - val_loss: 0.0171\n",
            "Epoch 56/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0041 - val_loss: 0.0171\n",
            "Epoch 57/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0041 - val_loss: 0.0170\n",
            "Epoch 58/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0041 - val_loss: 0.0170\n",
            "Epoch 59/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0170\n",
            "Epoch 60/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0040 - val_loss: 0.0170\n",
            "Epoch 61/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0040 - val_loss: 0.0170\n",
            "Epoch 62/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0039 - val_loss: 0.0169\n",
            "Epoch 63/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0039 - val_loss: 0.0169\n",
            "Epoch 64/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0039 - val_loss: 0.0169\n",
            "Epoch 65/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0039 - val_loss: 0.0168\n",
            "Epoch 66/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0038 - val_loss: 0.0169\n",
            "Epoch 67/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0038 - val_loss: 0.0168\n",
            "Epoch 68/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0038 - val_loss: 0.0168\n",
            "Epoch 69/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0038 - val_loss: 0.0168\n",
            "Epoch 70/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0037 - val_loss: 0.0167\n",
            "Epoch 71/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0037 - val_loss: 0.0167\n",
            "Epoch 72/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0167\n",
            "Epoch 73/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0167\n",
            "Epoch 74/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0036 - val_loss: 0.0167\n",
            "Epoch 75/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0036 - val_loss: 0.0167\n",
            "6/6 [==============================] - 0s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 6 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 3s 8ms/step - loss: 0.1149 - val_loss: 0.0759\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0414 - val_loss: 0.0505\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0265 - val_loss: 0.0389\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0187 - val_loss: 0.0319\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0140 - val_loss: 0.0273\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0109 - val_loss: 0.0241\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0088 - val_loss: 0.0219\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0074 - val_loss: 0.0203\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0065 - val_loss: 0.0192\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0058 - val_loss: 0.0184\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0054 - val_loss: 0.0179\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0050 - val_loss: 0.0173\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0048 - val_loss: 0.0170\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0046 - val_loss: 0.0167\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0044 - val_loss: 0.0164\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0043 - val_loss: 0.0163\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0042 - val_loss: 0.0161\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0041 - val_loss: 0.0159\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0040 - val_loss: 0.0159\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0039 - val_loss: 0.0157\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0039 - val_loss: 0.0156\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0038 - val_loss: 0.0156\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0156\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0037 - val_loss: 0.0155\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0155\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0036 - val_loss: 0.0155\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0154\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0035 - val_loss: 0.0152\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0035 - val_loss: 0.0151\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0034 - val_loss: 0.0151\n",
            "Epoch 31/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0034 - val_loss: 0.0151\n",
            "Epoch 32/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0034 - val_loss: 0.0151\n",
            "Epoch 33/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0033 - val_loss: 0.0151\n",
            "Epoch 34/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0033 - val_loss: 0.0151\n",
            "Epoch 35/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0033 - val_loss: 0.0149\n",
            "Epoch 36/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0032 - val_loss: 0.0150\n",
            "Epoch 37/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0149\n",
            "Epoch 38/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0032 - val_loss: 0.0149\n",
            "Epoch 39/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0147\n",
            "Epoch 40/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0148\n",
            "Epoch 41/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0031 - val_loss: 0.0147\n",
            "Epoch 42/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0147\n",
            "Epoch 43/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0145\n",
            "Epoch 44/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0145\n",
            "Epoch 45/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0145\n",
            "Epoch 46/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0030 - val_loss: 0.0146\n",
            "Epoch 47/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0029 - val_loss: 0.0143\n",
            "Epoch 48/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0029 - val_loss: 0.0143\n",
            "Epoch 49/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0029 - val_loss: 0.0143\n",
            "Epoch 50/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0029 - val_loss: 0.0143\n",
            "Epoch 51/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0029 - val_loss: 0.0143\n",
            "Epoch 52/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0028 - val_loss: 0.0143\n",
            "Epoch 53/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0028 - val_loss: 0.0142\n",
            "Epoch 54/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0028 - val_loss: 0.0142\n",
            "Epoch 55/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0028 - val_loss: 0.0141\n",
            "Epoch 56/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0141\n",
            "Epoch 57/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0140\n",
            "Epoch 58/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0140\n",
            "Epoch 59/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0140\n",
            "Epoch 60/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0027 - val_loss: 0.0139\n",
            "Epoch 61/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0139\n",
            "Epoch 62/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0026 - val_loss: 0.0139\n",
            "Epoch 63/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0026 - val_loss: 0.0138\n",
            "Epoch 64/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0026 - val_loss: 0.0138\n",
            "Epoch 65/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0026 - val_loss: 0.0138\n",
            "Epoch 66/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0026 - val_loss: 0.0137\n",
            "Epoch 67/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0026 - val_loss: 0.0137\n",
            "Epoch 68/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0025 - val_loss: 0.0136\n",
            "Epoch 69/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0025 - val_loss: 0.0135\n",
            "Epoch 70/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0025 - val_loss: 0.0136\n",
            "Epoch 71/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0025 - val_loss: 0.0135\n",
            "Epoch 72/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0025 - val_loss: 0.0134\n",
            "Epoch 73/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0135\n",
            "Epoch 74/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0025 - val_loss: 0.0134\n",
            "Epoch 75/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0024 - val_loss: 0.0133\n",
            "6/6 [==============================] - 0s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 7 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 4s 10ms/step - loss: 0.0793 - val_loss: 0.0442\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0322 - val_loss: 0.0335\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0217 - val_loss: 0.0287\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0165 - val_loss: 0.0252\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0133 - val_loss: 0.0225\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0111 - val_loss: 0.0206\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0097 - val_loss: 0.0195\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0087 - val_loss: 0.0186\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0080 - val_loss: 0.0180\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0074 - val_loss: 0.0176\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0070 - val_loss: 0.0174\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0067 - val_loss: 0.0171\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0064 - val_loss: 0.0171\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0062 - val_loss: 0.0169\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0060 - val_loss: 0.0168\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0058 - val_loss: 0.0168\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0056 - val_loss: 0.0167\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0055 - val_loss: 0.0168\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0053 - val_loss: 0.0168\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0052 - val_loss: 0.0168\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0051 - val_loss: 0.0167\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0049 - val_loss: 0.0167\n",
            "6/6 [==============================] - 0s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 8 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 3s 8ms/step - loss: 0.0470 - val_loss: 0.0258\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0252 - val_loss: 0.0192\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0200 - val_loss: 0.0165\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0172 - val_loss: 0.0151\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0152 - val_loss: 0.0141\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0138 - val_loss: 0.0134\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0126 - val_loss: 0.0128\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0118 - val_loss: 0.0124\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0110 - val_loss: 0.0121\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0104 - val_loss: 0.0118\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0099 - val_loss: 0.0115\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0095 - val_loss: 0.0113\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0091 - val_loss: 0.0112\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0088 - val_loss: 0.0110\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0085 - val_loss: 0.0109\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0082 - val_loss: 0.0107\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0080 - val_loss: 0.0106\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0078 - val_loss: 0.0105\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 0.0076 - val_loss: 0.0104\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0074 - val_loss: 0.0103\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0072 - val_loss: 0.0102\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0071 - val_loss: 0.0101\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0069 - val_loss: 0.0101\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0068 - val_loss: 0.0100\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0067 - val_loss: 0.0100\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0066 - val_loss: 0.0099\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0065 - val_loss: 0.0098\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0064 - val_loss: 0.0098\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0063 - val_loss: 0.0097\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0062 - val_loss: 0.0097\n",
            "Epoch 31/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0061 - val_loss: 0.0097\n",
            "Epoch 32/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0060 - val_loss: 0.0096\n",
            "Epoch 33/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0059 - val_loss: 0.0096\n",
            "Epoch 34/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0058 - val_loss: 0.0095\n",
            "Epoch 35/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0058 - val_loss: 0.0095\n",
            "Epoch 36/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0057 - val_loss: 0.0095\n",
            "Epoch 37/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0056 - val_loss: 0.0095\n",
            "Epoch 38/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0055 - val_loss: 0.0094\n",
            "Epoch 39/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0055 - val_loss: 0.0094\n",
            "Epoch 40/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0054 - val_loss: 0.0093\n",
            "Epoch 41/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0053 - val_loss: 0.0093\n",
            "Epoch 42/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0053 - val_loss: 0.0093\n",
            "Epoch 43/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0052 - val_loss: 0.0093\n",
            "Epoch 44/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0051 - val_loss: 0.0092\n",
            "Epoch 45/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0051 - val_loss: 0.0092\n",
            "Epoch 46/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0050 - val_loss: 0.0092\n",
            "Epoch 47/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0049 - val_loss: 0.0092\n",
            "Epoch 48/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0049 - val_loss: 0.0091\n",
            "Epoch 49/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0048 - val_loss: 0.0091\n",
            "Epoch 50/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0047 - val_loss: 0.0091\n",
            "Epoch 51/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0047 - val_loss: 0.0091\n",
            "Epoch 52/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0046 - val_loss: 0.0090\n",
            "Epoch 53/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0046 - val_loss: 0.0090\n",
            "Epoch 54/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0045 - val_loss: 0.0090\n",
            "Epoch 55/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0044 - val_loss: 0.0089\n",
            "Epoch 56/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0044 - val_loss: 0.0089\n",
            "Epoch 57/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0043 - val_loss: 0.0089\n",
            "Epoch 58/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0043 - val_loss: 0.0089\n",
            "Epoch 59/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0042 - val_loss: 0.0088\n",
            "Epoch 60/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0042 - val_loss: 0.0088\n",
            "Epoch 61/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0041 - val_loss: 0.0088\n",
            "Epoch 62/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0041 - val_loss: 0.0087\n",
            "Epoch 63/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0087\n",
            "Epoch 64/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0039 - val_loss: 0.0086\n",
            "Epoch 65/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0039 - val_loss: 0.0086\n",
            "Epoch 66/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0038 - val_loss: 0.0085\n",
            "Epoch 67/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0038 - val_loss: 0.0084\n",
            "Epoch 68/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0037 - val_loss: 0.0084\n",
            "Epoch 69/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0036 - val_loss: 0.0083\n",
            "Epoch 70/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0036 - val_loss: 0.0083\n",
            "Epoch 71/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0035 - val_loss: 0.0082\n",
            "Epoch 72/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0035 - val_loss: 0.0082\n",
            "Epoch 73/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0034 - val_loss: 0.0081\n",
            "Epoch 74/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0034 - val_loss: 0.0081\n",
            "Epoch 75/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0080\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 9 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 3s 8ms/step - loss: 0.2197 - val_loss: 0.1865\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 0.0924 - val_loss: 0.1349\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0569 - val_loss: 0.1117\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0423 - val_loss: 0.0987\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0338 - val_loss: 0.0896\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0280 - val_loss: 0.0827\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0237 - val_loss: 0.0771\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0205 - val_loss: 0.0728\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0180 - val_loss: 0.0690\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0159 - val_loss: 0.0657\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0141 - val_loss: 0.0627\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0127 - val_loss: 0.0601\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0115 - val_loss: 0.0578\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0106 - val_loss: 0.0558\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0098 - val_loss: 0.0540\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0092 - val_loss: 0.0523\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0087 - val_loss: 0.0508\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0083 - val_loss: 0.0495\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0080 - val_loss: 0.0483\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0077 - val_loss: 0.0472\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 0.0075 - val_loss: 0.0463\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0073 - val_loss: 0.0454\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0072 - val_loss: 0.0446\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0070 - val_loss: 0.0439\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0069 - val_loss: 0.0432\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0068 - val_loss: 0.0426\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0067 - val_loss: 0.0421\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0067 - val_loss: 0.0417\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0066 - val_loss: 0.0412\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0065 - val_loss: 0.0408\n",
            "Epoch 31/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0065 - val_loss: 0.0405\n",
            "Epoch 32/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0064 - val_loss: 0.0402\n",
            "Epoch 33/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0064 - val_loss: 0.0398\n",
            "Epoch 34/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0063 - val_loss: 0.0396\n",
            "Epoch 35/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0063 - val_loss: 0.0393\n",
            "Epoch 36/75\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 0.0062 - val_loss: 0.0389\n",
            "Epoch 37/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0062 - val_loss: 0.0387\n",
            "Epoch 38/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0061 - val_loss: 0.0384\n",
            "Epoch 39/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0061 - val_loss: 0.0381\n",
            "Epoch 40/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0061 - val_loss: 0.0379\n",
            "Epoch 41/75\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 0.0060 - val_loss: 0.0377\n",
            "Epoch 42/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0060 - val_loss: 0.0376\n",
            "Epoch 43/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0060 - val_loss: 0.0373\n",
            "Epoch 44/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0059 - val_loss: 0.0371\n",
            "Epoch 45/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0059 - val_loss: 0.0369\n",
            "Epoch 46/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0059 - val_loss: 0.0367\n",
            "Epoch 47/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0058 - val_loss: 0.0366\n",
            "Epoch 48/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0058 - val_loss: 0.0364\n",
            "Epoch 49/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0058 - val_loss: 0.0362\n",
            "Epoch 50/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0058 - val_loss: 0.0360\n",
            "Epoch 51/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0057 - val_loss: 0.0358\n",
            "Epoch 52/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0057 - val_loss: 0.0356\n",
            "Epoch 53/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0057 - val_loss: 0.0355\n",
            "Epoch 54/75\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 0.0057 - val_loss: 0.0353\n",
            "Epoch 55/75\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 0.0056 - val_loss: 0.0351\n",
            "Epoch 56/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0056 - val_loss: 0.0350\n",
            "Epoch 57/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0056 - val_loss: 0.0348\n",
            "Epoch 58/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0056 - val_loss: 0.0347\n",
            "Epoch 59/75\n",
            "171/171 [==============================] - 1s 5ms/step - loss: 0.0055 - val_loss: 0.0345\n",
            "Epoch 60/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0055 - val_loss: 0.0344\n",
            "Epoch 61/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0055 - val_loss: 0.0343\n",
            "Epoch 62/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0055 - val_loss: 0.0341\n",
            "Epoch 63/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0055 - val_loss: 0.0340\n",
            "Epoch 64/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0054 - val_loss: 0.0338\n",
            "Epoch 65/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0054 - val_loss: 0.0337\n",
            "Epoch 66/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0054 - val_loss: 0.0336\n",
            "Epoch 67/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0054 - val_loss: 0.0335\n",
            "Epoch 68/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0054 - val_loss: 0.0334\n",
            "Epoch 69/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0053 - val_loss: 0.0333\n",
            "Epoch 70/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0053 - val_loss: 0.0330\n",
            "Epoch 71/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0053 - val_loss: 0.0329\n",
            "Epoch 72/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0053 - val_loss: 0.0328\n",
            "Epoch 73/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0053 - val_loss: 0.0327\n",
            "Epoch 74/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0052 - val_loss: 0.0325\n",
            "Epoch 75/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0052 - val_loss: 0.0324\n",
            "6/6 [==============================] - 1s 6ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 0 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 4s 13ms/step - loss: 0.1848 - val_loss: 0.1149\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0764 - val_loss: 0.0626\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0388 - val_loss: 0.0414\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0229 - val_loss: 0.0318\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0154 - val_loss: 0.0274\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0117 - val_loss: 0.0252\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0098 - val_loss: 0.0240\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0088 - val_loss: 0.0233\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0081 - val_loss: 0.0228\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0076 - val_loss: 0.0223\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0073 - val_loss: 0.0219\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0070 - val_loss: 0.0215\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0067 - val_loss: 0.0212\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0065 - val_loss: 0.0209\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0064 - val_loss: 0.0207\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0062 - val_loss: 0.0204\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0061 - val_loss: 0.0203\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0060 - val_loss: 0.0201\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0059 - val_loss: 0.0200\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0059 - val_loss: 0.0199\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0058 - val_loss: 0.0197\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0057 - val_loss: 0.0196\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0057 - val_loss: 0.0196\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0056 - val_loss: 0.0195\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0056 - val_loss: 0.0194\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0055 - val_loss: 0.0193\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0055 - val_loss: 0.0193\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0055 - val_loss: 0.0192\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0054 - val_loss: 0.0192\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0054 - val_loss: 0.0192\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0053 - val_loss: 0.0191\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0053 - val_loss: 0.0191\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0053 - val_loss: 0.0191\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0053 - val_loss: 0.0190\n",
            "Epoch 35/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0052 - val_loss: 0.0190\n",
            "Epoch 36/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0052 - val_loss: 0.0190\n",
            "Epoch 37/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0052 - val_loss: 0.0190\n",
            "Epoch 38/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0051 - val_loss: 0.0190\n",
            "Epoch 39/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0051 - val_loss: 0.0190\n",
            "Epoch 40/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0051 - val_loss: 0.0190\n",
            "Epoch 41/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0051 - val_loss: 0.0190\n",
            "Epoch 42/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0050 - val_loss: 0.0190\n",
            "Epoch 43/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0050 - val_loss: 0.0189\n",
            "Epoch 44/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0050 - val_loss: 0.0190\n",
            "Epoch 45/75\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 0.0050 - val_loss: 0.0190\n",
            "Epoch 46/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0050 - val_loss: 0.0190\n",
            "Epoch 47/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0049 - val_loss: 0.0190\n",
            "Epoch 48/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0049 - val_loss: 0.0190\n",
            "6/6 [==============================] - 0s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 1 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 3s 14ms/step - loss: 0.1250 - val_loss: 0.0689\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0477 - val_loss: 0.0364\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0224 - val_loss: 0.0256\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0127 - val_loss: 0.0223\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0090 - val_loss: 0.0215\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0075 - val_loss: 0.0214\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0069 - val_loss: 0.0214\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0065 - val_loss: 0.0214\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0063 - val_loss: 0.0214\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0061 - val_loss: 0.0214\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0060 - val_loss: 0.0213\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0058 - val_loss: 0.0213\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0057 - val_loss: 0.0212\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0056 - val_loss: 0.0211\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0055 - val_loss: 0.0210\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0054 - val_loss: 0.0209\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0053 - val_loss: 0.0207\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0053 - val_loss: 0.0206\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0052 - val_loss: 0.0205\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0051 - val_loss: 0.0204\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0050 - val_loss: 0.0204\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0049 - val_loss: 0.0204\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0049 - val_loss: 0.0203\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0048 - val_loss: 0.0203\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 0.0047 - val_loss: 0.0203\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0046 - val_loss: 0.0203\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0046 - val_loss: 0.0202\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0045 - val_loss: 0.0203\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0044 - val_loss: 0.0202\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0044 - val_loss: 0.0202\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0044 - val_loss: 0.0202\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0043 - val_loss: 0.0202\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0043 - val_loss: 0.0203\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0042 - val_loss: 0.0202\n",
            "Epoch 35/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0042 - val_loss: 0.0202\n",
            "Epoch 36/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0042 - val_loss: 0.0202\n",
            "Epoch 37/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0041 - val_loss: 0.0202\n",
            "Epoch 38/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0041 - val_loss: 0.0201\n",
            "Epoch 39/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0041 - val_loss: 0.0202\n",
            "Epoch 40/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0041 - val_loss: 0.0202\n",
            "Epoch 41/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0041 - val_loss: 0.0201\n",
            "Epoch 42/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0201\n",
            "Epoch 43/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0040 - val_loss: 0.0201\n",
            "Epoch 44/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0040 - val_loss: 0.0201\n",
            "Epoch 45/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0201\n",
            "Epoch 46/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0200\n",
            "Epoch 47/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0039 - val_loss: 0.0200\n",
            "Epoch 48/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0039 - val_loss: 0.0200\n",
            "Epoch 49/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0039 - val_loss: 0.0199\n",
            "Epoch 50/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0039 - val_loss: 0.0199\n",
            "Epoch 51/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0039 - val_loss: 0.0199\n",
            "Epoch 52/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0039 - val_loss: 0.0199\n",
            "Epoch 53/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0038 - val_loss: 0.0199\n",
            "Epoch 54/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0038 - val_loss: 0.0198\n",
            "Epoch 55/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0038 - val_loss: 0.0198\n",
            "Epoch 56/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0038 - val_loss: 0.0198\n",
            "Epoch 57/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0038 - val_loss: 0.0198\n",
            "Epoch 58/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0038 - val_loss: 0.0197\n",
            "Epoch 59/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0037 - val_loss: 0.0197\n",
            "Epoch 60/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0037 - val_loss: 0.0197\n",
            "Epoch 61/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0037 - val_loss: 0.0197\n",
            "Epoch 62/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0037 - val_loss: 0.0197\n",
            "Epoch 63/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0037 - val_loss: 0.0197\n",
            "Epoch 64/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0037 - val_loss: 0.0197\n",
            "Epoch 65/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0196\n",
            "Epoch 66/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0196\n",
            "Epoch 67/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0036 - val_loss: 0.0196\n",
            "Epoch 68/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0196\n",
            "Epoch 69/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0036 - val_loss: 0.0196\n",
            "Epoch 70/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0195\n",
            "Epoch 71/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0036 - val_loss: 0.0195\n",
            "Epoch 72/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0195\n",
            "Epoch 73/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0195\n",
            "Epoch 74/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0035 - val_loss: 0.0195\n",
            "Epoch 75/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0035 - val_loss: 0.0194\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 2 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 3s 10ms/step - loss: 0.1293 - val_loss: 0.0796\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0621 - val_loss: 0.0396\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0308 - val_loss: 0.0222\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0175 - val_loss: 0.0165\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0128 - val_loss: 0.0145\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0110 - val_loss: 0.0139\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0101 - val_loss: 0.0137\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0097 - val_loss: 0.0135\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0093 - val_loss: 0.0135\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0090 - val_loss: 0.0134\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0088 - val_loss: 0.0134\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0085 - val_loss: 0.0133\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0083 - val_loss: 0.0133\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0081 - val_loss: 0.0132\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0080 - val_loss: 0.0132\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0078 - val_loss: 0.0132\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0077 - val_loss: 0.0131\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0075 - val_loss: 0.0131\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0074 - val_loss: 0.0131\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0073 - val_loss: 0.0131\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0071 - val_loss: 0.0130\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0070 - val_loss: 0.0130\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0069 - val_loss: 0.0130\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0068 - val_loss: 0.0130\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0067 - val_loss: 0.0130\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0066 - val_loss: 0.0130\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0065 - val_loss: 0.0129\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0064 - val_loss: 0.0129\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0063 - val_loss: 0.0129\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0062 - val_loss: 0.0129\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0061 - val_loss: 0.0130\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0061 - val_loss: 0.0130\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0060 - val_loss: 0.0129\n",
            "6/6 [==============================] - 0s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 3 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 3s 11ms/step - loss: 0.3718 - val_loss: 0.2806\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.2349 - val_loss: 0.1971\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.1693 - val_loss: 0.1463\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.1260 - val_loss: 0.1112\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0954 - val_loss: 0.0860\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0731 - val_loss: 0.0676\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0571 - val_loss: 0.0547\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0461 - val_loss: 0.0454\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0382 - val_loss: 0.0387\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0325 - val_loss: 0.0339\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0284 - val_loss: 0.0305\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0254 - val_loss: 0.0279\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0231 - val_loss: 0.0260\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0212 - val_loss: 0.0245\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0198 - val_loss: 0.0233\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0186 - val_loss: 0.0223\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0175 - val_loss: 0.0215\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0167 - val_loss: 0.0209\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0159 - val_loss: 0.0203\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0152 - val_loss: 0.0198\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0147 - val_loss: 0.0193\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0141 - val_loss: 0.0190\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0137 - val_loss: 0.0186\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0132 - val_loss: 0.0183\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0129 - val_loss: 0.0181\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0125 - val_loss: 0.0178\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0122 - val_loss: 0.0176\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 0.0119 - val_loss: 0.0174\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0117 - val_loss: 0.0172\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 0.0115 - val_loss: 0.0171\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0113 - val_loss: 0.0169\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0111 - val_loss: 0.0168\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0109 - val_loss: 0.0167\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0107 - val_loss: 0.0166\n",
            "Epoch 35/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0106 - val_loss: 0.0165\n",
            "Epoch 36/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0105 - val_loss: 0.0164\n",
            "Epoch 37/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0103 - val_loss: 0.0163\n",
            "Epoch 38/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0102 - val_loss: 0.0162\n",
            "Epoch 39/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0101 - val_loss: 0.0161\n",
            "Epoch 40/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0100 - val_loss: 0.0160\n",
            "Epoch 41/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0099 - val_loss: 0.0159\n",
            "Epoch 42/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0098 - val_loss: 0.0158\n",
            "Epoch 43/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0097 - val_loss: 0.0158\n",
            "Epoch 44/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0096 - val_loss: 0.0157\n",
            "Epoch 45/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0095 - val_loss: 0.0156\n",
            "Epoch 46/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0094 - val_loss: 0.0155\n",
            "Epoch 47/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0093 - val_loss: 0.0155\n",
            "Epoch 48/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0093 - val_loss: 0.0154\n",
            "Epoch 49/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0092 - val_loss: 0.0153\n",
            "Epoch 50/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0091 - val_loss: 0.0153\n",
            "Epoch 51/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0090 - val_loss: 0.0152\n",
            "Epoch 52/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0090 - val_loss: 0.0151\n",
            "Epoch 53/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0089 - val_loss: 0.0151\n",
            "Epoch 54/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0088 - val_loss: 0.0150\n",
            "Epoch 55/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0088 - val_loss: 0.0150\n",
            "Epoch 56/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0087 - val_loss: 0.0149\n",
            "Epoch 57/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0086 - val_loss: 0.0149\n",
            "Epoch 58/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0086 - val_loss: 0.0148\n",
            "Epoch 59/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0085 - val_loss: 0.0148\n",
            "Epoch 60/75\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 0.0084 - val_loss: 0.0147\n",
            "Epoch 61/75\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 0.0084 - val_loss: 0.0146\n",
            "Epoch 62/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0083 - val_loss: 0.0146\n",
            "Epoch 63/75\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 0.0082 - val_loss: 0.0145\n",
            "Epoch 64/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0082 - val_loss: 0.0145\n",
            "Epoch 65/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0081 - val_loss: 0.0144\n",
            "Epoch 66/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0081 - val_loss: 0.0144\n",
            "Epoch 67/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0080 - val_loss: 0.0143\n",
            "Epoch 68/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0079 - val_loss: 0.0143\n",
            "Epoch 69/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0079 - val_loss: 0.0142\n",
            "Epoch 70/75\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 0.0078 - val_loss: 0.0142\n",
            "Epoch 71/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0078 - val_loss: 0.0141\n",
            "Epoch 72/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0077 - val_loss: 0.0141\n",
            "Epoch 73/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0077 - val_loss: 0.0140\n",
            "Epoch 74/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0076 - val_loss: 0.0140\n",
            "Epoch 75/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0076 - val_loss: 0.0139\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 4 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 4s 15ms/step - loss: 0.2894 - val_loss: 0.2457\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.2266 - val_loss: 0.1919\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.1794 - val_loss: 0.1511\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 0.1438 - val_loss: 0.1205\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.1180 - val_loss: 0.0974\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0984 - val_loss: 0.0805\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 0.0827 - val_loss: 0.0676\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0701 - val_loss: 0.0581\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 0.0599 - val_loss: 0.0507\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0513 - val_loss: 0.0449\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 0.0443 - val_loss: 0.0401\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0384 - val_loss: 0.0359\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0333 - val_loss: 0.0321\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 0.0289 - val_loss: 0.0286\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0251 - val_loss: 0.0258\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0219 - val_loss: 0.0233\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0192 - val_loss: 0.0212\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 0.0169 - val_loss: 0.0194\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0150 - val_loss: 0.0179\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0133 - val_loss: 0.0167\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0119 - val_loss: 0.0157\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0108 - val_loss: 0.0148\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0098 - val_loss: 0.0141\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0089 - val_loss: 0.0134\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0082 - val_loss: 0.0129\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0076 - val_loss: 0.0125\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0071 - val_loss: 0.0121\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0066 - val_loss: 0.0118\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0063 - val_loss: 0.0116\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0059 - val_loss: 0.0114\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0056 - val_loss: 0.0112\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0054 - val_loss: 0.0110\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0052 - val_loss: 0.0108\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0050 - val_loss: 0.0107\n",
            "Epoch 35/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0048 - val_loss: 0.0105\n",
            "Epoch 36/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0047 - val_loss: 0.0105\n",
            "Epoch 37/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0045 - val_loss: 0.0104\n",
            "Epoch 38/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0044 - val_loss: 0.0103\n",
            "Epoch 39/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0043 - val_loss: 0.0103\n",
            "Epoch 40/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0042 - val_loss: 0.0101\n",
            "Epoch 41/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0041 - val_loss: 0.0100\n",
            "Epoch 42/75\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 0.0040 - val_loss: 0.0099\n",
            "Epoch 43/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0039 - val_loss: 0.0099\n",
            "Epoch 44/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0038 - val_loss: 0.0099\n",
            "Epoch 45/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0037 - val_loss: 0.0099\n",
            "Epoch 46/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0036 - val_loss: 0.0098\n",
            "Epoch 47/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0035 - val_loss: 0.0098\n",
            "Epoch 48/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0034 - val_loss: 0.0097\n",
            "Epoch 49/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0034 - val_loss: 0.0097\n",
            "Epoch 50/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0033 - val_loss: 0.0097\n",
            "Epoch 51/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0032 - val_loss: 0.0096\n",
            "Epoch 52/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0031 - val_loss: 0.0096\n",
            "Epoch 53/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0096\n",
            "Epoch 54/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0096\n",
            "Epoch 55/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0095\n",
            "Epoch 56/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0096\n",
            "Epoch 57/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0029 - val_loss: 0.0096\n",
            "Epoch 58/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0096\n",
            "Epoch 59/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0096\n",
            "Epoch 60/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0028 - val_loss: 0.0096\n",
            "6/6 [==============================] - 1s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 5 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 3s 10ms/step - loss: 0.2961 - val_loss: 0.2493\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.1971 - val_loss: 0.1950\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.1541 - val_loss: 0.1580\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.1230 - val_loss: 0.1299\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0996 - val_loss: 0.1096\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0828 - val_loss: 0.0958\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0708 - val_loss: 0.0853\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0616 - val_loss: 0.0772\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0544 - val_loss: 0.0709\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0489 - val_loss: 0.0658\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0445 - val_loss: 0.0617\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0410 - val_loss: 0.0583\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0381 - val_loss: 0.0554\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0358 - val_loss: 0.0530\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0338 - val_loss: 0.0509\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0321 - val_loss: 0.0491\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 0.0306 - val_loss: 0.0474\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0293 - val_loss: 0.0458\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0281 - val_loss: 0.0444\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 11ms/step - loss: 0.0270 - val_loss: 0.0430\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0260 - val_loss: 0.0417\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0251 - val_loss: 0.0406\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0242 - val_loss: 0.0394\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0234 - val_loss: 0.0383\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0226 - val_loss: 0.0373\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0218 - val_loss: 0.0363\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0211 - val_loss: 0.0353\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0204 - val_loss: 0.0343\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0197 - val_loss: 0.0334\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0191 - val_loss: 0.0325\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0185 - val_loss: 0.0316\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0179 - val_loss: 0.0308\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0173 - val_loss: 0.0300\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0167 - val_loss: 0.0292\n",
            "Epoch 35/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0162 - val_loss: 0.0284\n",
            "Epoch 36/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0157 - val_loss: 0.0277\n",
            "Epoch 37/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0152 - val_loss: 0.0270\n",
            "Epoch 38/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0148 - val_loss: 0.0263\n",
            "Epoch 39/75\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 0.0143 - val_loss: 0.0256\n",
            "Epoch 40/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0139 - val_loss: 0.0250\n",
            "Epoch 41/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0135 - val_loss: 0.0244\n",
            "Epoch 42/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0131 - val_loss: 0.0238\n",
            "Epoch 43/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0127 - val_loss: 0.0233\n",
            "Epoch 44/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0124 - val_loss: 0.0228\n",
            "Epoch 45/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0120 - val_loss: 0.0223\n",
            "Epoch 46/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0117 - val_loss: 0.0218\n",
            "Epoch 47/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0114 - val_loss: 0.0213\n",
            "Epoch 48/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0111 - val_loss: 0.0209\n",
            "Epoch 49/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0107 - val_loss: 0.0205\n",
            "Epoch 50/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0105 - val_loss: 0.0201\n",
            "Epoch 51/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0102 - val_loss: 0.0197\n",
            "Epoch 52/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0099 - val_loss: 0.0193\n",
            "Epoch 53/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0096 - val_loss: 0.0190\n",
            "Epoch 54/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0094 - val_loss: 0.0186\n",
            "Epoch 55/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0092 - val_loss: 0.0183\n",
            "Epoch 56/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0089 - val_loss: 0.0180\n",
            "Epoch 57/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0087 - val_loss: 0.0177\n",
            "Epoch 58/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0085 - val_loss: 0.0175\n",
            "Epoch 59/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0083 - val_loss: 0.0172\n",
            "Epoch 60/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0081 - val_loss: 0.0169\n",
            "Epoch 61/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0079 - val_loss: 0.0167\n",
            "Epoch 62/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0078 - val_loss: 0.0164\n",
            "Epoch 63/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0076 - val_loss: 0.0162\n",
            "Epoch 64/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0075 - val_loss: 0.0159\n",
            "Epoch 65/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0073 - val_loss: 0.0157\n",
            "Epoch 66/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0072 - val_loss: 0.0155\n",
            "Epoch 67/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0071 - val_loss: 0.0153\n",
            "Epoch 68/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0070 - val_loss: 0.0151\n",
            "Epoch 69/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0069 - val_loss: 0.0149\n",
            "Epoch 70/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0068 - val_loss: 0.0147\n",
            "Epoch 71/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0067 - val_loss: 0.0146\n",
            "Epoch 72/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0066 - val_loss: 0.0144\n",
            "Epoch 73/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0065 - val_loss: 0.0143\n",
            "Epoch 74/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0064 - val_loss: 0.0141\n",
            "Epoch 75/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0063 - val_loss: 0.0140\n",
            "6/6 [==============================] - 0s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 6 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 3s 14ms/step - loss: 0.3524 - val_loss: 0.2479\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.2276 - val_loss: 0.1775\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.1653 - val_loss: 0.1343\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.1245 - val_loss: 0.1065\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0962 - val_loss: 0.0872\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0757 - val_loss: 0.0741\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0610 - val_loss: 0.0651\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0504 - val_loss: 0.0593\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0430 - val_loss: 0.0553\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0375 - val_loss: 0.0526\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0336 - val_loss: 0.0507\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0306 - val_loss: 0.0492\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0283 - val_loss: 0.0480\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0264 - val_loss: 0.0469\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0248 - val_loss: 0.0459\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0235 - val_loss: 0.0449\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0224 - val_loss: 0.0439\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0213 - val_loss: 0.0430\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0203 - val_loss: 0.0420\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 0.0194 - val_loss: 0.0411\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0186 - val_loss: 0.0402\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0178 - val_loss: 0.0393\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0171 - val_loss: 0.0384\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0164 - val_loss: 0.0376\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0157 - val_loss: 0.0367\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0151 - val_loss: 0.0359\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0146 - val_loss: 0.0351\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0140 - val_loss: 0.0344\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0135 - val_loss: 0.0337\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0131 - val_loss: 0.0331\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0126 - val_loss: 0.0325\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0122 - val_loss: 0.0319\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0118 - val_loss: 0.0313\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0114 - val_loss: 0.0308\n",
            "Epoch 35/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0111 - val_loss: 0.0302\n",
            "Epoch 36/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0107 - val_loss: 0.0297\n",
            "Epoch 37/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0105 - val_loss: 0.0293\n",
            "Epoch 38/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0102 - val_loss: 0.0288\n",
            "Epoch 39/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0099 - val_loss: 0.0283\n",
            "Epoch 40/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0097 - val_loss: 0.0280\n",
            "Epoch 41/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0094 - val_loss: 0.0276\n",
            "Epoch 42/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0092 - val_loss: 0.0273\n",
            "Epoch 43/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0090 - val_loss: 0.0270\n",
            "Epoch 44/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0088 - val_loss: 0.0266\n",
            "Epoch 45/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0087 - val_loss: 0.0264\n",
            "Epoch 46/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0085 - val_loss: 0.0261\n",
            "Epoch 47/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0084 - val_loss: 0.0259\n",
            "Epoch 48/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0082 - val_loss: 0.0257\n",
            "Epoch 49/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0081 - val_loss: 0.0254\n",
            "Epoch 50/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0080 - val_loss: 0.0251\n",
            "Epoch 51/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0078 - val_loss: 0.0249\n",
            "Epoch 52/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0077 - val_loss: 0.0247\n",
            "Epoch 53/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0076 - val_loss: 0.0245\n",
            "Epoch 54/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0075 - val_loss: 0.0244\n",
            "Epoch 55/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0075 - val_loss: 0.0242\n",
            "Epoch 56/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0074 - val_loss: 0.0240\n",
            "Epoch 57/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0073 - val_loss: 0.0239\n",
            "Epoch 58/75\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 0.0072 - val_loss: 0.0237\n",
            "Epoch 59/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0071 - val_loss: 0.0236\n",
            "Epoch 60/75\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 0.0071 - val_loss: 0.0235\n",
            "Epoch 61/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0070 - val_loss: 0.0234\n",
            "Epoch 62/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0069 - val_loss: 0.0233\n",
            "Epoch 63/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0069 - val_loss: 0.0232\n",
            "Epoch 64/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0068 - val_loss: 0.0230\n",
            "Epoch 65/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0068 - val_loss: 0.0229\n",
            "Epoch 66/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0067 - val_loss: 0.0229\n",
            "Epoch 67/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0067 - val_loss: 0.0228\n",
            "Epoch 68/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0066 - val_loss: 0.0227\n",
            "Epoch 69/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0066 - val_loss: 0.0226\n",
            "Epoch 70/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0065 - val_loss: 0.0225\n",
            "Epoch 71/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0065 - val_loss: 0.0224\n",
            "Epoch 72/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0065 - val_loss: 0.0223\n",
            "Epoch 73/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0064 - val_loss: 0.0223\n",
            "Epoch 74/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0064 - val_loss: 0.0222\n",
            "Epoch 75/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0063 - val_loss: 0.0222\n",
            "6/6 [==============================] - 0s 3ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 7 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 3s 11ms/step - loss: 0.3023 - val_loss: 0.2428\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.2112 - val_loss: 0.1773\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.1571 - val_loss: 0.1382\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.1210 - val_loss: 0.1119\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0961 - val_loss: 0.0931\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0783 - val_loss: 0.0794\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0655 - val_loss: 0.0692\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0559 - val_loss: 0.0619\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0490 - val_loss: 0.0565\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0439 - val_loss: 0.0526\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0400 - val_loss: 0.0495\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0369 - val_loss: 0.0471\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0345 - val_loss: 0.0452\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0325 - val_loss: 0.0435\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0308 - val_loss: 0.0421\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0293 - val_loss: 0.0408\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0279 - val_loss: 0.0396\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0267 - val_loss: 0.0385\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0256 - val_loss: 0.0375\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0245 - val_loss: 0.0365\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0235 - val_loss: 0.0356\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0226 - val_loss: 0.0347\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0217 - val_loss: 0.0338\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0209 - val_loss: 0.0330\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0201 - val_loss: 0.0323\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0193 - val_loss: 0.0315\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0186 - val_loss: 0.0307\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0179 - val_loss: 0.0300\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0173 - val_loss: 0.0293\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0167 - val_loss: 0.0287\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0161 - val_loss: 0.0280\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0156 - val_loss: 0.0274\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0151 - val_loss: 0.0268\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0146 - val_loss: 0.0263\n",
            "Epoch 35/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0141 - val_loss: 0.0257\n",
            "Epoch 36/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0137 - val_loss: 0.0252\n",
            "Epoch 37/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0133 - val_loss: 0.0247\n",
            "Epoch 38/75\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 0.0129 - val_loss: 0.0242\n",
            "Epoch 39/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0126 - val_loss: 0.0238\n",
            "Epoch 40/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0123 - val_loss: 0.0234\n",
            "Epoch 41/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0119 - val_loss: 0.0229\n",
            "Epoch 42/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0116 - val_loss: 0.0226\n",
            "Epoch 43/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0114 - val_loss: 0.0222\n",
            "Epoch 44/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0111 - val_loss: 0.0219\n",
            "Epoch 45/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0108 - val_loss: 0.0215\n",
            "Epoch 46/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0106 - val_loss: 0.0212\n",
            "Epoch 47/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0104 - val_loss: 0.0209\n",
            "Epoch 48/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0102 - val_loss: 0.0207\n",
            "Epoch 49/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0100 - val_loss: 0.0204\n",
            "Epoch 50/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0098 - val_loss: 0.0202\n",
            "Epoch 51/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0097 - val_loss: 0.0199\n",
            "Epoch 52/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0095 - val_loss: 0.0197\n",
            "Epoch 53/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0094 - val_loss: 0.0195\n",
            "Epoch 54/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0092 - val_loss: 0.0193\n",
            "Epoch 55/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0091 - val_loss: 0.0191\n",
            "Epoch 56/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0090 - val_loss: 0.0189\n",
            "Epoch 57/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0088 - val_loss: 0.0188\n",
            "Epoch 58/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0087 - val_loss: 0.0186\n",
            "Epoch 59/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0086 - val_loss: 0.0185\n",
            "Epoch 60/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0085 - val_loss: 0.0183\n",
            "Epoch 61/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0084 - val_loss: 0.0182\n",
            "Epoch 62/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0083 - val_loss: 0.0181\n",
            "Epoch 63/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0082 - val_loss: 0.0179\n",
            "Epoch 64/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0082 - val_loss: 0.0178\n",
            "Epoch 65/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0081 - val_loss: 0.0177\n",
            "Epoch 66/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0080 - val_loss: 0.0176\n",
            "Epoch 67/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0079 - val_loss: 0.0175\n",
            "Epoch 68/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0079 - val_loss: 0.0174\n",
            "Epoch 69/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0078 - val_loss: 0.0173\n",
            "Epoch 70/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0077 - val_loss: 0.0172\n",
            "Epoch 71/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0077 - val_loss: 0.0172\n",
            "Epoch 72/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0076 - val_loss: 0.0171\n",
            "Epoch 73/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0076 - val_loss: 0.0170\n",
            "Epoch 74/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0075 - val_loss: 0.0169\n",
            "Epoch 75/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0075 - val_loss: 0.0169\n",
            "6/6 [==============================] - 0s 6ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 8 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 3s 11ms/step - loss: 0.1928 - val_loss: 0.1550\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.1148 - val_loss: 0.1018\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0749 - val_loss: 0.0743\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0535 - val_loss: 0.0580\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0406 - val_loss: 0.0481\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0327 - val_loss: 0.0415\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0274 - val_loss: 0.0368\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0236 - val_loss: 0.0334\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0208 - val_loss: 0.0307\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0186 - val_loss: 0.0284\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0168 - val_loss: 0.0266\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0153 - val_loss: 0.0250\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0140 - val_loss: 0.0236\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0130 - val_loss: 0.0223\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0120 - val_loss: 0.0212\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0112 - val_loss: 0.0202\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0105 - val_loss: 0.0193\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0099 - val_loss: 0.0185\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0094 - val_loss: 0.0178\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0089 - val_loss: 0.0172\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0085 - val_loss: 0.0166\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0081 - val_loss: 0.0160\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0078 - val_loss: 0.0155\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0075 - val_loss: 0.0151\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0073 - val_loss: 0.0147\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0070 - val_loss: 0.0143\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0068 - val_loss: 0.0140\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0066 - val_loss: 0.0137\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0065 - val_loss: 0.0134\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0063 - val_loss: 0.0131\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0062 - val_loss: 0.0129\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0061 - val_loss: 0.0127\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0060 - val_loss: 0.0125\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0059 - val_loss: 0.0123\n",
            "Epoch 35/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0058 - val_loss: 0.0121\n",
            "Epoch 36/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0057 - val_loss: 0.0119\n",
            "Epoch 37/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0056 - val_loss: 0.0118\n",
            "Epoch 38/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0055 - val_loss: 0.0116\n",
            "Epoch 39/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0055 - val_loss: 0.0115\n",
            "Epoch 40/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0054 - val_loss: 0.0114\n",
            "Epoch 41/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0053 - val_loss: 0.0113\n",
            "Epoch 42/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0053 - val_loss: 0.0112\n",
            "Epoch 43/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0052 - val_loss: 0.0111\n",
            "Epoch 44/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0052 - val_loss: 0.0110\n",
            "Epoch 45/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0051 - val_loss: 0.0109\n",
            "Epoch 46/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0051 - val_loss: 0.0109\n",
            "Epoch 47/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0050 - val_loss: 0.0108\n",
            "Epoch 48/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0050 - val_loss: 0.0107\n",
            "Epoch 49/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0049 - val_loss: 0.0107\n",
            "Epoch 50/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0049 - val_loss: 0.0106\n",
            "Epoch 51/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0049 - val_loss: 0.0106\n",
            "Epoch 52/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0048 - val_loss: 0.0105\n",
            "Epoch 53/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0048 - val_loss: 0.0105\n",
            "Epoch 54/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0047 - val_loss: 0.0104\n",
            "Epoch 55/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0047 - val_loss: 0.0104\n",
            "Epoch 56/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0047 - val_loss: 0.0103\n",
            "Epoch 57/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0046 - val_loss: 0.0103\n",
            "Epoch 58/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0046 - val_loss: 0.0103\n",
            "Epoch 59/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0046 - val_loss: 0.0102\n",
            "Epoch 60/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0045 - val_loss: 0.0102\n",
            "Epoch 61/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0045 - val_loss: 0.0102\n",
            "Epoch 62/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0045 - val_loss: 0.0101\n",
            "Epoch 63/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0045 - val_loss: 0.0101\n",
            "Epoch 64/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0044 - val_loss: 0.0101\n",
            "Epoch 65/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0044 - val_loss: 0.0100\n",
            "Epoch 66/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0044 - val_loss: 0.0100\n",
            "Epoch 67/75\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 0.0043 - val_loss: 0.0100\n",
            "Epoch 68/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0043 - val_loss: 0.0100\n",
            "Epoch 69/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0043 - val_loss: 0.0099\n",
            "Epoch 70/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0043 - val_loss: 0.0099\n",
            "Epoch 71/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0042 - val_loss: 0.0099\n",
            "Epoch 72/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0042 - val_loss: 0.0099\n",
            "Epoch 73/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0042 - val_loss: 0.0098\n",
            "Epoch 74/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0042 - val_loss: 0.0098\n",
            "Epoch 75/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0041 - val_loss: 0.0098\n",
            "6/6 [==============================] - 0s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 9 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 7s 15ms/step - loss: 0.1743 - val_loss: 0.1434\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0850 - val_loss: 0.0974\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0554 - val_loss: 0.0743\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0403 - val_loss: 0.0609\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0318 - val_loss: 0.0525\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0265 - val_loss: 0.0466\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0229 - val_loss: 0.0423\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0202 - val_loss: 0.0390\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0182 - val_loss: 0.0362\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0165 - val_loss: 0.0339\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0151 - val_loss: 0.0319\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0138 - val_loss: 0.0302\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0127 - val_loss: 0.0286\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0118 - val_loss: 0.0272\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0110 - val_loss: 0.0260\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0102 - val_loss: 0.0248\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0096 - val_loss: 0.0238\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0090 - val_loss: 0.0229\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0085 - val_loss: 0.0221\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0081 - val_loss: 0.0213\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0077 - val_loss: 0.0206\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0074 - val_loss: 0.0200\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0071 - val_loss: 0.0195\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0068 - val_loss: 0.0189\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0066 - val_loss: 0.0184\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0064 - val_loss: 0.0180\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0062 - val_loss: 0.0176\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0060 - val_loss: 0.0172\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0058 - val_loss: 0.0169\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0057 - val_loss: 0.0165\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0056 - val_loss: 0.0162\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0054 - val_loss: 0.0160\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0053 - val_loss: 0.0157\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0052 - val_loss: 0.0155\n",
            "Epoch 35/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0051 - val_loss: 0.0152\n",
            "Epoch 36/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0050 - val_loss: 0.0150\n",
            "Epoch 37/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0049 - val_loss: 0.0148\n",
            "Epoch 38/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0048 - val_loss: 0.0146\n",
            "Epoch 39/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0047 - val_loss: 0.0145\n",
            "Epoch 40/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0047 - val_loss: 0.0143\n",
            "Epoch 41/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0046 - val_loss: 0.0142\n",
            "Epoch 42/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0045 - val_loss: 0.0140\n",
            "Epoch 43/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0045 - val_loss: 0.0139\n",
            "Epoch 44/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0044 - val_loss: 0.0138\n",
            "Epoch 45/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0043 - val_loss: 0.0136\n",
            "Epoch 46/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0043 - val_loss: 0.0135\n",
            "Epoch 47/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0042 - val_loss: 0.0134\n",
            "Epoch 48/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0042 - val_loss: 0.0133\n",
            "Epoch 49/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0041 - val_loss: 0.0132\n",
            "Epoch 50/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0041 - val_loss: 0.0131\n",
            "Epoch 51/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0040 - val_loss: 0.0131\n",
            "Epoch 52/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0040 - val_loss: 0.0130\n",
            "Epoch 53/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0040 - val_loss: 0.0129\n",
            "Epoch 54/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0039 - val_loss: 0.0128\n",
            "Epoch 55/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0039 - val_loss: 0.0127\n",
            "Epoch 56/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0039 - val_loss: 0.0127\n",
            "Epoch 57/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0038 - val_loss: 0.0126\n",
            "Epoch 58/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0038 - val_loss: 0.0126\n",
            "Epoch 59/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0038 - val_loss: 0.0125\n",
            "Epoch 60/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0037 - val_loss: 0.0124\n",
            "Epoch 61/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0037 - val_loss: 0.0124\n",
            "Epoch 62/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0123\n",
            "Epoch 63/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0037 - val_loss: 0.0123\n",
            "Epoch 64/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0122\n",
            "Epoch 65/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0036 - val_loss: 0.0122\n",
            "Epoch 66/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0036 - val_loss: 0.0121\n",
            "Epoch 67/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0121\n",
            "Epoch 68/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0121\n",
            "Epoch 69/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0035 - val_loss: 0.0120\n",
            "Epoch 70/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0035 - val_loss: 0.0120\n",
            "Epoch 71/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0035 - val_loss: 0.0120\n",
            "Epoch 72/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0035 - val_loss: 0.0120\n",
            "Epoch 73/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0035 - val_loss: 0.0119\n",
            "Epoch 74/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0034 - val_loss: 0.0119\n",
            "Epoch 75/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0034 - val_loss: 0.0119\n",
            "6/6 [==============================] - 0s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 0 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 3s 16ms/step - loss: 0.2593 - val_loss: 0.2170\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.1876 - val_loss: 0.1729\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.1467 - val_loss: 0.1436\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.1184 - val_loss: 0.1222\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0974 - val_loss: 0.1061\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0814 - val_loss: 0.0938\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0691 - val_loss: 0.0841\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0595 - val_loss: 0.0765\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0519 - val_loss: 0.0703\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0459 - val_loss: 0.0654\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0411 - val_loss: 0.0614\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0373 - val_loss: 0.0581\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0342 - val_loss: 0.0554\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0317 - val_loss: 0.0530\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0296 - val_loss: 0.0511\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0279 - val_loss: 0.0493\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0264 - val_loss: 0.0478\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0251 - val_loss: 0.0463\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0239 - val_loss: 0.0450\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0229 - val_loss: 0.0438\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0220 - val_loss: 0.0427\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0212 - val_loss: 0.0417\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0204 - val_loss: 0.0407\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0197 - val_loss: 0.0397\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0190 - val_loss: 0.0388\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0184 - val_loss: 0.0379\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0178 - val_loss: 0.0371\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0172 - val_loss: 0.0363\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0167 - val_loss: 0.0355\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0162 - val_loss: 0.0348\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0157 - val_loss: 0.0341\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0152 - val_loss: 0.0334\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0148 - val_loss: 0.0327\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0144 - val_loss: 0.0321\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0140 - val_loss: 0.0314\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0136 - val_loss: 0.0308\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0132 - val_loss: 0.0302\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0129 - val_loss: 0.0297\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0126 - val_loss: 0.0291\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0122 - val_loss: 0.0286\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0119 - val_loss: 0.0281\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0116 - val_loss: 0.0276\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0113 - val_loss: 0.0271\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0111 - val_loss: 0.0266\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0108 - val_loss: 0.0261\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0106 - val_loss: 0.0257\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0103 - val_loss: 0.0252\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0101 - val_loss: 0.0248\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0099 - val_loss: 0.0244\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0240\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0094 - val_loss: 0.0236\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0093 - val_loss: 0.0232\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0091 - val_loss: 0.0229\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0089 - val_loss: 0.0225\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0087 - val_loss: 0.0222\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0085 - val_loss: 0.0218\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0084 - val_loss: 0.0215\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0082 - val_loss: 0.0212\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0081 - val_loss: 0.0209\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0079 - val_loss: 0.0206\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0078 - val_loss: 0.0203\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0077 - val_loss: 0.0201\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0075 - val_loss: 0.0198\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0074 - val_loss: 0.0195\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0073 - val_loss: 0.0193\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0072 - val_loss: 0.0190\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0071 - val_loss: 0.0188\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0070 - val_loss: 0.0186\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0069 - val_loss: 0.0183\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0068 - val_loss: 0.0181\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0067 - val_loss: 0.0179\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0066 - val_loss: 0.0177\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0065 - val_loss: 0.0175\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0065 - val_loss: 0.0173\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0064 - val_loss: 0.0172\n",
            "6/6 [==============================] - 1s 6ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 1 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 3s 17ms/step - loss: 0.1731 - val_loss: 0.1358\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.1118 - val_loss: 0.0999\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0813 - val_loss: 0.0797\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0624 - val_loss: 0.0674\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0502 - val_loss: 0.0595\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0422 - val_loss: 0.0537\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0368 - val_loss: 0.0497\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0328 - val_loss: 0.0466\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0296 - val_loss: 0.0442\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0270 - val_loss: 0.0421\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0246 - val_loss: 0.0404\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0226 - val_loss: 0.0387\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0208 - val_loss: 0.0372\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0192 - val_loss: 0.0358\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0177 - val_loss: 0.0344\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0164 - val_loss: 0.0332\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0152 - val_loss: 0.0320\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0141 - val_loss: 0.0309\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0131 - val_loss: 0.0299\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0122 - val_loss: 0.0289\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0114 - val_loss: 0.0279\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0106 - val_loss: 0.0270\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0099 - val_loss: 0.0261\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0093 - val_loss: 0.0253\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0087 - val_loss: 0.0246\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0082 - val_loss: 0.0239\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0077 - val_loss: 0.0232\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0073 - val_loss: 0.0225\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0069 - val_loss: 0.0220\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0065 - val_loss: 0.0214\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0062 - val_loss: 0.0209\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0059 - val_loss: 0.0205\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0056 - val_loss: 0.0200\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0054 - val_loss: 0.0196\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0052 - val_loss: 0.0192\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0050 - val_loss: 0.0189\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0048 - val_loss: 0.0186\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0046 - val_loss: 0.0183\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0045 - val_loss: 0.0180\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0043 - val_loss: 0.0177\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0042 - val_loss: 0.0175\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0041 - val_loss: 0.0172\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0040 - val_loss: 0.0170\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0039 - val_loss: 0.0168\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0038 - val_loss: 0.0166\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0037 - val_loss: 0.0164\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0036 - val_loss: 0.0162\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0036 - val_loss: 0.0160\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0035 - val_loss: 0.0159\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0034 - val_loss: 0.0157\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0034 - val_loss: 0.0156\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0033 - val_loss: 0.0155\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0033 - val_loss: 0.0153\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0033 - val_loss: 0.0152\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0032 - val_loss: 0.0151\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0032 - val_loss: 0.0150\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0031 - val_loss: 0.0149\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0031 - val_loss: 0.0148\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0031 - val_loss: 0.0147\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0031 - val_loss: 0.0147\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0030 - val_loss: 0.0146\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0030 - val_loss: 0.0145\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0030 - val_loss: 0.0144\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0030 - val_loss: 0.0143\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0143\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0142\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0142\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0029 - val_loss: 0.0141\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0141\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0028 - val_loss: 0.0140\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0028 - val_loss: 0.0140\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0028 - val_loss: 0.0139\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0028 - val_loss: 0.0139\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0028 - val_loss: 0.0138\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0028 - val_loss: 0.0138\n",
            "6/6 [==============================] - 0s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 2 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 3s 23ms/step - loss: 0.3761 - val_loss: 0.3334\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.3019 - val_loss: 0.2825\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.2532 - val_loss: 0.2438\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.2153 - val_loss: 0.2136\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.1853 - val_loss: 0.1894\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.1609 - val_loss: 0.1693\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.1409 - val_loss: 0.1522\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.1241 - val_loss: 0.1377\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.1099 - val_loss: 0.1252\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0979 - val_loss: 0.1145\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0875 - val_loss: 0.1053\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0787 - val_loss: 0.0974\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0712 - val_loss: 0.0906\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0647 - val_loss: 0.0847\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0592 - val_loss: 0.0795\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0544 - val_loss: 0.0750\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0503 - val_loss: 0.0712\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0468 - val_loss: 0.0678\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0437 - val_loss: 0.0648\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0410 - val_loss: 0.0622\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0386 - val_loss: 0.0598\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0366 - val_loss: 0.0577\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0347 - val_loss: 0.0559\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0331 - val_loss: 0.0542\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0316 - val_loss: 0.0527\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0303 - val_loss: 0.0513\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0292 - val_loss: 0.0501\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0281 - val_loss: 0.0490\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0272 - val_loss: 0.0479\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0263 - val_loss: 0.0469\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0255 - val_loss: 0.0460\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0247 - val_loss: 0.0452\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0240 - val_loss: 0.0444\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0233 - val_loss: 0.0436\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0227 - val_loss: 0.0429\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0221 - val_loss: 0.0422\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0216 - val_loss: 0.0416\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0211 - val_loss: 0.0409\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0206 - val_loss: 0.0403\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0201 - val_loss: 0.0398\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0196 - val_loss: 0.0392\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0192 - val_loss: 0.0387\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0188 - val_loss: 0.0382\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0184 - val_loss: 0.0377\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0180 - val_loss: 0.0372\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0177 - val_loss: 0.0368\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0173 - val_loss: 0.0363\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0170 - val_loss: 0.0359\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0167 - val_loss: 0.0355\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0164 - val_loss: 0.0351\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0161 - val_loss: 0.0347\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0158 - val_loss: 0.0343\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0155 - val_loss: 0.0340\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0153 - val_loss: 0.0336\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0150 - val_loss: 0.0333\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0148 - val_loss: 0.0330\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0146 - val_loss: 0.0326\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0143 - val_loss: 0.0323\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0141 - val_loss: 0.0321\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0139 - val_loss: 0.0318\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0137 - val_loss: 0.0315\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0135 - val_loss: 0.0312\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0133 - val_loss: 0.0310\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0131 - val_loss: 0.0307\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0129 - val_loss: 0.0305\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0128 - val_loss: 0.0303\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0126 - val_loss: 0.0300\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0124 - val_loss: 0.0298\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0123 - val_loss: 0.0296\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0121 - val_loss: 0.0294\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0120 - val_loss: 0.0292\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0118 - val_loss: 0.0290\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0117 - val_loss: 0.0289\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0115 - val_loss: 0.0287\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0114 - val_loss: 0.0285\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 3 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 3s 17ms/step - loss: 0.3644 - val_loss: 0.3473\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.2759 - val_loss: 0.2848\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.2247 - val_loss: 0.2411\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.1870 - val_loss: 0.2074\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.1575 - val_loss: 0.1803\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.1337 - val_loss: 0.1575\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.1140 - val_loss: 0.1382\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0975 - val_loss: 0.1217\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0835 - val_loss: 0.1076\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0716 - val_loss: 0.0955\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0615 - val_loss: 0.0851\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0529 - val_loss: 0.0761\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0455 - val_loss: 0.0684\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0392 - val_loss: 0.0617\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0339 - val_loss: 0.0560\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0293 - val_loss: 0.0512\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0256 - val_loss: 0.0471\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0224 - val_loss: 0.0436\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0198 - val_loss: 0.0406\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0176 - val_loss: 0.0380\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0157 - val_loss: 0.0359\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0142 - val_loss: 0.0340\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0129 - val_loss: 0.0324\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0118 - val_loss: 0.0311\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0109 - val_loss: 0.0299\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0102 - val_loss: 0.0289\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0096 - val_loss: 0.0281\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0091 - val_loss: 0.0273\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0087 - val_loss: 0.0267\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0083 - val_loss: 0.0261\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0080 - val_loss: 0.0257\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0078 - val_loss: 0.0252\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0076 - val_loss: 0.0248\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0074 - val_loss: 0.0245\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0072 - val_loss: 0.0242\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0071 - val_loss: 0.0239\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0070 - val_loss: 0.0236\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0069 - val_loss: 0.0234\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0068 - val_loss: 0.0231\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0067 - val_loss: 0.0229\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0067 - val_loss: 0.0227\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0066 - val_loss: 0.0225\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0066 - val_loss: 0.0224\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0065 - val_loss: 0.0222\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0065 - val_loss: 0.0220\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0064 - val_loss: 0.0219\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0064 - val_loss: 0.0217\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0063 - val_loss: 0.0216\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0063 - val_loss: 0.0214\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0063 - val_loss: 0.0213\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0062 - val_loss: 0.0212\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0062 - val_loss: 0.0211\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0062 - val_loss: 0.0209\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0061 - val_loss: 0.0208\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0061 - val_loss: 0.0207\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0061 - val_loss: 0.0206\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0061 - val_loss: 0.0205\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0060 - val_loss: 0.0204\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0060 - val_loss: 0.0203\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0060 - val_loss: 0.0202\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0060 - val_loss: 0.0201\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0059 - val_loss: 0.0200\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0059 - val_loss: 0.0199\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0059 - val_loss: 0.0198\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0059 - val_loss: 0.0197\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0059 - val_loss: 0.0196\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0058 - val_loss: 0.0195\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0058 - val_loss: 0.0194\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0058 - val_loss: 0.0194\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0058 - val_loss: 0.0193\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0058 - val_loss: 0.0192\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0057 - val_loss: 0.0191\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0057 - val_loss: 0.0190\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0057 - val_loss: 0.0190\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0057 - val_loss: 0.0189\n",
            "6/6 [==============================] - 0s 3ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 4 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 3s 22ms/step - loss: 0.2447 - val_loss: 0.1717\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.1397 - val_loss: 0.1183\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0938 - val_loss: 0.0892\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0677 - val_loss: 0.0709\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0506 - val_loss: 0.0579\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0394 - val_loss: 0.0496\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0323 - val_loss: 0.0442\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0275 - val_loss: 0.0404\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0243 - val_loss: 0.0376\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0220 - val_loss: 0.0355\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0203 - val_loss: 0.0338\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0189 - val_loss: 0.0324\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0178 - val_loss: 0.0312\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0169 - val_loss: 0.0301\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0161 - val_loss: 0.0292\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0155 - val_loss: 0.0283\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0149 - val_loss: 0.0276\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0144 - val_loss: 0.0269\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0139 - val_loss: 0.0263\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0135 - val_loss: 0.0257\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0132 - val_loss: 0.0252\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0129 - val_loss: 0.0247\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0126 - val_loss: 0.0243\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0123 - val_loss: 0.0239\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0121 - val_loss: 0.0236\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0119 - val_loss: 0.0232\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0117 - val_loss: 0.0229\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0115 - val_loss: 0.0227\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0113 - val_loss: 0.0224\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0112 - val_loss: 0.0222\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0111 - val_loss: 0.0220\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0109 - val_loss: 0.0218\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0108 - val_loss: 0.0217\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0107 - val_loss: 0.0215\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0106 - val_loss: 0.0214\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0105 - val_loss: 0.0212\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0104 - val_loss: 0.0211\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0103 - val_loss: 0.0210\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0102 - val_loss: 0.0209\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0101 - val_loss: 0.0208\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0101 - val_loss: 0.0207\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0100 - val_loss: 0.0207\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0099 - val_loss: 0.0206\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0099 - val_loss: 0.0205\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0098 - val_loss: 0.0204\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0204\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0203\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0096 - val_loss: 0.0203\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0096 - val_loss: 0.0202\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0095 - val_loss: 0.0202\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0094 - val_loss: 0.0202\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0094 - val_loss: 0.0201\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0093 - val_loss: 0.0201\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0093 - val_loss: 0.0201\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0092 - val_loss: 0.0201\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0092 - val_loss: 0.0200\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0091 - val_loss: 0.0200\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0091 - val_loss: 0.0200\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0090 - val_loss: 0.0200\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0090 - val_loss: 0.0200\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0089 - val_loss: 0.0200\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0089 - val_loss: 0.0200\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0089 - val_loss: 0.0200\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0088 - val_loss: 0.0200\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0088 - val_loss: 0.0200\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0087 - val_loss: 0.0200\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0087 - val_loss: 0.0200\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0086 - val_loss: 0.0200\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0086 - val_loss: 0.0200\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0086 - val_loss: 0.0200\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0085 - val_loss: 0.0200\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0085 - val_loss: 0.0200\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0084 - val_loss: 0.0200\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0084 - val_loss: 0.0200\n",
            "6/6 [==============================] - 1s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 5 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 3s 17ms/step - loss: 0.2345 - val_loss: 0.1919\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.1704 - val_loss: 0.1500\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.1329 - val_loss: 0.1221\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.1068 - val_loss: 0.1020\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0876 - val_loss: 0.0874\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0732 - val_loss: 0.0759\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0617 - val_loss: 0.0665\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0523 - val_loss: 0.0590\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0448 - val_loss: 0.0531\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0389 - val_loss: 0.0487\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0343 - val_loss: 0.0452\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0307 - val_loss: 0.0424\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0278 - val_loss: 0.0402\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0255 - val_loss: 0.0384\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0236 - val_loss: 0.0370\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0221 - val_loss: 0.0358\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0208 - val_loss: 0.0348\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0197 - val_loss: 0.0339\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0188 - val_loss: 0.0331\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0180 - val_loss: 0.0324\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0173 - val_loss: 0.0317\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0166 - val_loss: 0.0311\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0160 - val_loss: 0.0306\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0154 - val_loss: 0.0300\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0149 - val_loss: 0.0295\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0145 - val_loss: 0.0290\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0140 - val_loss: 0.0285\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0136 - val_loss: 0.0281\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0132 - val_loss: 0.0276\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0128 - val_loss: 0.0272\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0124 - val_loss: 0.0268\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0121 - val_loss: 0.0264\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0117 - val_loss: 0.0260\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0114 - val_loss: 0.0257\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0111 - val_loss: 0.0253\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0108 - val_loss: 0.0250\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0105 - val_loss: 0.0246\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0102 - val_loss: 0.0243\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0099 - val_loss: 0.0240\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0097 - val_loss: 0.0237\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0094 - val_loss: 0.0235\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0092 - val_loss: 0.0232\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0090 - val_loss: 0.0229\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0088 - val_loss: 0.0227\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0086 - val_loss: 0.0224\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0084 - val_loss: 0.0222\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0082 - val_loss: 0.0220\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0080 - val_loss: 0.0218\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0078 - val_loss: 0.0216\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0077 - val_loss: 0.0214\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0075 - val_loss: 0.0212\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0074 - val_loss: 0.0210\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0072 - val_loss: 0.0208\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0071 - val_loss: 0.0207\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0069 - val_loss: 0.0205\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0068 - val_loss: 0.0203\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0067 - val_loss: 0.0202\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0066 - val_loss: 0.0200\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0065 - val_loss: 0.0199\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0064 - val_loss: 0.0198\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0063 - val_loss: 0.0196\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0062 - val_loss: 0.0195\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0061 - val_loss: 0.0194\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0060 - val_loss: 0.0193\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0060 - val_loss: 0.0192\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0059 - val_loss: 0.0191\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0058 - val_loss: 0.0190\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0058 - val_loss: 0.0189\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0057 - val_loss: 0.0188\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0057 - val_loss: 0.0187\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0056 - val_loss: 0.0186\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0056 - val_loss: 0.0185\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0055 - val_loss: 0.0185\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0055 - val_loss: 0.0184\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0054 - val_loss: 0.0183\n",
            "6/6 [==============================] - 0s 3ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 6 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 3s 23ms/step - loss: 0.2386 - val_loss: 0.1502\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.1617 - val_loss: 0.1061\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.1197 - val_loss: 0.0795\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0920 - val_loss: 0.0619\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0723 - val_loss: 0.0497\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0576 - val_loss: 0.0413\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0467 - val_loss: 0.0355\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0386 - val_loss: 0.0316\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0326 - val_loss: 0.0291\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0281 - val_loss: 0.0275\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0247 - val_loss: 0.0267\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0222 - val_loss: 0.0263\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0203 - val_loss: 0.0262\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0189 - val_loss: 0.0263\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0179 - val_loss: 0.0265\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0171 - val_loss: 0.0268\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0165 - val_loss: 0.0271\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0160 - val_loss: 0.0273\n",
            "6/6 [==============================] - 0s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 7 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 2s 16ms/step - loss: 0.1622 - val_loss: 0.1128\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0881 - val_loss: 0.0730\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0581 - val_loss: 0.0536\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0424 - val_loss: 0.0426\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0330 - val_loss: 0.0365\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0270 - val_loss: 0.0326\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0229 - val_loss: 0.0299\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0200 - val_loss: 0.0280\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0178 - val_loss: 0.0265\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0162 - val_loss: 0.0253\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0149 - val_loss: 0.0243\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0139 - val_loss: 0.0234\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0130 - val_loss: 0.0226\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0122 - val_loss: 0.0220\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0116 - val_loss: 0.0213\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0110 - val_loss: 0.0207\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0104 - val_loss: 0.0202\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0099 - val_loss: 0.0197\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0095 - val_loss: 0.0192\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0090 - val_loss: 0.0188\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0087 - val_loss: 0.0184\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0083 - val_loss: 0.0180\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0080 - val_loss: 0.0177\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0077 - val_loss: 0.0174\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0074 - val_loss: 0.0171\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0072 - val_loss: 0.0168\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0070 - val_loss: 0.0165\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0067 - val_loss: 0.0163\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0065 - val_loss: 0.0160\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0064 - val_loss: 0.0158\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0062 - val_loss: 0.0156\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0060 - val_loss: 0.0154\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0059 - val_loss: 0.0152\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0058 - val_loss: 0.0151\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0057 - val_loss: 0.0150\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0055 - val_loss: 0.0148\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0054 - val_loss: 0.0147\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0054 - val_loss: 0.0146\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0053 - val_loss: 0.0145\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0052 - val_loss: 0.0144\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0051 - val_loss: 0.0143\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0050 - val_loss: 0.0142\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0050 - val_loss: 0.0141\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0049 - val_loss: 0.0140\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0049 - val_loss: 0.0140\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0048 - val_loss: 0.0139\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0047 - val_loss: 0.0139\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0047 - val_loss: 0.0138\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0047 - val_loss: 0.0138\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0046 - val_loss: 0.0137\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0046 - val_loss: 0.0137\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0045 - val_loss: 0.0136\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0045 - val_loss: 0.0136\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0045 - val_loss: 0.0136\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0044 - val_loss: 0.0135\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0044 - val_loss: 0.0135\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0043 - val_loss: 0.0135\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0043 - val_loss: 0.0135\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0043 - val_loss: 0.0134\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0043 - val_loss: 0.0134\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0042 - val_loss: 0.0134\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0042 - val_loss: 0.0134\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0042 - val_loss: 0.0134\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0041 - val_loss: 0.0134\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0041 - val_loss: 0.0134\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0041 - val_loss: 0.0134\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0041 - val_loss: 0.0133\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0040 - val_loss: 0.0133\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0040 - val_loss: 0.0133\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0040 - val_loss: 0.0133\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0040 - val_loss: 0.0133\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0039 - val_loss: 0.0133\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0039 - val_loss: 0.0133\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0039 - val_loss: 0.0133\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0039 - val_loss: 0.0133\n",
            "6/6 [==============================] - 1s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 8 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 4s 25ms/step - loss: 0.1875 - val_loss: 0.1888\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.1462 - val_loss: 0.1569\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.1162 - val_loss: 0.1321\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0935 - val_loss: 0.1143\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0768 - val_loss: 0.1015\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0644 - val_loss: 0.0922\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0552 - val_loss: 0.0854\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0485 - val_loss: 0.0804\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0435 - val_loss: 0.0767\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0397 - val_loss: 0.0738\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0368 - val_loss: 0.0715\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0345 - val_loss: 0.0696\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0326 - val_loss: 0.0680\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0310 - val_loss: 0.0665\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0296 - val_loss: 0.0651\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0283 - val_loss: 0.0638\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0272 - val_loss: 0.0626\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0261 - val_loss: 0.0614\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0252 - val_loss: 0.0603\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0243 - val_loss: 0.0592\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0234 - val_loss: 0.0581\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0226 - val_loss: 0.0571\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0219 - val_loss: 0.0562\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0213 - val_loss: 0.0552\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0206 - val_loss: 0.0543\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0200 - val_loss: 0.0534\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0194 - val_loss: 0.0526\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0189 - val_loss: 0.0517\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0183 - val_loss: 0.0509\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0178 - val_loss: 0.0501\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0174 - val_loss: 0.0493\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0169 - val_loss: 0.0485\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0164 - val_loss: 0.0478\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0160 - val_loss: 0.0470\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0156 - val_loss: 0.0463\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0152 - val_loss: 0.0456\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0148 - val_loss: 0.0450\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0144 - val_loss: 0.0443\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0141 - val_loss: 0.0437\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0137 - val_loss: 0.0430\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0134 - val_loss: 0.0424\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0131 - val_loss: 0.0419\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0127 - val_loss: 0.0413\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0124 - val_loss: 0.0407\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0122 - val_loss: 0.0402\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0119 - val_loss: 0.0397\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0116 - val_loss: 0.0392\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0114 - val_loss: 0.0387\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0111 - val_loss: 0.0382\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0109 - val_loss: 0.0377\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0107 - val_loss: 0.0373\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0104 - val_loss: 0.0368\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0102 - val_loss: 0.0364\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0100 - val_loss: 0.0360\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0098 - val_loss: 0.0355\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0096 - val_loss: 0.0351\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0094 - val_loss: 0.0347\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0093 - val_loss: 0.0343\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0091 - val_loss: 0.0340\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0089 - val_loss: 0.0336\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0088 - val_loss: 0.0332\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0086 - val_loss: 0.0329\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0085 - val_loss: 0.0325\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0083 - val_loss: 0.0322\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0082 - val_loss: 0.0319\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0081 - val_loss: 0.0316\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0080 - val_loss: 0.0313\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0078 - val_loss: 0.0310\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0077 - val_loss: 0.0307\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0076 - val_loss: 0.0304\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0075 - val_loss: 0.0301\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0074 - val_loss: 0.0298\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0073 - val_loss: 0.0295\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0072 - val_loss: 0.0293\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0071 - val_loss: 0.0290\n",
            "6/6 [==============================] - 0s 7ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 9 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 2s 16ms/step - loss: 0.3640 - val_loss: 0.3325\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.3056 - val_loss: 0.2902\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.2700 - val_loss: 0.2609\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.2430 - val_loss: 0.2375\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.2211 - val_loss: 0.2177\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.2025 - val_loss: 0.2011\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.1863 - val_loss: 0.1868\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.1721 - val_loss: 0.1744\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.1593 - val_loss: 0.1632\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.1478 - val_loss: 0.1526\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.1373 - val_loss: 0.1436\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.1277 - val_loss: 0.1350\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.1190 - val_loss: 0.1269\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.1109 - val_loss: 0.1196\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.1034 - val_loss: 0.1128\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0966 - val_loss: 0.1068\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0904 - val_loss: 0.1012\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0846 - val_loss: 0.0962\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0793 - val_loss: 0.0915\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0744 - val_loss: 0.0872\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0699 - val_loss: 0.0834\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0657 - val_loss: 0.0798\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0619 - val_loss: 0.0766\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0585 - val_loss: 0.0736\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0552 - val_loss: 0.0708\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0523 - val_loss: 0.0683\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0496 - val_loss: 0.0659\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0470 - val_loss: 0.0637\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0447 - val_loss: 0.0617\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0426 - val_loss: 0.0598\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0406 - val_loss: 0.0580\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0388 - val_loss: 0.0564\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0371 - val_loss: 0.0549\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0355 - val_loss: 0.0535\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0340 - val_loss: 0.0522\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0327 - val_loss: 0.0511\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0314 - val_loss: 0.0500\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0302 - val_loss: 0.0489\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0290 - val_loss: 0.0480\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0280 - val_loss: 0.0471\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0270 - val_loss: 0.0463\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0261 - val_loss: 0.0456\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0252 - val_loss: 0.0448\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0244 - val_loss: 0.0442\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0236 - val_loss: 0.0436\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0228 - val_loss: 0.0430\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0221 - val_loss: 0.0424\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0215 - val_loss: 0.0419\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0209 - val_loss: 0.0414\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0203 - val_loss: 0.0409\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0197 - val_loss: 0.0404\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0192 - val_loss: 0.0400\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0187 - val_loss: 0.0396\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0182 - val_loss: 0.0392\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0177 - val_loss: 0.0389\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0173 - val_loss: 0.0385\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0168 - val_loss: 0.0381\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0164 - val_loss: 0.0378\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0160 - val_loss: 0.0374\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0157 - val_loss: 0.0371\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0153 - val_loss: 0.0368\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0150 - val_loss: 0.0364\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0146 - val_loss: 0.0361\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0143 - val_loss: 0.0358\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0140 - val_loss: 0.0355\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0137 - val_loss: 0.0352\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0134 - val_loss: 0.0348\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0132 - val_loss: 0.0345\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0129 - val_loss: 0.0342\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0127 - val_loss: 0.0338\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0124 - val_loss: 0.0335\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0122 - val_loss: 0.0332\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0120 - val_loss: 0.0329\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0118 - val_loss: 0.0326\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0116 - val_loss: 0.0323\n",
            "6/6 [==============================] - 1s 5ms/step\n",
            "Best_hyper_parameters(LSTM): \n",
            " {'model': [16], 'optimizer': 'Adam', 'learning_rate': 0.1, 'batch_size': 16, 'best_avg_rmse': 1747.3345562410727}\n",
            "all_avg_rmse(LSTM): \n",
            " [[[2151.53105803 1948.50608993 1747.33455624]\n",
            "  [1755.1420313  1784.93700123 1842.69758623]\n",
            "  [1901.96844793 1932.01734758 1980.81661527]]\n",
            "\n",
            " [[1953.74242062 1956.238376   1949.64126976]\n",
            "  [1967.68300798 1994.21588929 2065.9992352 ]\n",
            "  [2239.31925768 2392.09613314 2583.46649851]]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model': [16],\n",
              " 'optimizer': 'Adam',\n",
              " 'learning_rate': 0.1,\n",
              " 'batch_size': 16,\n",
              " 'best_avg_rmse': 1747.3345562410727}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Case III: 32N-LSTM**"
      ],
      "metadata": {
        "id": "bI6xY--5xINm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layers = [32]\n",
        "time_step = 5\n",
        "optimizers_names = ['Adam', 'Adagrad']\n",
        "learning_rates =  [0.1, 0.01, 0.001]\n",
        "batch_sizes =  [4, 8, 16]\n",
        "epochs = 75\n",
        "num_replicates = 10\n",
        "test_split = 0.2\n",
        "val_split = 0.2\n",
        "\n",
        "train_data, test_data = data_split(data, test_split)\n",
        "\n",
        "lstm_N32_best_hyper_parameters = LSTM_Hyper_Parameter_Tuning(layers, train_data, time_step, val_split, optimizers_names, learning_rates, batch_sizes, epochs = epochs, num_replicates = num_replicates)\n",
        "lstm_N32_best_hyper_parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkRySw_ZwnUP",
        "outputId": "547a1915-365e-4e4d-9c97-a2be0cae886b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5.5190e-04 - val_loss: 0.0029\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 5.4328e-04 - val_loss: 0.0029\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adagrad optimizer 0.01 learning_rate 16 batch_size and 4 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 3s 18ms/step - loss: 0.0552 - val_loss: 0.0280\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0129 - val_loss: 0.0180\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0085 - val_loss: 0.0139\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0069 - val_loss: 0.0123\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0061 - val_loss: 0.0111\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0056 - val_loss: 0.0107\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0052 - val_loss: 0.0104\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0049 - val_loss: 0.0102\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0046 - val_loss: 0.0100\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0043 - val_loss: 0.0099\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0041 - val_loss: 0.0096\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0039 - val_loss: 0.0095\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0037 - val_loss: 0.0090\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0035 - val_loss: 0.0091\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0033 - val_loss: 0.0089\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0032 - val_loss: 0.0084\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0030 - val_loss: 0.0082\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0028 - val_loss: 0.0078\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0026 - val_loss: 0.0075\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0025 - val_loss: 0.0071\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0024 - val_loss: 0.0068\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0024 - val_loss: 0.0068\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0023 - val_loss: 0.0065\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0022 - val_loss: 0.0063\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0022 - val_loss: 0.0062\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0021 - val_loss: 0.0060\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0021 - val_loss: 0.0061\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0020 - val_loss: 0.0060\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0058\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0019 - val_loss: 0.0058\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0057\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0055\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0054\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0052\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0051\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0052\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0051\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0049\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0049\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0047\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0046\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0045\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0014 - val_loss: 0.0044\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0045\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0043\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0042\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0041\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0013 - val_loss: 0.0040\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0038\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0037\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0037\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0036\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 0.0037\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0011 - val_loss: 0.0034\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0011 - val_loss: 0.0034\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0011 - val_loss: 0.0033\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0010 - val_loss: 0.0033\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0010 - val_loss: 0.0032\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 9.9194e-04 - val_loss: 0.0031\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 9.7359e-04 - val_loss: 0.0030\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 9.5229e-04 - val_loss: 0.0030\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 9.3432e-04 - val_loss: 0.0029\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 9.1322e-04 - val_loss: 0.0028\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 8.9376e-04 - val_loss: 0.0027\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 8.7290e-04 - val_loss: 0.0027\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 8.5060e-04 - val_loss: 0.0026\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 8.5296e-04 - val_loss: 0.0026\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 8.2431e-04 - val_loss: 0.0025\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 8.0686e-04 - val_loss: 0.0025\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 7.8666e-04 - val_loss: 0.0025\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 7.8313e-04 - val_loss: 0.0024\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 7.5848e-04 - val_loss: 0.0023\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 7.4576e-04 - val_loss: 0.0022\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 7.3145e-04 - val_loss: 0.0022\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 7.1847e-04 - val_loss: 0.0021\n",
            "6/6 [==============================] - 1s 7ms/step\n",
            "Running for Adagrad optimizer 0.01 learning_rate 16 batch_size and 5 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 3s 18ms/step - loss: 0.1152 - val_loss: 0.0718\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0239 - val_loss: 0.0482\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0122 - val_loss: 0.0373\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0086 - val_loss: 0.0330\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0073 - val_loss: 0.0313\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0066 - val_loss: 0.0317\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0062 - val_loss: 0.0302\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0058 - val_loss: 0.0292\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0055 - val_loss: 0.0297\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0053 - val_loss: 0.0302\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0051 - val_loss: 0.0292\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0049 - val_loss: 0.0282\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0048 - val_loss: 0.0279\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0046 - val_loss: 0.0279\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0045 - val_loss: 0.0273\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0044 - val_loss: 0.0263\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0043 - val_loss: 0.0257\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0042 - val_loss: 0.0254\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0041 - val_loss: 0.0256\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0040 - val_loss: 0.0248\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0039 - val_loss: 0.0255\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0038 - val_loss: 0.0235\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0037 - val_loss: 0.0231\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0036 - val_loss: 0.0216\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0035 - val_loss: 0.0215\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0034 - val_loss: 0.0209\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0033 - val_loss: 0.0208\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0032 - val_loss: 0.0206\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0031 - val_loss: 0.0194\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0031 - val_loss: 0.0198\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0030 - val_loss: 0.0198\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0185\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0028 - val_loss: 0.0189\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0028 - val_loss: 0.0180\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0027 - val_loss: 0.0184\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0027 - val_loss: 0.0179\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0026 - val_loss: 0.0169\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0026 - val_loss: 0.0168\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0025 - val_loss: 0.0167\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0024 - val_loss: 0.0167\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0024 - val_loss: 0.0166\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0023 - val_loss: 0.0156\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0023 - val_loss: 0.0166\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0022 - val_loss: 0.0163\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0022 - val_loss: 0.0156\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0022 - val_loss: 0.0151\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0021 - val_loss: 0.0151\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0021 - val_loss: 0.0144\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0020 - val_loss: 0.0148\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0146\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0145\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0135\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0136\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0018 - val_loss: 0.0135\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0129\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0128\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0127\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0017 - val_loss: 0.0125\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0119\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0117\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0120\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0016 - val_loss: 0.0115\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0118\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0111\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0015 - val_loss: 0.0108\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0108\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0109\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0014 - val_loss: 0.0108\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0105\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0107\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0101\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0013 - val_loss: 0.0105\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0096\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0099\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0012 - val_loss: 0.0096\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adagrad optimizer 0.01 learning_rate 16 batch_size and 6 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 4s 25ms/step - loss: 0.0865 - val_loss: 0.0495\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0158 - val_loss: 0.0342\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0095 - val_loss: 0.0269\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0069 - val_loss: 0.0236\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0054 - val_loss: 0.0210\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0051 - val_loss: 0.0203\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0049 - val_loss: 0.0198\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0047 - val_loss: 0.0199\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0045 - val_loss: 0.0196\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0043 - val_loss: 0.0196\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0042 - val_loss: 0.0193\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0040 - val_loss: 0.0192\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0039 - val_loss: 0.0189\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0038 - val_loss: 0.0191\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0037 - val_loss: 0.0189\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0036 - val_loss: 0.0187\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0035 - val_loss: 0.0179\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0034 - val_loss: 0.0188\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0034 - val_loss: 0.0180\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0033 - val_loss: 0.0175\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0032 - val_loss: 0.0175\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0031 - val_loss: 0.0175\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0031 - val_loss: 0.0172\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0030 - val_loss: 0.0170\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0030 - val_loss: 0.0170\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0166\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0168\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0028 - val_loss: 0.0164\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0028 - val_loss: 0.0165\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0027 - val_loss: 0.0162\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0027 - val_loss: 0.0157\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0157\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0026 - val_loss: 0.0157\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0025 - val_loss: 0.0156\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0025 - val_loss: 0.0157\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0025 - val_loss: 0.0155\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0024 - val_loss: 0.0151\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0024 - val_loss: 0.0147\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0023 - val_loss: 0.0146\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0023 - val_loss: 0.0145\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0023 - val_loss: 0.0142\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0022 - val_loss: 0.0139\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0022 - val_loss: 0.0143\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0022 - val_loss: 0.0140\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0021 - val_loss: 0.0137\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0021 - val_loss: 0.0136\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0134\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0020 - val_loss: 0.0133\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0020 - val_loss: 0.0131\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0020 - val_loss: 0.0128\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0019 - val_loss: 0.0128\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0126\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0123\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0122\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0118\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0118\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0117\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0113\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0112\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0110\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0109\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0016 - val_loss: 0.0110\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0016 - val_loss: 0.0108\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0104\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0103\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0106\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0105\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0015 - val_loss: 0.0098\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0015 - val_loss: 0.0097\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0014 - val_loss: 0.0097\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0014 - val_loss: 0.0100\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0014 - val_loss: 0.0095\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0014 - val_loss: 0.0095\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0091\n",
            "6/6 [==============================] - 0s 4ms/step\n",
            "Running for Adagrad optimizer 0.01 learning_rate 16 batch_size and 7 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 4s 28ms/step - loss: 0.0749 - val_loss: 0.0300\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0134 - val_loss: 0.0201\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0080 - val_loss: 0.0159\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0059 - val_loss: 0.0144\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0050 - val_loss: 0.0133\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0045 - val_loss: 0.0131\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0042 - val_loss: 0.0128\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0039 - val_loss: 0.0128\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0037 - val_loss: 0.0128\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0035 - val_loss: 0.0123\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0033 - val_loss: 0.0118\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0031 - val_loss: 0.0119\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0030 - val_loss: 0.0116\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0029 - val_loss: 0.0115\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0027 - val_loss: 0.0116\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0026 - val_loss: 0.0110\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0025 - val_loss: 0.0106\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0025 - val_loss: 0.0105\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0024 - val_loss: 0.0106\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0023 - val_loss: 0.0107\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0022 - val_loss: 0.0106\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0021 - val_loss: 0.0104\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0021 - val_loss: 0.0096\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0096\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0019 - val_loss: 0.0093\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0090\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0018 - val_loss: 0.0095\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0091\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0084\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0017 - val_loss: 0.0080\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0017 - val_loss: 0.0081\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0016 - val_loss: 0.0082\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0078\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0079\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0015 - val_loss: 0.0078\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0015 - val_loss: 0.0073\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0071\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0014 - val_loss: 0.0072\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0065\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0069\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0065\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0062\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0012 - val_loss: 0.0061\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0012 - val_loss: 0.0060\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0012 - val_loss: 0.0060\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0011 - val_loss: 0.0057\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0011 - val_loss: 0.0056\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0011 - val_loss: 0.0054\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0011 - val_loss: 0.0051\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0010 - val_loss: 0.0051\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0010 - val_loss: 0.0049\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0010 - val_loss: 0.0047\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 9.8338e-04 - val_loss: 0.0046\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 9.5401e-04 - val_loss: 0.0047\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 9.3434e-04 - val_loss: 0.0045\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 9.1480e-04 - val_loss: 0.0043\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 8.9773e-04 - val_loss: 0.0044\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 8.7328e-04 - val_loss: 0.0041\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 8.5649e-04 - val_loss: 0.0041\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 8.3559e-04 - val_loss: 0.0038\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 8.2142e-04 - val_loss: 0.0037\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 8.0183e-04 - val_loss: 0.0037\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 7.8392e-04 - val_loss: 0.0036\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 7.6594e-04 - val_loss: 0.0033\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 7.4787e-04 - val_loss: 0.0034\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 7.3587e-04 - val_loss: 0.0034\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 7.2097e-04 - val_loss: 0.0032\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 7.0418e-04 - val_loss: 0.0031\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 6.8891e-04 - val_loss: 0.0030\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 6.7274e-04 - val_loss: 0.0029\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 6.6034e-04 - val_loss: 0.0028\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 6.4612e-04 - val_loss: 0.0028\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 6.3389e-04 - val_loss: 0.0026\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 6.2382e-04 - val_loss: 0.0026\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 6.0789e-04 - val_loss: 0.0028\n",
            "6/6 [==============================] - 0s 3ms/step\n",
            "Running for Adagrad optimizer 0.01 learning_rate 16 batch_size and 8 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 3s 20ms/step - loss: 0.0285 - val_loss: 0.0171\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0060 - val_loss: 0.0120\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0042 - val_loss: 0.0114\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0037 - val_loss: 0.0117\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0033 - val_loss: 0.0117\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0031 - val_loss: 0.0115\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0030 - val_loss: 0.0102\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0028 - val_loss: 0.0109\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0027 - val_loss: 0.0105\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0026 - val_loss: 0.0094\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0024 - val_loss: 0.0086\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0023 - val_loss: 0.0090\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0022 - val_loss: 0.0088\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0089\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0019 - val_loss: 0.0080\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0077\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0078\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0073\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0016 - val_loss: 0.0067\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0015 - val_loss: 0.0065\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0015 - val_loss: 0.0063\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 0.0061\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0014 - val_loss: 0.0057\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0058\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0013 - val_loss: 0.0051\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0012 - val_loss: 0.0050\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0012 - val_loss: 0.0050\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0011 - val_loss: 0.0048\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0011 - val_loss: 0.0046\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0010 - val_loss: 0.0043\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0010 - val_loss: 0.0042\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 9.7859e-04 - val_loss: 0.0041\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 9.5226e-04 - val_loss: 0.0041\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 9.1106e-04 - val_loss: 0.0039\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 8.8429e-04 - val_loss: 0.0038\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 8.4996e-04 - val_loss: 0.0035\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 8.2055e-04 - val_loss: 0.0035\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 7.9127e-04 - val_loss: 0.0033\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 7.5816e-04 - val_loss: 0.0034\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 7.3494e-04 - val_loss: 0.0033\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 7.1150e-04 - val_loss: 0.0032\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.7934e-04 - val_loss: 0.0034\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 6.6537e-04 - val_loss: 0.0032\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 6.4766e-04 - val_loss: 0.0028\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.2641e-04 - val_loss: 0.0029\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 6.0347e-04 - val_loss: 0.0027\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 5.8381e-04 - val_loss: 0.0028\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 5.6625e-04 - val_loss: 0.0027\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 5.5030e-04 - val_loss: 0.0027\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 5.3552e-04 - val_loss: 0.0026\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 5.1958e-04 - val_loss: 0.0027\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 5.0437e-04 - val_loss: 0.0026\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 4.9269e-04 - val_loss: 0.0026\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.8313e-04 - val_loss: 0.0025\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.6806e-04 - val_loss: 0.0025\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.5986e-04 - val_loss: 0.0025\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 4.4736e-04 - val_loss: 0.0025\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.3829e-04 - val_loss: 0.0024\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.3143e-04 - val_loss: 0.0025\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.1592e-04 - val_loss: 0.0025\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.1513e-04 - val_loss: 0.0025\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.0480e-04 - val_loss: 0.0025\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 3.9404e-04 - val_loss: 0.0024\n",
            "6/6 [==============================] - 0s 3ms/step\n",
            "Running for Adagrad optimizer 0.01 learning_rate 16 batch_size and 9 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 3s 20ms/step - loss: 0.0189 - val_loss: 0.0217\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0064 - val_loss: 0.0165\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0051 - val_loss: 0.0154\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0047 - val_loss: 0.0145\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0043 - val_loss: 0.0146\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0041 - val_loss: 0.0141\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0039 - val_loss: 0.0142\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0037 - val_loss: 0.0139\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0035 - val_loss: 0.0143\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0033 - val_loss: 0.0134\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0032 - val_loss: 0.0133\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0031 - val_loss: 0.0131\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0030 - val_loss: 0.0124\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0029 - val_loss: 0.0123\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0028 - val_loss: 0.0121\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0027 - val_loss: 0.0116\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0026 - val_loss: 0.0112\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0025 - val_loss: 0.0115\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0024 - val_loss: 0.0105\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0023 - val_loss: 0.0103\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0023 - val_loss: 0.0100\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0022 - val_loss: 0.0099\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0094\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0093\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0020 - val_loss: 0.0093\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0020 - val_loss: 0.0090\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0019 - val_loss: 0.0085\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0084\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0082\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0083\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0076\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0016 - val_loss: 0.0076\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0016 - val_loss: 0.0074\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0076\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 0.0070\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0015 - val_loss: 0.0069\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0014 - val_loss: 0.0067\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0014 - val_loss: 0.0066\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0013 - val_loss: 0.0064\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0013 - val_loss: 0.0062\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0060\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0062\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0012 - val_loss: 0.0057\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0057\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0011 - val_loss: 0.0054\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0011 - val_loss: 0.0054\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0011 - val_loss: 0.0052\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0011 - val_loss: 0.0051\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0010 - val_loss: 0.0050\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 9.8981e-04 - val_loss: 0.0049\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 9.6865e-04 - val_loss: 0.0048\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 9.3893e-04 - val_loss: 0.0046\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 9.1333e-04 - val_loss: 0.0045\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 8.9023e-04 - val_loss: 0.0045\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 8.6975e-04 - val_loss: 0.0044\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 8.5238e-04 - val_loss: 0.0043\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 8.2455e-04 - val_loss: 0.0042\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 8.0862e-04 - val_loss: 0.0040\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 7.8338e-04 - val_loss: 0.0040\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 7.6151e-04 - val_loss: 0.0038\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 7.4505e-04 - val_loss: 0.0039\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 7.2766e-04 - val_loss: 0.0037\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 7.1116e-04 - val_loss: 0.0036\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.9911e-04 - val_loss: 0.0037\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 6.8006e-04 - val_loss: 0.0035\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 6.6602e-04 - val_loss: 0.0034\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 6.4883e-04 - val_loss: 0.0034\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 6.3102e-04 - val_loss: 0.0032\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.1693e-04 - val_loss: 0.0032\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.0196e-04 - val_loss: 0.0030\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 5.9291e-04 - val_loss: 0.0032\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5.8462e-04 - val_loss: 0.0030\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 5.7161e-04 - val_loss: 0.0030\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5.5918e-04 - val_loss: 0.0029\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 5.4906e-04 - val_loss: 0.0028\n",
            "6/6 [==============================] - 1s 3ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 0 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 5s 10ms/step - loss: 0.1938 - val_loss: 0.1157\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0667 - val_loss: 0.0704\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0373 - val_loss: 0.0536\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0254 - val_loss: 0.0454\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0196 - val_loss: 0.0411\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0160 - val_loss: 0.0382\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0137 - val_loss: 0.0360\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0119 - val_loss: 0.0340\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0104 - val_loss: 0.0322\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0092 - val_loss: 0.0306\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0082 - val_loss: 0.0291\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0074 - val_loss: 0.0279\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0067 - val_loss: 0.0266\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0062 - val_loss: 0.0254\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0057 - val_loss: 0.0244\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0053 - val_loss: 0.0236\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0050 - val_loss: 0.0227\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0048 - val_loss: 0.0222\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0045 - val_loss: 0.0216\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0044 - val_loss: 0.0211\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0042 - val_loss: 0.0206\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0041 - val_loss: 0.0203\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0199\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0039 - val_loss: 0.0195\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0038 - val_loss: 0.0194\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0038 - val_loss: 0.0190\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0037 - val_loss: 0.0188\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0187\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0185\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0182\n",
            "Epoch 31/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0182\n",
            "Epoch 32/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0035 - val_loss: 0.0180\n",
            "Epoch 33/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0035 - val_loss: 0.0180\n",
            "Epoch 34/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0035 - val_loss: 0.0179\n",
            "Epoch 35/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0035 - val_loss: 0.0178\n",
            "Epoch 36/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0035 - val_loss: 0.0177\n",
            "Epoch 37/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0034 - val_loss: 0.0176\n",
            "Epoch 38/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0034 - val_loss: 0.0174\n",
            "Epoch 39/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0034 - val_loss: 0.0174\n",
            "Epoch 40/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0034 - val_loss: 0.0174\n",
            "Epoch 41/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0034 - val_loss: 0.0174\n",
            "Epoch 42/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0173\n",
            "Epoch 43/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0172\n",
            "Epoch 44/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0171\n",
            "Epoch 45/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0170\n",
            "Epoch 46/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0170\n",
            "Epoch 47/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0169\n",
            "Epoch 48/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0168\n",
            "Epoch 49/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0168\n",
            "Epoch 50/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0168\n",
            "Epoch 51/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0167\n",
            "Epoch 52/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0166\n",
            "Epoch 53/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0166\n",
            "Epoch 54/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0165\n",
            "Epoch 55/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0165\n",
            "Epoch 56/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0165\n",
            "Epoch 57/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0031 - val_loss: 0.0164\n",
            "Epoch 58/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0164\n",
            "Epoch 59/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0164\n",
            "Epoch 60/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0164\n",
            "Epoch 61/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0163\n",
            "Epoch 62/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0162\n",
            "Epoch 63/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0162\n",
            "Epoch 64/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0162\n",
            "Epoch 65/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0162\n",
            "Epoch 66/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0161\n",
            "Epoch 67/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0160\n",
            "Epoch 68/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0160\n",
            "Epoch 69/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0160\n",
            "Epoch 70/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0159\n",
            "Epoch 71/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0158\n",
            "Epoch 72/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0158\n",
            "Epoch 73/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0158\n",
            "Epoch 74/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0158\n",
            "Epoch 75/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0157\n",
            "6/6 [==============================] - 0s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 1 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 4s 11ms/step - loss: 0.1608 - val_loss: 0.1105\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0659 - val_loss: 0.0658\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0402 - val_loss: 0.0491\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0291 - val_loss: 0.0405\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0230 - val_loss: 0.0352\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0188 - val_loss: 0.0316\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0158 - val_loss: 0.0287\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0135 - val_loss: 0.0265\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0118 - val_loss: 0.0247\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0105 - val_loss: 0.0233\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0094 - val_loss: 0.0221\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0086 - val_loss: 0.0211\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0080 - val_loss: 0.0203\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0075 - val_loss: 0.0197\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0071 - val_loss: 0.0192\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0068 - val_loss: 0.0188\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0066 - val_loss: 0.0184\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0064 - val_loss: 0.0181\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0062 - val_loss: 0.0179\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0061 - val_loss: 0.0176\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0060 - val_loss: 0.0175\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0059 - val_loss: 0.0173\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0058 - val_loss: 0.0172\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0057 - val_loss: 0.0171\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0056 - val_loss: 0.0170\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0055 - val_loss: 0.0170\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0055 - val_loss: 0.0169\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0054 - val_loss: 0.0169\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0053 - val_loss: 0.0168\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0053 - val_loss: 0.0168\n",
            "Epoch 31/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0052 - val_loss: 0.0168\n",
            "Epoch 32/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0052 - val_loss: 0.0168\n",
            "Epoch 33/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0051 - val_loss: 0.0168\n",
            "Epoch 34/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0050 - val_loss: 0.0167\n",
            "Epoch 35/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0050 - val_loss: 0.0167\n",
            "Epoch 36/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0049 - val_loss: 0.0167\n",
            "Epoch 37/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0049 - val_loss: 0.0167\n",
            "Epoch 38/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0048 - val_loss: 0.0167\n",
            "Epoch 39/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0048 - val_loss: 0.0167\n",
            "Epoch 40/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0048 - val_loss: 0.0167\n",
            "Epoch 41/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0047 - val_loss: 0.0166\n",
            "Epoch 42/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0047 - val_loss: 0.0167\n",
            "Epoch 43/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0046 - val_loss: 0.0166\n",
            "Epoch 44/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0046 - val_loss: 0.0166\n",
            "Epoch 45/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0046 - val_loss: 0.0166\n",
            "Epoch 46/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0045 - val_loss: 0.0165\n",
            "Epoch 47/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0045 - val_loss: 0.0165\n",
            "Epoch 48/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0044 - val_loss: 0.0165\n",
            "Epoch 49/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0044 - val_loss: 0.0165\n",
            "Epoch 50/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0044 - val_loss: 0.0164\n",
            "Epoch 51/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0043 - val_loss: 0.0164\n",
            "Epoch 52/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0043 - val_loss: 0.0164\n",
            "Epoch 53/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0043 - val_loss: 0.0163\n",
            "Epoch 54/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0043 - val_loss: 0.0163\n",
            "Epoch 55/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0042 - val_loss: 0.0163\n",
            "Epoch 56/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0042 - val_loss: 0.0163\n",
            "Epoch 57/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0042 - val_loss: 0.0162\n",
            "Epoch 58/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0041 - val_loss: 0.0162\n",
            "Epoch 59/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0041 - val_loss: 0.0161\n",
            "Epoch 60/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0041 - val_loss: 0.0161\n",
            "Epoch 61/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0041 - val_loss: 0.0161\n",
            "Epoch 62/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0160\n",
            "Epoch 63/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0160\n",
            "Epoch 64/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0160\n",
            "Epoch 65/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0039 - val_loss: 0.0159\n",
            "Epoch 66/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0039 - val_loss: 0.0159\n",
            "Epoch 67/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0039 - val_loss: 0.0158\n",
            "Epoch 68/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0039 - val_loss: 0.0158\n",
            "Epoch 69/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0039 - val_loss: 0.0157\n",
            "Epoch 70/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0038 - val_loss: 0.0157\n",
            "Epoch 71/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0038 - val_loss: 0.0157\n",
            "Epoch 72/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0038 - val_loss: 0.0157\n",
            "Epoch 73/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0038 - val_loss: 0.0156\n",
            "Epoch 74/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0037 - val_loss: 0.0155\n",
            "Epoch 75/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0037 - val_loss: 0.0155\n",
            "6/6 [==============================] - 1s 7ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 2 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 4s 9ms/step - loss: 0.0566 - val_loss: 0.0374\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0275\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0147 - val_loss: 0.0220\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0095 - val_loss: 0.0184\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0066 - val_loss: 0.0159\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0051 - val_loss: 0.0140\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0042 - val_loss: 0.0128\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0037 - val_loss: 0.0120\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0114\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0031 - val_loss: 0.0110\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0105\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0102\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0099\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0099\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0026 - val_loss: 0.0097\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0025 - val_loss: 0.0097\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0024 - val_loss: 0.0096\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0023 - val_loss: 0.0095\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0023 - val_loss: 0.0096\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0022 - val_loss: 0.0096\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0022 - val_loss: 0.0097\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0021 - val_loss: 0.0095\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0021 - val_loss: 0.0095\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0094\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0020 - val_loss: 0.0092\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0020 - val_loss: 0.0093\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0093\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0093\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0018 - val_loss: 0.0091\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0018 - val_loss: 0.0090\n",
            "Epoch 31/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0018 - val_loss: 0.0089\n",
            "Epoch 32/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0018 - val_loss: 0.0089\n",
            "Epoch 33/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0017 - val_loss: 0.0089\n",
            "Epoch 34/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0017 - val_loss: 0.0088\n",
            "Epoch 35/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0017 - val_loss: 0.0088\n",
            "Epoch 36/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0016 - val_loss: 0.0087\n",
            "Epoch 37/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0016 - val_loss: 0.0086\n",
            "Epoch 38/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0016 - val_loss: 0.0086\n",
            "Epoch 39/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0016 - val_loss: 0.0085\n",
            "Epoch 40/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0015 - val_loss: 0.0086\n",
            "Epoch 41/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0015 - val_loss: 0.0085\n",
            "Epoch 42/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0015 - val_loss: 0.0085\n",
            "Epoch 43/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0015 - val_loss: 0.0085\n",
            "Epoch 44/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0015 - val_loss: 0.0084\n",
            "Epoch 45/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0015 - val_loss: 0.0082\n",
            "Epoch 46/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0014 - val_loss: 0.0083\n",
            "Epoch 47/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0014 - val_loss: 0.0082\n",
            "Epoch 48/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0014 - val_loss: 0.0081\n",
            "Epoch 49/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0014 - val_loss: 0.0081\n",
            "Epoch 50/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0014 - val_loss: 0.0080\n",
            "Epoch 51/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0014 - val_loss: 0.0081\n",
            "Epoch 52/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0014 - val_loss: 0.0080\n",
            "Epoch 53/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0013 - val_loss: 0.0079\n",
            "Epoch 54/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0013 - val_loss: 0.0078\n",
            "Epoch 55/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0013 - val_loss: 0.0078\n",
            "Epoch 56/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0013 - val_loss: 0.0078\n",
            "Epoch 57/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0013 - val_loss: 0.0078\n",
            "Epoch 58/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0013 - val_loss: 0.0077\n",
            "Epoch 59/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0013 - val_loss: 0.0077\n",
            "Epoch 60/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0013 - val_loss: 0.0076\n",
            "Epoch 61/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0012 - val_loss: 0.0076\n",
            "Epoch 62/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0012 - val_loss: 0.0075\n",
            "Epoch 63/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0012 - val_loss: 0.0074\n",
            "Epoch 64/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0012 - val_loss: 0.0074\n",
            "Epoch 65/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0012 - val_loss: 0.0074\n",
            "Epoch 66/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0012 - val_loss: 0.0074\n",
            "Epoch 67/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0012 - val_loss: 0.0072\n",
            "Epoch 68/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0012 - val_loss: 0.0072\n",
            "Epoch 69/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0012 - val_loss: 0.0072\n",
            "Epoch 70/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0012 - val_loss: 0.0071\n",
            "Epoch 71/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0012 - val_loss: 0.0071\n",
            "Epoch 72/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0011 - val_loss: 0.0070\n",
            "Epoch 73/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0011 - val_loss: 0.0071\n",
            "Epoch 74/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0011 - val_loss: 0.0070\n",
            "Epoch 75/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0011 - val_loss: 0.0069\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 3 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 5s 12ms/step - loss: 0.3177 - val_loss: 0.1774\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.1145 - val_loss: 0.0793\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0505 - val_loss: 0.0443\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0292 - val_loss: 0.0313\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0212 - val_loss: 0.0253\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0174 - val_loss: 0.0218\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0150 - val_loss: 0.0194\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0132 - val_loss: 0.0174\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0117 - val_loss: 0.0159\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0106 - val_loss: 0.0147\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0096 - val_loss: 0.0137\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0087 - val_loss: 0.0128\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0079 - val_loss: 0.0121\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0072 - val_loss: 0.0116\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0067 - val_loss: 0.0112\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0063 - val_loss: 0.0108\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0059 - val_loss: 0.0105\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0056 - val_loss: 0.0102\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0054 - val_loss: 0.0100\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0052 - val_loss: 0.0098\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0050 - val_loss: 0.0097\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0049 - val_loss: 0.0096\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0047 - val_loss: 0.0094\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0046 - val_loss: 0.0094\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0045 - val_loss: 0.0093\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0045 - val_loss: 0.0092\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0044 - val_loss: 0.0091\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0043 - val_loss: 0.0091\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0042 - val_loss: 0.0090\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0042 - val_loss: 0.0090\n",
            "Epoch 31/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0041 - val_loss: 0.0090\n",
            "Epoch 32/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0040 - val_loss: 0.0089\n",
            "Epoch 33/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0040 - val_loss: 0.0089\n",
            "Epoch 34/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0039 - val_loss: 0.0089\n",
            "Epoch 35/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0039 - val_loss: 0.0088\n",
            "Epoch 36/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0038 - val_loss: 0.0088\n",
            "Epoch 37/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0037 - val_loss: 0.0088\n",
            "Epoch 38/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0037 - val_loss: 0.0088\n",
            "Epoch 39/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0036 - val_loss: 0.0087\n",
            "Epoch 40/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0036 - val_loss: 0.0087\n",
            "Epoch 41/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0036 - val_loss: 0.0087\n",
            "Epoch 42/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0035 - val_loss: 0.0086\n",
            "Epoch 43/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0035 - val_loss: 0.0086\n",
            "Epoch 44/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0034 - val_loss: 0.0086\n",
            "Epoch 45/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0034 - val_loss: 0.0085\n",
            "Epoch 46/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0034 - val_loss: 0.0085\n",
            "Epoch 47/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0084\n",
            "Epoch 48/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0033 - val_loss: 0.0084\n",
            "Epoch 49/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0083\n",
            "Epoch 50/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0083\n",
            "Epoch 51/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0083\n",
            "Epoch 52/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0082\n",
            "Epoch 53/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0031 - val_loss: 0.0082\n",
            "Epoch 54/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0082\n",
            "Epoch 55/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0081\n",
            "Epoch 56/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0081\n",
            "Epoch 57/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0030 - val_loss: 0.0081\n",
            "Epoch 58/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0030 - val_loss: 0.0080\n",
            "Epoch 59/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0080\n",
            "Epoch 60/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0080\n",
            "Epoch 61/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0029 - val_loss: 0.0079\n",
            "Epoch 62/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0079\n",
            "Epoch 63/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0079\n",
            "Epoch 64/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0078\n",
            "Epoch 65/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0078\n",
            "Epoch 66/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0078\n",
            "Epoch 67/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0077\n",
            "Epoch 68/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0077\n",
            "Epoch 69/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0077\n",
            "Epoch 70/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0077\n",
            "Epoch 71/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0076\n",
            "Epoch 72/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0076\n",
            "Epoch 73/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0076\n",
            "Epoch 74/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0075\n",
            "Epoch 75/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0075\n",
            "6/6 [==============================] - 0s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 4 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 3s 9ms/step - loss: 0.1284 - val_loss: 0.0579\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0352 - val_loss: 0.0389\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0235 - val_loss: 0.0342\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0194 - val_loss: 0.0314\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0167 - val_loss: 0.0293\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0147 - val_loss: 0.0276\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0132 - val_loss: 0.0264\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0119 - val_loss: 0.0253\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0109 - val_loss: 0.0244\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0100 - val_loss: 0.0236\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0093 - val_loss: 0.0230\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0088 - val_loss: 0.0225\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0083 - val_loss: 0.0220\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0079 - val_loss: 0.0216\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0075 - val_loss: 0.0214\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0072 - val_loss: 0.0211\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0069 - val_loss: 0.0208\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0067 - val_loss: 0.0206\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0065 - val_loss: 0.0205\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0063 - val_loss: 0.0202\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0061 - val_loss: 0.0201\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0060 - val_loss: 0.0200\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0058 - val_loss: 0.0199\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0057 - val_loss: 0.0197\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0056 - val_loss: 0.0197\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0054 - val_loss: 0.0196\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0053 - val_loss: 0.0195\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0052 - val_loss: 0.0194\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0051 - val_loss: 0.0194\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0050 - val_loss: 0.0193\n",
            "Epoch 31/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0049 - val_loss: 0.0192\n",
            "Epoch 32/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0049 - val_loss: 0.0191\n",
            "Epoch 33/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0048 - val_loss: 0.0191\n",
            "Epoch 34/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0047 - val_loss: 0.0191\n",
            "Epoch 35/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0046 - val_loss: 0.0189\n",
            "Epoch 36/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0046 - val_loss: 0.0189\n",
            "Epoch 37/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0045 - val_loss: 0.0189\n",
            "Epoch 38/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0044 - val_loss: 0.0189\n",
            "Epoch 39/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0044 - val_loss: 0.0189\n",
            "Epoch 40/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0043 - val_loss: 0.0188\n",
            "Epoch 41/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0043 - val_loss: 0.0188\n",
            "Epoch 42/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0042 - val_loss: 0.0187\n",
            "Epoch 43/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0041 - val_loss: 0.0186\n",
            "Epoch 44/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0041 - val_loss: 0.0187\n",
            "Epoch 45/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0187\n",
            "Epoch 46/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0186\n",
            "Epoch 47/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0039 - val_loss: 0.0186\n",
            "Epoch 48/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0039 - val_loss: 0.0186\n",
            "Epoch 49/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0039 - val_loss: 0.0185\n",
            "Epoch 50/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0038 - val_loss: 0.0185\n",
            "Epoch 51/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0038 - val_loss: 0.0184\n",
            "Epoch 52/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0037 - val_loss: 0.0184\n",
            "Epoch 53/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0037 - val_loss: 0.0184\n",
            "Epoch 54/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0036 - val_loss: 0.0184\n",
            "Epoch 55/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0184\n",
            "Epoch 56/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0036 - val_loss: 0.0183\n",
            "Epoch 57/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0035 - val_loss: 0.0183\n",
            "Epoch 58/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0035 - val_loss: 0.0182\n",
            "Epoch 59/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0035 - val_loss: 0.0183\n",
            "Epoch 60/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0034 - val_loss: 0.0183\n",
            "Epoch 61/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0034 - val_loss: 0.0181\n",
            "Epoch 62/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0034 - val_loss: 0.0181\n",
            "Epoch 63/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0181\n",
            "Epoch 64/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0181\n",
            "Epoch 65/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0033 - val_loss: 0.0180\n",
            "Epoch 66/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0179\n",
            "Epoch 67/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0179\n",
            "Epoch 68/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0180\n",
            "Epoch 69/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0179\n",
            "Epoch 70/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0179\n",
            "Epoch 71/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0178\n",
            "Epoch 72/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0178\n",
            "Epoch 73/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0177\n",
            "Epoch 74/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0177\n",
            "Epoch 75/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0177\n",
            "6/6 [==============================] - 1s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 5 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 5s 11ms/step - loss: 0.3801 - val_loss: 0.1938\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.1258 - val_loss: 0.0912\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0658 - val_loss: 0.0565\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0425 - val_loss: 0.0427\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0318 - val_loss: 0.0365\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0256 - val_loss: 0.0329\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0216 - val_loss: 0.0303\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0187 - val_loss: 0.0283\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0165 - val_loss: 0.0266\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0148 - val_loss: 0.0253\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0134 - val_loss: 0.0242\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0122 - val_loss: 0.0232\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0113 - val_loss: 0.0224\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0105 - val_loss: 0.0217\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0098 - val_loss: 0.0211\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0093 - val_loss: 0.0206\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0088 - val_loss: 0.0202\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0084 - val_loss: 0.0197\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0081 - val_loss: 0.0195\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0078 - val_loss: 0.0193\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0076 - val_loss: 0.0190\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0074 - val_loss: 0.0188\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0072 - val_loss: 0.0187\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0070 - val_loss: 0.0186\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0069 - val_loss: 0.0186\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0067 - val_loss: 0.0186\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0066 - val_loss: 0.0186\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0065 - val_loss: 0.0186\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0064 - val_loss: 0.0187\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0063 - val_loss: 0.0189\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 6 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 4s 13ms/step - loss: 0.2958 - val_loss: 0.1778\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.1205 - val_loss: 0.0940\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0635 - val_loss: 0.0563\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0363 - val_loss: 0.0383\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0226 - val_loss: 0.0300\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0159 - val_loss: 0.0255\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0122 - val_loss: 0.0226\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0100 - val_loss: 0.0206\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0085 - val_loss: 0.0191\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0075 - val_loss: 0.0180\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0067 - val_loss: 0.0172\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0061 - val_loss: 0.0166\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0057 - val_loss: 0.0160\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0054 - val_loss: 0.0154\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0051 - val_loss: 0.0151\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0049 - val_loss: 0.0148\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0047 - val_loss: 0.0145\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0046 - val_loss: 0.0143\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0045 - val_loss: 0.0141\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0044 - val_loss: 0.0140\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0043 - val_loss: 0.0139\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0042 - val_loss: 0.0139\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0042 - val_loss: 0.0138\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0041 - val_loss: 0.0138\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0041 - val_loss: 0.0137\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0137\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0040 - val_loss: 0.0137\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0039 - val_loss: 0.0137\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0039 - val_loss: 0.0137\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0038 - val_loss: 0.0137\n",
            "Epoch 31/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0038 - val_loss: 0.0137\n",
            "Epoch 32/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0038 - val_loss: 0.0137\n",
            "Epoch 33/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0037 - val_loss: 0.0137\n",
            "Epoch 34/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0137\n",
            "Epoch 35/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0137\n",
            "Epoch 36/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0138\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 7 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 5s 10ms/step - loss: 0.1213 - val_loss: 0.0410\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0277 - val_loss: 0.0175\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0155 - val_loss: 0.0124\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0121 - val_loss: 0.0104\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0101 - val_loss: 0.0093\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0088 - val_loss: 0.0086\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0078 - val_loss: 0.0081\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0071 - val_loss: 0.0077\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0065 - val_loss: 0.0075\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0060 - val_loss: 0.0074\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0056 - val_loss: 0.0073\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0053 - val_loss: 0.0071\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0051 - val_loss: 0.0070\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0049 - val_loss: 0.0070\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0048 - val_loss: 0.0070\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0046 - val_loss: 0.0069\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0045 - val_loss: 0.0069\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0044 - val_loss: 0.0068\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0043 - val_loss: 0.0068\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0042 - val_loss: 0.0068\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0041 - val_loss: 0.0067\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0040 - val_loss: 0.0066\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0039 - val_loss: 0.0066\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0039 - val_loss: 0.0066\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0038 - val_loss: 0.0066\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0066\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0065\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0065\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0035 - val_loss: 0.0065\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0035 - val_loss: 0.0065\n",
            "Epoch 31/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0034 - val_loss: 0.0065\n",
            "Epoch 32/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0034 - val_loss: 0.0064\n",
            "Epoch 33/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0033 - val_loss: 0.0064\n",
            "Epoch 34/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0064\n",
            "Epoch 35/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0064\n",
            "Epoch 36/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0063\n",
            "Epoch 37/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0063\n",
            "Epoch 38/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0031 - val_loss: 0.0063\n",
            "Epoch 39/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0063\n",
            "Epoch 40/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0063\n",
            "Epoch 41/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0063\n",
            "Epoch 42/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0062\n",
            "Epoch 43/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0062\n",
            "Epoch 44/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0062\n",
            "Epoch 45/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0062\n",
            "Epoch 46/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0062\n",
            "Epoch 47/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0027 - val_loss: 0.0061\n",
            "Epoch 48/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0027 - val_loss: 0.0061\n",
            "Epoch 49/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0061\n",
            "Epoch 50/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0026 - val_loss: 0.0061\n",
            "Epoch 51/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0060\n",
            "Epoch 52/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0026 - val_loss: 0.0060\n",
            "Epoch 53/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0060\n",
            "Epoch 54/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0025 - val_loss: 0.0060\n",
            "Epoch 55/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0025 - val_loss: 0.0060\n",
            "Epoch 56/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0025 - val_loss: 0.0059\n",
            "Epoch 57/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0024 - val_loss: 0.0059\n",
            "Epoch 58/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0024 - val_loss: 0.0059\n",
            "Epoch 59/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0024 - val_loss: 0.0059\n",
            "Epoch 60/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0024 - val_loss: 0.0059\n",
            "Epoch 61/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0024 - val_loss: 0.0058\n",
            "Epoch 62/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0023 - val_loss: 0.0058\n",
            "Epoch 63/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0023 - val_loss: 0.0058\n",
            "Epoch 64/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0023 - val_loss: 0.0058\n",
            "Epoch 65/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0023 - val_loss: 0.0057\n",
            "Epoch 66/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0023 - val_loss: 0.0057\n",
            "Epoch 67/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0022 - val_loss: 0.0057\n",
            "Epoch 68/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0022 - val_loss: 0.0057\n",
            "Epoch 69/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0022 - val_loss: 0.0056\n",
            "Epoch 70/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0022 - val_loss: 0.0056\n",
            "Epoch 71/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0022 - val_loss: 0.0056\n",
            "Epoch 72/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0056\n",
            "Epoch 73/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0021 - val_loss: 0.0055\n",
            "Epoch 74/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0021 - val_loss: 0.0055\n",
            "Epoch 75/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0021 - val_loss: 0.0055\n",
            "6/6 [==============================] - 0s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 8 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 4s 12ms/step - loss: 0.1929 - val_loss: 0.1859\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.1036 - val_loss: 0.1224\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0635 - val_loss: 0.0915\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0440 - val_loss: 0.0725\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0324 - val_loss: 0.0598\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0252 - val_loss: 0.0516\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0208 - val_loss: 0.0460\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0178 - val_loss: 0.0419\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0156 - val_loss: 0.0386\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0138 - val_loss: 0.0360\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0124 - val_loss: 0.0337\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0112 - val_loss: 0.0318\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0102 - val_loss: 0.0301\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0094 - val_loss: 0.0287\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0088 - val_loss: 0.0274\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0082 - val_loss: 0.0262\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0078 - val_loss: 0.0252\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0074 - val_loss: 0.0244\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0070 - val_loss: 0.0236\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0068 - val_loss: 0.0229\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0065 - val_loss: 0.0223\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0063 - val_loss: 0.0218\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0062 - val_loss: 0.0214\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0060 - val_loss: 0.0209\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0059 - val_loss: 0.0206\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0058 - val_loss: 0.0203\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0057 - val_loss: 0.0200\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0056 - val_loss: 0.0197\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0055 - val_loss: 0.0194\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0055 - val_loss: 0.0192\n",
            "Epoch 31/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0054 - val_loss: 0.0190\n",
            "Epoch 32/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0053 - val_loss: 0.0189\n",
            "Epoch 33/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0053 - val_loss: 0.0187\n",
            "Epoch 34/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0052 - val_loss: 0.0186\n",
            "Epoch 35/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0052 - val_loss: 0.0184\n",
            "Epoch 36/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0051 - val_loss: 0.0183\n",
            "Epoch 37/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0051 - val_loss: 0.0182\n",
            "Epoch 38/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0050 - val_loss: 0.0181\n",
            "Epoch 39/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0050 - val_loss: 0.0180\n",
            "Epoch 40/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0050 - val_loss: 0.0179\n",
            "Epoch 41/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0049 - val_loss: 0.0178\n",
            "Epoch 42/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0049 - val_loss: 0.0177\n",
            "Epoch 43/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0048 - val_loss: 0.0176\n",
            "Epoch 44/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0048 - val_loss: 0.0175\n",
            "Epoch 45/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0048 - val_loss: 0.0174\n",
            "Epoch 46/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0047 - val_loss: 0.0174\n",
            "Epoch 47/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0047 - val_loss: 0.0173\n",
            "Epoch 48/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0047 - val_loss: 0.0172\n",
            "Epoch 49/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0046 - val_loss: 0.0172\n",
            "Epoch 50/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0046 - val_loss: 0.0171\n",
            "Epoch 51/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0045 - val_loss: 0.0170\n",
            "Epoch 52/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0045 - val_loss: 0.0170\n",
            "Epoch 53/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0045 - val_loss: 0.0169\n",
            "Epoch 54/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0044 - val_loss: 0.0169\n",
            "Epoch 55/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0044 - val_loss: 0.0168\n",
            "Epoch 56/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0044 - val_loss: 0.0168\n",
            "Epoch 57/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0044 - val_loss: 0.0167\n",
            "Epoch 58/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0043 - val_loss: 0.0167\n",
            "Epoch 59/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0043 - val_loss: 0.0166\n",
            "Epoch 60/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0043 - val_loss: 0.0165\n",
            "Epoch 61/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0042 - val_loss: 0.0165\n",
            "Epoch 62/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0042 - val_loss: 0.0164\n",
            "Epoch 63/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0042 - val_loss: 0.0164\n",
            "Epoch 64/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0041 - val_loss: 0.0164\n",
            "Epoch 65/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0041 - val_loss: 0.0163\n",
            "Epoch 66/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0041 - val_loss: 0.0163\n",
            "Epoch 67/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0041 - val_loss: 0.0162\n",
            "Epoch 68/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0040 - val_loss: 0.0162\n",
            "Epoch 69/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0161\n",
            "Epoch 70/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0161\n",
            "Epoch 71/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0161\n",
            "Epoch 72/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0039 - val_loss: 0.0160\n",
            "Epoch 73/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0039 - val_loss: 0.0160\n",
            "Epoch 74/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0039 - val_loss: 0.0160\n",
            "Epoch 75/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0039 - val_loss: 0.0159\n",
            "6/6 [==============================] - 0s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 9 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 5s 13ms/step - loss: 0.0966 - val_loss: 0.0821\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0367 - val_loss: 0.0599\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0231 - val_loss: 0.0498\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0162 - val_loss: 0.0429\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0120 - val_loss: 0.0377\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0094 - val_loss: 0.0336\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0076 - val_loss: 0.0305\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0065 - val_loss: 0.0281\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0057 - val_loss: 0.0262\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0051 - val_loss: 0.0247\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0047 - val_loss: 0.0235\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0044 - val_loss: 0.0225\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0042 - val_loss: 0.0217\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0040 - val_loss: 0.0210\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0038 - val_loss: 0.0205\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0200\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0196\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0035 - val_loss: 0.0192\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0034 - val_loss: 0.0189\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0187\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0184\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0182\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0179\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0178\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0176\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0174\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0173\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0029 - val_loss: 0.0171\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0170\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0028 - val_loss: 0.0169\n",
            "Epoch 31/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0027 - val_loss: 0.0168\n",
            "Epoch 32/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0168\n",
            "Epoch 33/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0167\n",
            "Epoch 34/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0026 - val_loss: 0.0166\n",
            "Epoch 35/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0026 - val_loss: 0.0165\n",
            "Epoch 36/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0164\n",
            "Epoch 37/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0025 - val_loss: 0.0164\n",
            "Epoch 38/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0163\n",
            "Epoch 39/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0025 - val_loss: 0.0162\n",
            "Epoch 40/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0162\n",
            "Epoch 41/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0024 - val_loss: 0.0162\n",
            "Epoch 42/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0024 - val_loss: 0.0161\n",
            "Epoch 43/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0024 - val_loss: 0.0160\n",
            "Epoch 44/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0023 - val_loss: 0.0160\n",
            "Epoch 45/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0023 - val_loss: 0.0159\n",
            "Epoch 46/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0023 - val_loss: 0.0159\n",
            "Epoch 47/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0023 - val_loss: 0.0158\n",
            "Epoch 48/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0022 - val_loss: 0.0157\n",
            "Epoch 49/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0022 - val_loss: 0.0157\n",
            "Epoch 50/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0022 - val_loss: 0.0156\n",
            "Epoch 51/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0022 - val_loss: 0.0156\n",
            "Epoch 52/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0022 - val_loss: 0.0156\n",
            "Epoch 53/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0022 - val_loss: 0.0156\n",
            "Epoch 54/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0021 - val_loss: 0.0155\n",
            "Epoch 55/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0021 - val_loss: 0.0155\n",
            "Epoch 56/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0021 - val_loss: 0.0154\n",
            "Epoch 57/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0021 - val_loss: 0.0154\n",
            "Epoch 58/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0021 - val_loss: 0.0153\n",
            "Epoch 59/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0020 - val_loss: 0.0153\n",
            "Epoch 60/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0020 - val_loss: 0.0153\n",
            "Epoch 61/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0152\n",
            "Epoch 62/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0152\n",
            "Epoch 63/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0020 - val_loss: 0.0152\n",
            "Epoch 64/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0020 - val_loss: 0.0151\n",
            "Epoch 65/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0020 - val_loss: 0.0151\n",
            "Epoch 66/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0019 - val_loss: 0.0151\n",
            "Epoch 67/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0019 - val_loss: 0.0151\n",
            "Epoch 68/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0019 - val_loss: 0.0150\n",
            "Epoch 69/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0150\n",
            "Epoch 70/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0149\n",
            "Epoch 71/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0149\n",
            "Epoch 72/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0149\n",
            "Epoch 73/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0149\n",
            "Epoch 74/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0018 - val_loss: 0.0149\n",
            "Epoch 75/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0018 - val_loss: 0.0148\n",
            "6/6 [==============================] - 1s 7ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 0 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 3s 12ms/step - loss: 0.3931 - val_loss: 0.2916\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.2189 - val_loss: 0.1929\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.1508 - val_loss: 0.1405\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.1110 - val_loss: 0.1078\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0853 - val_loss: 0.0860\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0682 - val_loss: 0.0712\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0566 - val_loss: 0.0608\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0483 - val_loss: 0.0538\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0426 - val_loss: 0.0484\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0382 - val_loss: 0.0444\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0347 - val_loss: 0.0410\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0318 - val_loss: 0.0383\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0293 - val_loss: 0.0359\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0272 - val_loss: 0.0338\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0254 - val_loss: 0.0320\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0237 - val_loss: 0.0303\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0222 - val_loss: 0.0288\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0208 - val_loss: 0.0274\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0196 - val_loss: 0.0261\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0184 - val_loss: 0.0249\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0174 - val_loss: 0.0239\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0165 - val_loss: 0.0229\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0156 - val_loss: 0.0220\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0148 - val_loss: 0.0212\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0141 - val_loss: 0.0204\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0134 - val_loss: 0.0197\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0128 - val_loss: 0.0190\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0122 - val_loss: 0.0184\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0117 - val_loss: 0.0178\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0112 - val_loss: 0.0173\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0108 - val_loss: 0.0168\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0104 - val_loss: 0.0163\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0100 - val_loss: 0.0159\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0096 - val_loss: 0.0155\n",
            "Epoch 35/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0093 - val_loss: 0.0151\n",
            "Epoch 36/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0090 - val_loss: 0.0148\n",
            "Epoch 37/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0087 - val_loss: 0.0145\n",
            "Epoch 38/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0085 - val_loss: 0.0142\n",
            "Epoch 39/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0082 - val_loss: 0.0139\n",
            "Epoch 40/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0080 - val_loss: 0.0137\n",
            "Epoch 41/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0078 - val_loss: 0.0134\n",
            "Epoch 42/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0076 - val_loss: 0.0132\n",
            "Epoch 43/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0075 - val_loss: 0.0130\n",
            "Epoch 44/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0073 - val_loss: 0.0128\n",
            "Epoch 45/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0071 - val_loss: 0.0126\n",
            "Epoch 46/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0070 - val_loss: 0.0125\n",
            "Epoch 47/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0069 - val_loss: 0.0123\n",
            "Epoch 48/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0067 - val_loss: 0.0122\n",
            "Epoch 49/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0066 - val_loss: 0.0121\n",
            "Epoch 50/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0065 - val_loss: 0.0119\n",
            "Epoch 51/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0064 - val_loss: 0.0118\n",
            "Epoch 52/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0063 - val_loss: 0.0117\n",
            "Epoch 53/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0062 - val_loss: 0.0116\n",
            "Epoch 54/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0062 - val_loss: 0.0115\n",
            "Epoch 55/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0061 - val_loss: 0.0114\n",
            "Epoch 56/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0060 - val_loss: 0.0113\n",
            "Epoch 57/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0059 - val_loss: 0.0113\n",
            "Epoch 58/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0059 - val_loss: 0.0112\n",
            "Epoch 59/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0058 - val_loss: 0.0111\n",
            "Epoch 60/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0058 - val_loss: 0.0111\n",
            "Epoch 61/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0057 - val_loss: 0.0110\n",
            "Epoch 62/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0056 - val_loss: 0.0109\n",
            "Epoch 63/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0056 - val_loss: 0.0109\n",
            "Epoch 64/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0056 - val_loss: 0.0109\n",
            "Epoch 65/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0055 - val_loss: 0.0108\n",
            "Epoch 66/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0055 - val_loss: 0.0108\n",
            "Epoch 67/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0054 - val_loss: 0.0107\n",
            "Epoch 68/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0054 - val_loss: 0.0107\n",
            "Epoch 69/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0053 - val_loss: 0.0107\n",
            "Epoch 70/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0053 - val_loss: 0.0106\n",
            "Epoch 71/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0053 - val_loss: 0.0106\n",
            "Epoch 72/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0052 - val_loss: 0.0106\n",
            "Epoch 73/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0052 - val_loss: 0.0105\n",
            "Epoch 74/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0052 - val_loss: 0.0105\n",
            "Epoch 75/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0051 - val_loss: 0.0105\n",
            "6/6 [==============================] - 0s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 1 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 3s 13ms/step - loss: 0.1731 - val_loss: 0.0965\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0775 - val_loss: 0.0510\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0417 - val_loss: 0.0341\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0273 - val_loss: 0.0275\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0206 - val_loss: 0.0246\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0170 - val_loss: 0.0230\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0146 - val_loss: 0.0218\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0129 - val_loss: 0.0209\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0116 - val_loss: 0.0201\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0105 - val_loss: 0.0194\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0095 - val_loss: 0.0188\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0087 - val_loss: 0.0183\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0081 - val_loss: 0.0179\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0075 - val_loss: 0.0176\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0070 - val_loss: 0.0173\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0066 - val_loss: 0.0170\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0063 - val_loss: 0.0167\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0060 - val_loss: 0.0166\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0058 - val_loss: 0.0164\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0056 - val_loss: 0.0162\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0054 - val_loss: 0.0161\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0052 - val_loss: 0.0160\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0051 - val_loss: 0.0159\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0050 - val_loss: 0.0158\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0048 - val_loss: 0.0157\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0047 - val_loss: 0.0155\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0046 - val_loss: 0.0154\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0046 - val_loss: 0.0154\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0045 - val_loss: 0.0153\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0044 - val_loss: 0.0153\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0044 - val_loss: 0.0152\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0043 - val_loss: 0.0152\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0042 - val_loss: 0.0151\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0042 - val_loss: 0.0151\n",
            "Epoch 35/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0041 - val_loss: 0.0151\n",
            "Epoch 36/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0041 - val_loss: 0.0150\n",
            "Epoch 37/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0040 - val_loss: 0.0149\n",
            "Epoch 38/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0148\n",
            "Epoch 39/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0040 - val_loss: 0.0148\n",
            "Epoch 40/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0039 - val_loss: 0.0148\n",
            "Epoch 41/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0039 - val_loss: 0.0148\n",
            "Epoch 42/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0038 - val_loss: 0.0148\n",
            "Epoch 43/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0038 - val_loss: 0.0148\n",
            "Epoch 44/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0038 - val_loss: 0.0147\n",
            "Epoch 45/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0147\n",
            "Epoch 46/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0147\n",
            "Epoch 47/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0146\n",
            "Epoch 48/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0036 - val_loss: 0.0146\n",
            "Epoch 49/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0036 - val_loss: 0.0146\n",
            "Epoch 50/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0146\n",
            "Epoch 51/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0035 - val_loss: 0.0146\n",
            "Epoch 52/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0035 - val_loss: 0.0146\n",
            "Epoch 53/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0035 - val_loss: 0.0146\n",
            "Epoch 54/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0035 - val_loss: 0.0146\n",
            "Epoch 55/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0034 - val_loss: 0.0145\n",
            "Epoch 56/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0034 - val_loss: 0.0145\n",
            "Epoch 57/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0034 - val_loss: 0.0145\n",
            "Epoch 58/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0145\n",
            "Epoch 59/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0145\n",
            "Epoch 60/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0145\n",
            "Epoch 61/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0144\n",
            "Epoch 62/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0144\n",
            "Epoch 63/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0144\n",
            "Epoch 64/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0144\n",
            "Epoch 65/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0144\n",
            "Epoch 66/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0144\n",
            "Epoch 67/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0143\n",
            "Epoch 68/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0143\n",
            "Epoch 69/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0031 - val_loss: 0.0143\n",
            "Epoch 70/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0031 - val_loss: 0.0143\n",
            "Epoch 71/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0031 - val_loss: 0.0142\n",
            "Epoch 72/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0143\n",
            "Epoch 73/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0142\n",
            "Epoch 74/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0030 - val_loss: 0.0142\n",
            "Epoch 75/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0030 - val_loss: 0.0142\n",
            "6/6 [==============================] - 1s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 2 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 4s 15ms/step - loss: 0.2015 - val_loss: 0.1491\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0887 - val_loss: 0.0938\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0467 - val_loss: 0.0659\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0307 - val_loss: 0.0535\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0237 - val_loss: 0.0462\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0197 - val_loss: 0.0415\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0170 - val_loss: 0.0379\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0151 - val_loss: 0.0352\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0136 - val_loss: 0.0330\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0124 - val_loss: 0.0312\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0115 - val_loss: 0.0297\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0108 - val_loss: 0.0285\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0102 - val_loss: 0.0275\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0097 - val_loss: 0.0266\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0093 - val_loss: 0.0258\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0090 - val_loss: 0.0252\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0087 - val_loss: 0.0247\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0085 - val_loss: 0.0241\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0083 - val_loss: 0.0237\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0081 - val_loss: 0.0234\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0080 - val_loss: 0.0230\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0078 - val_loss: 0.0228\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0077 - val_loss: 0.0226\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0076 - val_loss: 0.0224\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0075 - val_loss: 0.0222\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0074 - val_loss: 0.0220\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0073 - val_loss: 0.0219\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0072 - val_loss: 0.0219\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0072 - val_loss: 0.0217\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0071 - val_loss: 0.0217\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0070 - val_loss: 0.0216\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0069 - val_loss: 0.0215\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0069 - val_loss: 0.0215\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0068 - val_loss: 0.0214\n",
            "Epoch 35/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0067 - val_loss: 0.0214\n",
            "Epoch 36/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0067 - val_loss: 0.0213\n",
            "Epoch 37/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0066 - val_loss: 0.0213\n",
            "Epoch 38/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0065 - val_loss: 0.0214\n",
            "Epoch 39/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0065 - val_loss: 0.0213\n",
            "Epoch 40/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0064 - val_loss: 0.0213\n",
            "Epoch 41/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0064 - val_loss: 0.0213\n",
            "Epoch 42/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0063 - val_loss: 0.0213\n",
            "Epoch 43/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0062 - val_loss: 0.0213\n",
            "Epoch 44/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0062 - val_loss: 0.0214\n",
            "Epoch 45/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0061 - val_loss: 0.0214\n",
            "Epoch 46/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0061 - val_loss: 0.0214\n",
            "Epoch 47/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0060 - val_loss: 0.0214\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 3 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 5s 21ms/step - loss: 0.0515 - val_loss: 0.0330\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0180 - val_loss: 0.0238\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0124 - val_loss: 0.0225\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0107 - val_loss: 0.0221\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0098 - val_loss: 0.0218\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0091 - val_loss: 0.0214\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0085 - val_loss: 0.0212\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0081 - val_loss: 0.0211\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0077 - val_loss: 0.0210\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0074 - val_loss: 0.0208\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0071 - val_loss: 0.0206\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0068 - val_loss: 0.0207\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0066 - val_loss: 0.0207\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0064 - val_loss: 0.0208\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0062 - val_loss: 0.0208\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0060 - val_loss: 0.0206\n",
            "6/6 [==============================] - 0s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 4 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 4s 16ms/step - loss: 0.2558 - val_loss: 0.1976\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.1439 - val_loss: 0.1297\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0936 - val_loss: 0.0934\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0626 - val_loss: 0.0711\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0413 - val_loss: 0.0553\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0263 - val_loss: 0.0415\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0169 - val_loss: 0.0339\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0125 - val_loss: 0.0303\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0104 - val_loss: 0.0283\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0093 - val_loss: 0.0270\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0086 - val_loss: 0.0261\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0081 - val_loss: 0.0253\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0077 - val_loss: 0.0246\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0074 - val_loss: 0.0240\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0071 - val_loss: 0.0234\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0068 - val_loss: 0.0229\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0066 - val_loss: 0.0224\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0064 - val_loss: 0.0220\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0062 - val_loss: 0.0216\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0061 - val_loss: 0.0212\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0059 - val_loss: 0.0208\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0058 - val_loss: 0.0205\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0057 - val_loss: 0.0202\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0056 - val_loss: 0.0199\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0055 - val_loss: 0.0196\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0054 - val_loss: 0.0194\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0053 - val_loss: 0.0191\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0053 - val_loss: 0.0189\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0052 - val_loss: 0.0187\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0051 - val_loss: 0.0185\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0051 - val_loss: 0.0183\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0050 - val_loss: 0.0181\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0050 - val_loss: 0.0180\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0049 - val_loss: 0.0178\n",
            "Epoch 35/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0049 - val_loss: 0.0177\n",
            "Epoch 36/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0048 - val_loss: 0.0175\n",
            "Epoch 37/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0048 - val_loss: 0.0174\n",
            "Epoch 38/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0048 - val_loss: 0.0173\n",
            "Epoch 39/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0047 - val_loss: 0.0172\n",
            "Epoch 40/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0047 - val_loss: 0.0171\n",
            "Epoch 41/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0047 - val_loss: 0.0170\n",
            "Epoch 42/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0046 - val_loss: 0.0169\n",
            "Epoch 43/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0046 - val_loss: 0.0168\n",
            "Epoch 44/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0046 - val_loss: 0.0167\n",
            "Epoch 45/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0045 - val_loss: 0.0166\n",
            "Epoch 46/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0045 - val_loss: 0.0166\n",
            "Epoch 47/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0045 - val_loss: 0.0165\n",
            "Epoch 48/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0045 - val_loss: 0.0164\n",
            "Epoch 49/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0044 - val_loss: 0.0163\n",
            "Epoch 50/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0044 - val_loss: 0.0163\n",
            "Epoch 51/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0044 - val_loss: 0.0162\n",
            "Epoch 52/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0044 - val_loss: 0.0161\n",
            "Epoch 53/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0043 - val_loss: 0.0161\n",
            "Epoch 54/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0043 - val_loss: 0.0160\n",
            "Epoch 55/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0043 - val_loss: 0.0160\n",
            "Epoch 56/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0043 - val_loss: 0.0159\n",
            "Epoch 57/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0042 - val_loss: 0.0159\n",
            "Epoch 58/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0042 - val_loss: 0.0158\n",
            "Epoch 59/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0042 - val_loss: 0.0158\n",
            "Epoch 60/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0042 - val_loss: 0.0157\n",
            "Epoch 61/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0042 - val_loss: 0.0157\n",
            "Epoch 62/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0041 - val_loss: 0.0156\n",
            "Epoch 63/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0041 - val_loss: 0.0156\n",
            "Epoch 64/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0041 - val_loss: 0.0155\n",
            "Epoch 65/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0041 - val_loss: 0.0155\n",
            "Epoch 66/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0041 - val_loss: 0.0154\n",
            "Epoch 67/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0154\n",
            "Epoch 68/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0154\n",
            "Epoch 69/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0153\n",
            "Epoch 70/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0153\n",
            "Epoch 71/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0153\n",
            "Epoch 72/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0152\n",
            "Epoch 73/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0039 - val_loss: 0.0152\n",
            "Epoch 74/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0039 - val_loss: 0.0151\n",
            "Epoch 75/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0039 - val_loss: 0.0151\n",
            "6/6 [==============================] - 1s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 5 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 3s 13ms/step - loss: 0.0740 - val_loss: 0.0341\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0126 - val_loss: 0.0271\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0081 - val_loss: 0.0261\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0072 - val_loss: 0.0253\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0066 - val_loss: 0.0245\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0063 - val_loss: 0.0239\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0060 - val_loss: 0.0234\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0058 - val_loss: 0.0229\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0056 - val_loss: 0.0226\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0054 - val_loss: 0.0224\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0053 - val_loss: 0.0222\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0052 - val_loss: 0.0221\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0051 - val_loss: 0.0219\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0050 - val_loss: 0.0216\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0049 - val_loss: 0.0214\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0048 - val_loss: 0.0214\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0048 - val_loss: 0.0214\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0047 - val_loss: 0.0214\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0046 - val_loss: 0.0213\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0046 - val_loss: 0.0213\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0045 - val_loss: 0.0212\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0045 - val_loss: 0.0212\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0044 - val_loss: 0.0211\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0044 - val_loss: 0.0211\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0043 - val_loss: 0.0211\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0043 - val_loss: 0.0209\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0043 - val_loss: 0.0209\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0042 - val_loss: 0.0211\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0042 - val_loss: 0.0209\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0041 - val_loss: 0.0208\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0041 - val_loss: 0.0209\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0041 - val_loss: 0.0208\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0208\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0208\n",
            "Epoch 35/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0207\n",
            "Epoch 36/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0039 - val_loss: 0.0207\n",
            "Epoch 37/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0039 - val_loss: 0.0206\n",
            "Epoch 38/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0039 - val_loss: 0.0206\n",
            "Epoch 39/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0038 - val_loss: 0.0205\n",
            "Epoch 40/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0038 - val_loss: 0.0205\n",
            "Epoch 41/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0038 - val_loss: 0.0204\n",
            "Epoch 42/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0038 - val_loss: 0.0205\n",
            "Epoch 43/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0037 - val_loss: 0.0205\n",
            "Epoch 44/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0037 - val_loss: 0.0205\n",
            "Epoch 45/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0037 - val_loss: 0.0204\n",
            "Epoch 46/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0037 - val_loss: 0.0205\n",
            "Epoch 47/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0036 - val_loss: 0.0204\n",
            "Epoch 48/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0036 - val_loss: 0.0202\n",
            "Epoch 49/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0036 - val_loss: 0.0202\n",
            "Epoch 50/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0201\n",
            "Epoch 51/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0035 - val_loss: 0.0201\n",
            "Epoch 52/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0035 - val_loss: 0.0200\n",
            "Epoch 53/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0035 - val_loss: 0.0201\n",
            "Epoch 54/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0035 - val_loss: 0.0199\n",
            "Epoch 55/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0035 - val_loss: 0.0199\n",
            "Epoch 56/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0034 - val_loss: 0.0200\n",
            "Epoch 57/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0034 - val_loss: 0.0198\n",
            "Epoch 58/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0034 - val_loss: 0.0198\n",
            "Epoch 59/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0034 - val_loss: 0.0198\n",
            "Epoch 60/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0034 - val_loss: 0.0198\n",
            "Epoch 61/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0196\n",
            "Epoch 62/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0196\n",
            "Epoch 63/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0196\n",
            "Epoch 64/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0195\n",
            "Epoch 65/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0195\n",
            "Epoch 66/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0195\n",
            "Epoch 67/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0194\n",
            "Epoch 68/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0193\n",
            "Epoch 69/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0193\n",
            "Epoch 70/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0192\n",
            "Epoch 71/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0191\n",
            "Epoch 72/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0192\n",
            "Epoch 73/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0191\n",
            "Epoch 74/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0190\n",
            "Epoch 75/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0190\n",
            "6/6 [==============================] - 1s 3ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 6 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 4s 16ms/step - loss: 0.2529 - val_loss: 0.2086\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.1549 - val_loss: 0.1442\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.1015 - val_loss: 0.1020\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0651 - val_loss: 0.0713\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0438 - val_loss: 0.0538\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0322 - val_loss: 0.0442\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0259 - val_loss: 0.0383\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0218 - val_loss: 0.0341\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0189 - val_loss: 0.0311\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0168 - val_loss: 0.0288\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0151 - val_loss: 0.0269\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0138 - val_loss: 0.0253\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0127 - val_loss: 0.0239\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0118 - val_loss: 0.0228\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0111 - val_loss: 0.0218\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0104 - val_loss: 0.0209\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0099 - val_loss: 0.0202\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0094 - val_loss: 0.0195\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0090 - val_loss: 0.0189\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0086 - val_loss: 0.0184\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0083 - val_loss: 0.0179\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0080 - val_loss: 0.0176\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0078 - val_loss: 0.0173\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0075 - val_loss: 0.0170\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0073 - val_loss: 0.0167\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0072 - val_loss: 0.0165\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0070 - val_loss: 0.0163\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0068 - val_loss: 0.0162\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0067 - val_loss: 0.0160\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0066 - val_loss: 0.0159\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0064 - val_loss: 0.0159\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0063 - val_loss: 0.0158\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0062 - val_loss: 0.0158\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0061 - val_loss: 0.0157\n",
            "Epoch 35/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0060 - val_loss: 0.0157\n",
            "Epoch 36/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0059 - val_loss: 0.0157\n",
            "Epoch 37/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0058 - val_loss: 0.0157\n",
            "Epoch 38/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0057 - val_loss: 0.0156\n",
            "Epoch 39/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0057 - val_loss: 0.0157\n",
            "Epoch 40/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0056 - val_loss: 0.0157\n",
            "Epoch 41/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0055 - val_loss: 0.0158\n",
            "Epoch 42/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0054 - val_loss: 0.0158\n",
            "Epoch 43/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0054 - val_loss: 0.0157\n",
            "6/6 [==============================] - 1s 3ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 7 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 4s 16ms/step - loss: 0.1514 - val_loss: 0.1024\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0863 - val_loss: 0.0770\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0680 - val_loss: 0.0653\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0579 - val_loss: 0.0581\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0507 - val_loss: 0.0525\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0449 - val_loss: 0.0481\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0401 - val_loss: 0.0444\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0362 - val_loss: 0.0412\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0328 - val_loss: 0.0385\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0298 - val_loss: 0.0361\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0273 - val_loss: 0.0339\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0250 - val_loss: 0.0321\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0230 - val_loss: 0.0304\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0212 - val_loss: 0.0291\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0197 - val_loss: 0.0279\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0184 - val_loss: 0.0269\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0172 - val_loss: 0.0261\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0161 - val_loss: 0.0253\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0152 - val_loss: 0.0247\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0143 - val_loss: 0.0241\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0136 - val_loss: 0.0235\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0129 - val_loss: 0.0229\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0122 - val_loss: 0.0225\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0116 - val_loss: 0.0220\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0111 - val_loss: 0.0216\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0106 - val_loss: 0.0213\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0102 - val_loss: 0.0210\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0098 - val_loss: 0.0208\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0094 - val_loss: 0.0206\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0091 - val_loss: 0.0204\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0088 - val_loss: 0.0201\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0085 - val_loss: 0.0200\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0083 - val_loss: 0.0198\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0081 - val_loss: 0.0197\n",
            "Epoch 35/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0079 - val_loss: 0.0196\n",
            "Epoch 36/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0077 - val_loss: 0.0195\n",
            "Epoch 37/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0075 - val_loss: 0.0193\n",
            "Epoch 38/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0073 - val_loss: 0.0192\n",
            "Epoch 39/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0072 - val_loss: 0.0191\n",
            "Epoch 40/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0070 - val_loss: 0.0190\n",
            "Epoch 41/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0069 - val_loss: 0.0189\n",
            "Epoch 42/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0068 - val_loss: 0.0188\n",
            "Epoch 43/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0067 - val_loss: 0.0187\n",
            "Epoch 44/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0066 - val_loss: 0.0187\n",
            "Epoch 45/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0065 - val_loss: 0.0185\n",
            "Epoch 46/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0064 - val_loss: 0.0185\n",
            "Epoch 47/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0063 - val_loss: 0.0184\n",
            "Epoch 48/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0062 - val_loss: 0.0183\n",
            "Epoch 49/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0061 - val_loss: 0.0182\n",
            "Epoch 50/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0060 - val_loss: 0.0181\n",
            "Epoch 51/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0059 - val_loss: 0.0181\n",
            "Epoch 52/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0059 - val_loss: 0.0182\n",
            "Epoch 53/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0058 - val_loss: 0.0181\n",
            "Epoch 54/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0057 - val_loss: 0.0180\n",
            "Epoch 55/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0056 - val_loss: 0.0179\n",
            "Epoch 56/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0056 - val_loss: 0.0179\n",
            "Epoch 57/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0055 - val_loss: 0.0179\n",
            "Epoch 58/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0055 - val_loss: 0.0178\n",
            "Epoch 59/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0054 - val_loss: 0.0177\n",
            "Epoch 60/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0053 - val_loss: 0.0176\n",
            "Epoch 61/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0053 - val_loss: 0.0176\n",
            "Epoch 62/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0052 - val_loss: 0.0176\n",
            "Epoch 63/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0052 - val_loss: 0.0176\n",
            "Epoch 64/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0051 - val_loss: 0.0176\n",
            "Epoch 65/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0051 - val_loss: 0.0175\n",
            "Epoch 66/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0050 - val_loss: 0.0174\n",
            "Epoch 67/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0050 - val_loss: 0.0173\n",
            "Epoch 68/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0049 - val_loss: 0.0174\n",
            "Epoch 69/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0049 - val_loss: 0.0173\n",
            "Epoch 70/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0048 - val_loss: 0.0173\n",
            "Epoch 71/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0048 - val_loss: 0.0172\n",
            "Epoch 72/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0048 - val_loss: 0.0172\n",
            "Epoch 73/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0047 - val_loss: 0.0172\n",
            "Epoch 74/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0047 - val_loss: 0.0172\n",
            "Epoch 75/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0046 - val_loss: 0.0172\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 8 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 4s 16ms/step - loss: 0.2039 - val_loss: 0.1526\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.1084 - val_loss: 0.1050\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0750 - val_loss: 0.0813\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0572 - val_loss: 0.0669\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0457 - val_loss: 0.0565\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0374 - val_loss: 0.0485\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0308 - val_loss: 0.0421\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0256 - val_loss: 0.0370\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0215 - val_loss: 0.0328\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0183 - val_loss: 0.0295\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0157 - val_loss: 0.0268\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0136 - val_loss: 0.0245\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0120 - val_loss: 0.0226\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0106 - val_loss: 0.0211\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0095 - val_loss: 0.0199\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0086 - val_loss: 0.0188\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0079 - val_loss: 0.0179\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0073 - val_loss: 0.0173\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0068 - val_loss: 0.0166\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0064 - val_loss: 0.0162\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0060 - val_loss: 0.0156\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0058 - val_loss: 0.0153\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0055 - val_loss: 0.0151\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0053 - val_loss: 0.0147\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0051 - val_loss: 0.0146\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0050 - val_loss: 0.0143\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0048 - val_loss: 0.0141\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0047 - val_loss: 0.0139\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0046 - val_loss: 0.0138\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0045 - val_loss: 0.0137\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0044 - val_loss: 0.0136\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0043 - val_loss: 0.0134\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0043 - val_loss: 0.0134\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0042 - val_loss: 0.0133\n",
            "Epoch 35/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0041 - val_loss: 0.0133\n",
            "Epoch 36/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0041 - val_loss: 0.0133\n",
            "Epoch 37/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0040 - val_loss: 0.0131\n",
            "Epoch 38/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0040 - val_loss: 0.0131\n",
            "Epoch 39/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0039 - val_loss: 0.0130\n",
            "Epoch 40/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0039 - val_loss: 0.0129\n",
            "Epoch 41/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0039 - val_loss: 0.0129\n",
            "Epoch 42/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0038 - val_loss: 0.0129\n",
            "Epoch 43/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0038 - val_loss: 0.0129\n",
            "Epoch 44/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0037 - val_loss: 0.0128\n",
            "Epoch 45/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0128\n",
            "Epoch 46/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0127\n",
            "Epoch 47/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0127\n",
            "Epoch 48/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0036 - val_loss: 0.0127\n",
            "Epoch 49/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0036 - val_loss: 0.0127\n",
            "Epoch 50/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0035 - val_loss: 0.0126\n",
            "Epoch 51/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0035 - val_loss: 0.0126\n",
            "Epoch 52/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0035 - val_loss: 0.0125\n",
            "Epoch 53/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0035 - val_loss: 0.0125\n",
            "Epoch 54/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0034 - val_loss: 0.0125\n",
            "Epoch 55/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0034 - val_loss: 0.0125\n",
            "Epoch 56/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0034 - val_loss: 0.0124\n",
            "Epoch 57/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0124\n",
            "Epoch 58/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0124\n",
            "Epoch 59/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0124\n",
            "Epoch 60/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0123\n",
            "Epoch 61/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0123\n",
            "Epoch 62/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0032 - val_loss: 0.0124\n",
            "Epoch 63/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0123\n",
            "Epoch 64/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0123\n",
            "Epoch 65/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0122\n",
            "Epoch 66/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0031 - val_loss: 0.0122\n",
            "Epoch 67/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0122\n",
            "Epoch 68/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0121\n",
            "Epoch 69/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0121\n",
            "Epoch 70/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0121\n",
            "Epoch 71/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0030 - val_loss: 0.0121\n",
            "Epoch 72/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0120\n",
            "Epoch 73/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0030 - val_loss: 0.0120\n",
            "Epoch 74/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0120\n",
            "Epoch 75/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0119\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 9 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 4s 15ms/step - loss: 0.1765 - val_loss: 0.1223\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0923 - val_loss: 0.0745\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0560 - val_loss: 0.0495\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0361 - val_loss: 0.0363\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0255 - val_loss: 0.0292\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0197 - val_loss: 0.0250\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0162 - val_loss: 0.0223\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0139 - val_loss: 0.0203\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0122 - val_loss: 0.0187\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0109 - val_loss: 0.0174\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0098 - val_loss: 0.0163\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0090 - val_loss: 0.0154\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0083 - val_loss: 0.0147\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0077 - val_loss: 0.0141\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0072 - val_loss: 0.0136\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0068 - val_loss: 0.0132\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0065 - val_loss: 0.0129\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0062 - val_loss: 0.0126\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0059 - val_loss: 0.0124\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0057 - val_loss: 0.0122\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0055 - val_loss: 0.0120\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0053 - val_loss: 0.0119\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0052 - val_loss: 0.0118\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0050 - val_loss: 0.0117\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0049 - val_loss: 0.0116\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0048 - val_loss: 0.0115\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0047 - val_loss: 0.0114\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0046 - val_loss: 0.0113\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0045 - val_loss: 0.0112\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0044 - val_loss: 0.0111\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0043 - val_loss: 0.0110\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0043 - val_loss: 0.0109\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0042 - val_loss: 0.0109\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0041 - val_loss: 0.0108\n",
            "Epoch 35/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0108\n",
            "Epoch 36/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0107\n",
            "Epoch 37/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0039 - val_loss: 0.0107\n",
            "Epoch 38/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0039 - val_loss: 0.0106\n",
            "Epoch 39/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0038 - val_loss: 0.0106\n",
            "Epoch 40/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0038 - val_loss: 0.0106\n",
            "Epoch 41/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0037 - val_loss: 0.0106\n",
            "Epoch 42/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0105\n",
            "Epoch 43/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0105\n",
            "Epoch 44/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0105\n",
            "Epoch 45/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0105\n",
            "Epoch 46/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0035 - val_loss: 0.0105\n",
            "Epoch 47/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0035 - val_loss: 0.0105\n",
            "Epoch 48/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0034 - val_loss: 0.0105\n",
            "Epoch 49/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0034 - val_loss: 0.0104\n",
            "Epoch 50/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0034 - val_loss: 0.0104\n",
            "Epoch 51/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0033 - val_loss: 0.0104\n",
            "Epoch 52/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0104\n",
            "Epoch 53/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0104\n",
            "Epoch 54/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0104\n",
            "Epoch 55/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0104\n",
            "Epoch 56/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0104\n",
            "Epoch 57/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0103\n",
            "Epoch 58/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0103\n",
            "Epoch 59/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0103\n",
            "Epoch 60/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0103\n",
            "Epoch 61/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0103\n",
            "Epoch 62/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0103\n",
            "Epoch 63/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0103\n",
            "Epoch 64/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0102\n",
            "Epoch 65/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0102\n",
            "Epoch 66/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0102\n",
            "Epoch 67/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0102\n",
            "Epoch 68/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0102\n",
            "Epoch 69/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0101\n",
            "Epoch 70/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0101\n",
            "Epoch 71/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0101\n",
            "Epoch 72/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0102\n",
            "Epoch 73/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0101\n",
            "Epoch 74/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0101\n",
            "Epoch 75/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0101\n",
            "6/6 [==============================] - 1s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 0 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 4s 29ms/step - loss: 0.3378 - val_loss: 0.2609\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.2436 - val_loss: 0.1902\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.1839 - val_loss: 0.1501\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.1437 - val_loss: 0.1193\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.1080 - val_loss: 0.0937\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0830 - val_loss: 0.0743\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0661 - val_loss: 0.0609\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0544 - val_loss: 0.0516\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0462 - val_loss: 0.0450\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0402 - val_loss: 0.0402\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0358 - val_loss: 0.0367\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0326 - val_loss: 0.0341\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0300 - val_loss: 0.0321\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0280 - val_loss: 0.0305\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0263 - val_loss: 0.0292\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0248 - val_loss: 0.0281\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0236 - val_loss: 0.0272\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0224 - val_loss: 0.0264\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0215 - val_loss: 0.0257\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0206 - val_loss: 0.0251\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0198 - val_loss: 0.0245\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0190 - val_loss: 0.0240\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0183 - val_loss: 0.0235\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0177 - val_loss: 0.0231\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0171 - val_loss: 0.0227\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0166 - val_loss: 0.0223\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0160 - val_loss: 0.0220\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0156 - val_loss: 0.0217\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0151 - val_loss: 0.0214\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0147 - val_loss: 0.0211\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0143 - val_loss: 0.0209\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0139 - val_loss: 0.0206\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0136 - val_loss: 0.0204\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0133 - val_loss: 0.0202\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0130 - val_loss: 0.0200\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0127 - val_loss: 0.0198\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0124 - val_loss: 0.0196\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0121 - val_loss: 0.0195\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0119 - val_loss: 0.0193\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0116 - val_loss: 0.0192\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0114 - val_loss: 0.0190\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0112 - val_loss: 0.0189\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0110 - val_loss: 0.0188\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0108 - val_loss: 0.0186\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0106 - val_loss: 0.0185\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0105 - val_loss: 0.0184\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0103 - val_loss: 0.0183\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0101 - val_loss: 0.0182\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0100 - val_loss: 0.0181\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0098 - val_loss: 0.0180\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0097 - val_loss: 0.0179\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0096 - val_loss: 0.0178\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0095 - val_loss: 0.0177\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0093 - val_loss: 0.0176\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0092 - val_loss: 0.0175\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0091 - val_loss: 0.0175\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0090 - val_loss: 0.0174\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0089 - val_loss: 0.0173\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0088 - val_loss: 0.0173\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0087 - val_loss: 0.0172\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0086 - val_loss: 0.0171\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0085 - val_loss: 0.0171\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0084 - val_loss: 0.0170\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0084 - val_loss: 0.0170\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0083 - val_loss: 0.0169\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0082 - val_loss: 0.0168\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0081 - val_loss: 0.0168\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0081 - val_loss: 0.0168\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0080 - val_loss: 0.0167\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0079 - val_loss: 0.0167\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0079 - val_loss: 0.0166\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0078 - val_loss: 0.0166\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0078 - val_loss: 0.0166\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0077 - val_loss: 0.0165\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0076 - val_loss: 0.0165\n",
            "6/6 [==============================] - 0s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 1 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 3s 18ms/step - loss: 0.3414 - val_loss: 0.3051\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.2664 - val_loss: 0.2515\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.2218 - val_loss: 0.2131\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.1875 - val_loss: 0.1837\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.1597 - val_loss: 0.1602\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.1366 - val_loss: 0.1406\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.1171 - val_loss: 0.1239\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.1005 - val_loss: 0.1098\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0840 - val_loss: 0.0937\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0669 - val_loss: 0.0794\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0542 - val_loss: 0.0684\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0449 - val_loss: 0.0601\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0379 - val_loss: 0.0539\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0326 - val_loss: 0.0492\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0286 - val_loss: 0.0455\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0255 - val_loss: 0.0426\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0231 - val_loss: 0.0402\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0212 - val_loss: 0.0383\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0197 - val_loss: 0.0367\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0185 - val_loss: 0.0354\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0174 - val_loss: 0.0342\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0165 - val_loss: 0.0332\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0158 - val_loss: 0.0322\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0151 - val_loss: 0.0314\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0145 - val_loss: 0.0306\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0140 - val_loss: 0.0298\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0135 - val_loss: 0.0292\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0130 - val_loss: 0.0285\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0126 - val_loss: 0.0279\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0122 - val_loss: 0.0273\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0118 - val_loss: 0.0267\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0114 - val_loss: 0.0262\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0111 - val_loss: 0.0256\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0108 - val_loss: 0.0251\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0105 - val_loss: 0.0247\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0102 - val_loss: 0.0242\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0099 - val_loss: 0.0237\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0096 - val_loss: 0.0233\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0094 - val_loss: 0.0229\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0092 - val_loss: 0.0225\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0089 - val_loss: 0.0221\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0087 - val_loss: 0.0218\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0085 - val_loss: 0.0214\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0083 - val_loss: 0.0211\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0082 - val_loss: 0.0208\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0080 - val_loss: 0.0205\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0078 - val_loss: 0.0202\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0077 - val_loss: 0.0199\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0075 - val_loss: 0.0196\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0074 - val_loss: 0.0194\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0073 - val_loss: 0.0191\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0071 - val_loss: 0.0189\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0070 - val_loss: 0.0186\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0069 - val_loss: 0.0184\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0068 - val_loss: 0.0182\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0067 - val_loss: 0.0180\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0066 - val_loss: 0.0178\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0065 - val_loss: 0.0176\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0064 - val_loss: 0.0175\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0063 - val_loss: 0.0173\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0062 - val_loss: 0.0171\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0062 - val_loss: 0.0170\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0061 - val_loss: 0.0168\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0060 - val_loss: 0.0167\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0059 - val_loss: 0.0165\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0059 - val_loss: 0.0164\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0058 - val_loss: 0.0163\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0058 - val_loss: 0.0161\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0057 - val_loss: 0.0160\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0056 - val_loss: 0.0159\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0056 - val_loss: 0.0158\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0055 - val_loss: 0.0156\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0055 - val_loss: 0.0155\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0054 - val_loss: 0.0154\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0054 - val_loss: 0.0153\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 2 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 4s 23ms/step - loss: 0.2840 - val_loss: 0.1813\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.1702 - val_loss: 0.1313\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.1212 - val_loss: 0.1004\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0909 - val_loss: 0.0789\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0699 - val_loss: 0.0631\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0546 - val_loss: 0.0514\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0433 - val_loss: 0.0427\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0347 - val_loss: 0.0360\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0282 - val_loss: 0.0310\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0232 - val_loss: 0.0273\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0194 - val_loss: 0.0245\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0165 - val_loss: 0.0224\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0142 - val_loss: 0.0208\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0125 - val_loss: 0.0196\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0111 - val_loss: 0.0186\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0101 - val_loss: 0.0179\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0092 - val_loss: 0.0173\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0085 - val_loss: 0.0168\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0079 - val_loss: 0.0164\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0074 - val_loss: 0.0160\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0070 - val_loss: 0.0157\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0067 - val_loss: 0.0155\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0064 - val_loss: 0.0152\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0061 - val_loss: 0.0150\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0059 - val_loss: 0.0148\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0057 - val_loss: 0.0146\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0055 - val_loss: 0.0144\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0053 - val_loss: 0.0143\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0052 - val_loss: 0.0141\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0050 - val_loss: 0.0139\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0049 - val_loss: 0.0138\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0048 - val_loss: 0.0137\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0047 - val_loss: 0.0135\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0046 - val_loss: 0.0134\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0045 - val_loss: 0.0133\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0044 - val_loss: 0.0132\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0044 - val_loss: 0.0132\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0043 - val_loss: 0.0131\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0043 - val_loss: 0.0130\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0042 - val_loss: 0.0129\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0042 - val_loss: 0.0129\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0041 - val_loss: 0.0128\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0041 - val_loss: 0.0128\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0041 - val_loss: 0.0127\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0040 - val_loss: 0.0127\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0040 - val_loss: 0.0126\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0040 - val_loss: 0.0126\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0039 - val_loss: 0.0126\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0039 - val_loss: 0.0125\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0039 - val_loss: 0.0125\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0039 - val_loss: 0.0125\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0039 - val_loss: 0.0125\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0038 - val_loss: 0.0124\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0038 - val_loss: 0.0124\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0038 - val_loss: 0.0124\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0038 - val_loss: 0.0124\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0038 - val_loss: 0.0124\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0038 - val_loss: 0.0123\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0038 - val_loss: 0.0123\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0037 - val_loss: 0.0123\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0037 - val_loss: 0.0123\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0037 - val_loss: 0.0123\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0037 - val_loss: 0.0123\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0037 - val_loss: 0.0123\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0037 - val_loss: 0.0123\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0037 - val_loss: 0.0123\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0037 - val_loss: 0.0123\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0037 - val_loss: 0.0123\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0036 - val_loss: 0.0123\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0036 - val_loss: 0.0123\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0036 - val_loss: 0.0123\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0036 - val_loss: 0.0123\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0036 - val_loss: 0.0122\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0036 - val_loss: 0.0122\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0036 - val_loss: 0.0122\n",
            "6/6 [==============================] - 1s 3ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 3 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 3s 28ms/step - loss: 0.3093 - val_loss: 0.2765\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.2644 - val_loss: 0.2431\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.2334 - val_loss: 0.2172\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.2082 - val_loss: 0.1956\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.1868 - val_loss: 0.1774\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.1684 - val_loss: 0.1614\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.1463 - val_loss: 0.1393\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.1159 - val_loss: 0.1128\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0897 - val_loss: 0.0926\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0708 - val_loss: 0.0780\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0581 - val_loss: 0.0681\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0494 - val_loss: 0.0610\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0433 - val_loss: 0.0560\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0387 - val_loss: 0.0522\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0353 - val_loss: 0.0493\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0327 - val_loss: 0.0470\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0306 - val_loss: 0.0451\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0289 - val_loss: 0.0435\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0274 - val_loss: 0.0421\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0261 - val_loss: 0.0408\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0249 - val_loss: 0.0396\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0239 - val_loss: 0.0385\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0229 - val_loss: 0.0374\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0220 - val_loss: 0.0364\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0211 - val_loss: 0.0355\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0203 - val_loss: 0.0346\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0195 - val_loss: 0.0338\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0188 - val_loss: 0.0330\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0181 - val_loss: 0.0322\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0174 - val_loss: 0.0315\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0168 - val_loss: 0.0308\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0162 - val_loss: 0.0301\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0157 - val_loss: 0.0295\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0152 - val_loss: 0.0289\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0147 - val_loss: 0.0283\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0142 - val_loss: 0.0277\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0137 - val_loss: 0.0272\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0133 - val_loss: 0.0267\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0129 - val_loss: 0.0262\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0125 - val_loss: 0.0258\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0122 - val_loss: 0.0253\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0118 - val_loss: 0.0249\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0115 - val_loss: 0.0245\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0112 - val_loss: 0.0242\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0109 - val_loss: 0.0238\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0106 - val_loss: 0.0234\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0104 - val_loss: 0.0231\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0101 - val_loss: 0.0228\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0099 - val_loss: 0.0225\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0096 - val_loss: 0.0222\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0094 - val_loss: 0.0219\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0092 - val_loss: 0.0216\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0090 - val_loss: 0.0214\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0089 - val_loss: 0.0211\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0087 - val_loss: 0.0209\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0085 - val_loss: 0.0207\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0084 - val_loss: 0.0204\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0082 - val_loss: 0.0202\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0081 - val_loss: 0.0200\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0080 - val_loss: 0.0199\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0078 - val_loss: 0.0197\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0077 - val_loss: 0.0195\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0076 - val_loss: 0.0193\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0075 - val_loss: 0.0192\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0074 - val_loss: 0.0190\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0073 - val_loss: 0.0189\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0072 - val_loss: 0.0187\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0071 - val_loss: 0.0186\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0070 - val_loss: 0.0185\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0070 - val_loss: 0.0184\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0069 - val_loss: 0.0183\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0068 - val_loss: 0.0181\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0068 - val_loss: 0.0180\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0067 - val_loss: 0.0179\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0066 - val_loss: 0.0178\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 4 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 3s 18ms/step - loss: 0.2647 - val_loss: 0.2070\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.1565 - val_loss: 0.1374\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.1018 - val_loss: 0.1020\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0712 - val_loss: 0.0802\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0520 - val_loss: 0.0665\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0399 - val_loss: 0.0575\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0319 - val_loss: 0.0514\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0265 - val_loss: 0.0469\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0226 - val_loss: 0.0436\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0197 - val_loss: 0.0410\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0176 - val_loss: 0.0389\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0160 - val_loss: 0.0372\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0147 - val_loss: 0.0357\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0136 - val_loss: 0.0344\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0127 - val_loss: 0.0332\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0119 - val_loss: 0.0321\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0111 - val_loss: 0.0311\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0105 - val_loss: 0.0302\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0099 - val_loss: 0.0293\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0094 - val_loss: 0.0285\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0090 - val_loss: 0.0277\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0086 - val_loss: 0.0270\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0082 - val_loss: 0.0264\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0079 - val_loss: 0.0257\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0075 - val_loss: 0.0252\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0073 - val_loss: 0.0246\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0070 - val_loss: 0.0241\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0068 - val_loss: 0.0236\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0066 - val_loss: 0.0231\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0064 - val_loss: 0.0227\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0062 - val_loss: 0.0223\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0060 - val_loss: 0.0219\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0057 - val_loss: 0.0212\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0056 - val_loss: 0.0208\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0055 - val_loss: 0.0205\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0053 - val_loss: 0.0202\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0052 - val_loss: 0.0199\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0051 - val_loss: 0.0196\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0050 - val_loss: 0.0194\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0049 - val_loss: 0.0191\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0049 - val_loss: 0.0189\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0048 - val_loss: 0.0187\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0047 - val_loss: 0.0185\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0046 - val_loss: 0.0183\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0046 - val_loss: 0.0181\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0045 - val_loss: 0.0179\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0045 - val_loss: 0.0177\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0044 - val_loss: 0.0175\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0044 - val_loss: 0.0174\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0043 - val_loss: 0.0172\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0043 - val_loss: 0.0171\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0042 - val_loss: 0.0170\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0042 - val_loss: 0.0168\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0041 - val_loss: 0.0167\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0041 - val_loss: 0.0166\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0041 - val_loss: 0.0165\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0040 - val_loss: 0.0164\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0040 - val_loss: 0.0162\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0040 - val_loss: 0.0161\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0040 - val_loss: 0.0161\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0039 - val_loss: 0.0160\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0039 - val_loss: 0.0159\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0039 - val_loss: 0.0158\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0039 - val_loss: 0.0157\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0038 - val_loss: 0.0156\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0038 - val_loss: 0.0155\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0038 - val_loss: 0.0155\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0038 - val_loss: 0.0154\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0037 - val_loss: 0.0153\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0037 - val_loss: 0.0152\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0037 - val_loss: 0.0152\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0037 - val_loss: 0.0151\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0037 - val_loss: 0.0151\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0037 - val_loss: 0.0150\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 5 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 4s 24ms/step - loss: 0.0869 - val_loss: 0.0761\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0405 - val_loss: 0.0531\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0256 - val_loss: 0.0435\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0195 - val_loss: 0.0386\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0166 - val_loss: 0.0353\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0147 - val_loss: 0.0327\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0132 - val_loss: 0.0306\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0120 - val_loss: 0.0287\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0110 - val_loss: 0.0270\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0101 - val_loss: 0.0255\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0093 - val_loss: 0.0242\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0087 - val_loss: 0.0231\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0081 - val_loss: 0.0221\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0076 - val_loss: 0.0213\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0072 - val_loss: 0.0206\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0069 - val_loss: 0.0200\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0066 - val_loss: 0.0195\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0064 - val_loss: 0.0190\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0062 - val_loss: 0.0186\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0060 - val_loss: 0.0183\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0059 - val_loss: 0.0180\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0057 - val_loss: 0.0178\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0056 - val_loss: 0.0176\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0055 - val_loss: 0.0174\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0054 - val_loss: 0.0173\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0053 - val_loss: 0.0171\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0052 - val_loss: 0.0171\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0051 - val_loss: 0.0170\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0050 - val_loss: 0.0169\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0049 - val_loss: 0.0169\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0048 - val_loss: 0.0168\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0047 - val_loss: 0.0168\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0047 - val_loss: 0.0168\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0046 - val_loss: 0.0167\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0045 - val_loss: 0.0167\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0044 - val_loss: 0.0167\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0044 - val_loss: 0.0168\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0043 - val_loss: 0.0168\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0042 - val_loss: 0.0168\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0042 - val_loss: 0.0168\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 6 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 3s 18ms/step - loss: 0.1281 - val_loss: 0.0866\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0777 - val_loss: 0.0541\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0492 - val_loss: 0.0389\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0344 - val_loss: 0.0319\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0264 - val_loss: 0.0287\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0218 - val_loss: 0.0272\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0189 - val_loss: 0.0264\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0170 - val_loss: 0.0258\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0155 - val_loss: 0.0252\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0143 - val_loss: 0.0246\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0133 - val_loss: 0.0240\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0125 - val_loss: 0.0235\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0118 - val_loss: 0.0230\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0111 - val_loss: 0.0225\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0105 - val_loss: 0.0219\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0100 - val_loss: 0.0215\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0095 - val_loss: 0.0210\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0090 - val_loss: 0.0206\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0086 - val_loss: 0.0203\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0082 - val_loss: 0.0199\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0079 - val_loss: 0.0196\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0076 - val_loss: 0.0193\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0073 - val_loss: 0.0191\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0070 - val_loss: 0.0188\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0068 - val_loss: 0.0186\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0066 - val_loss: 0.0184\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0064 - val_loss: 0.0182\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0062 - val_loss: 0.0181\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0061 - val_loss: 0.0179\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0059 - val_loss: 0.0177\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0058 - val_loss: 0.0176\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0057 - val_loss: 0.0175\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0055 - val_loss: 0.0173\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0054 - val_loss: 0.0172\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0053 - val_loss: 0.0171\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0052 - val_loss: 0.0170\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0051 - val_loss: 0.0169\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0050 - val_loss: 0.0167\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0050 - val_loss: 0.0166\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0049 - val_loss: 0.0165\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0048 - val_loss: 0.0165\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0047 - val_loss: 0.0164\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0047 - val_loss: 0.0163\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0046 - val_loss: 0.0163\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0045 - val_loss: 0.0162\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0045 - val_loss: 0.0161\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0044 - val_loss: 0.0161\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0044 - val_loss: 0.0160\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0043 - val_loss: 0.0160\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0043 - val_loss: 0.0159\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0042 - val_loss: 0.0158\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0042 - val_loss: 0.0158\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0041 - val_loss: 0.0157\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0041 - val_loss: 0.0156\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0040 - val_loss: 0.0156\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0040 - val_loss: 0.0156\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0040 - val_loss: 0.0155\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0039 - val_loss: 0.0155\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0039 - val_loss: 0.0154\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0038 - val_loss: 0.0154\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0038 - val_loss: 0.0154\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0038 - val_loss: 0.0153\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0037 - val_loss: 0.0153\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0037 - val_loss: 0.0152\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0037 - val_loss: 0.0152\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0036 - val_loss: 0.0152\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0036 - val_loss: 0.0152\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0036 - val_loss: 0.0152\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0035 - val_loss: 0.0151\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0035 - val_loss: 0.0151\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0035 - val_loss: 0.0151\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0035 - val_loss: 0.0151\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0034 - val_loss: 0.0150\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0034 - val_loss: 0.0150\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0034 - val_loss: 0.0150\n",
            "6/6 [==============================] - 0s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 7 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 4s 24ms/step - loss: 0.2044 - val_loss: 0.1444\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.1129 - val_loss: 0.0980\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0761 - val_loss: 0.0774\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0576 - val_loss: 0.0670\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0474 - val_loss: 0.0613\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0411 - val_loss: 0.0578\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0368 - val_loss: 0.0552\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0335 - val_loss: 0.0532\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0309 - val_loss: 0.0515\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0286 - val_loss: 0.0499\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0267 - val_loss: 0.0484\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0249 - val_loss: 0.0470\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0234 - val_loss: 0.0458\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0220 - val_loss: 0.0446\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0207 - val_loss: 0.0434\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0196 - val_loss: 0.0424\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0185 - val_loss: 0.0414\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0175 - val_loss: 0.0405\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0167 - val_loss: 0.0397\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0159 - val_loss: 0.0389\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0151 - val_loss: 0.0381\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0144 - val_loss: 0.0374\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0138 - val_loss: 0.0368\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0132 - val_loss: 0.0361\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0126 - val_loss: 0.0356\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0121 - val_loss: 0.0350\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0117 - val_loss: 0.0345\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0113 - val_loss: 0.0340\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0108 - val_loss: 0.0336\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0105 - val_loss: 0.0332\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0101 - val_loss: 0.0328\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0098 - val_loss: 0.0324\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0095 - val_loss: 0.0320\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0093 - val_loss: 0.0317\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0090 - val_loss: 0.0314\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0088 - val_loss: 0.0311\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0086 - val_loss: 0.0308\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0084 - val_loss: 0.0305\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0082 - val_loss: 0.0303\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0080 - val_loss: 0.0300\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0078 - val_loss: 0.0298\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0077 - val_loss: 0.0296\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0075 - val_loss: 0.0293\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0074 - val_loss: 0.0292\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0073 - val_loss: 0.0290\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0071 - val_loss: 0.0288\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0070 - val_loss: 0.0287\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0069 - val_loss: 0.0286\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0068 - val_loss: 0.0284\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0067 - val_loss: 0.0283\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0066 - val_loss: 0.0282\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0065 - val_loss: 0.0282\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0065 - val_loss: 0.0281\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0064 - val_loss: 0.0280\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0063 - val_loss: 0.0280\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0062 - val_loss: 0.0279\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0062 - val_loss: 0.0278\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0061 - val_loss: 0.0277\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0060 - val_loss: 0.0276\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0060 - val_loss: 0.0275\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0060 - val_loss: 0.0274\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0059 - val_loss: 0.0273\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0059 - val_loss: 0.0273\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0058 - val_loss: 0.0272\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0058 - val_loss: 0.0271\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0058 - val_loss: 0.0271\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0057 - val_loss: 0.0270\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0057 - val_loss: 0.0269\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0057 - val_loss: 0.0269\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0268\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0267\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0267\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0056 - val_loss: 0.0266\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0055 - val_loss: 0.0266\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0055 - val_loss: 0.0265\n",
            "6/6 [==============================] - 1s 3ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 8 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 3s 18ms/step - loss: 0.4083 - val_loss: 0.3254\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.2979 - val_loss: 0.2629\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.2504 - val_loss: 0.2320\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.2205 - val_loss: 0.2092\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.1981 - val_loss: 0.1902\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.1795 - val_loss: 0.1737\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.1633 - val_loss: 0.1593\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.1490 - val_loss: 0.1467\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.1362 - val_loss: 0.1352\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.1248 - val_loss: 0.1248\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.1144 - val_loss: 0.1153\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.1051 - val_loss: 0.1067\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0966 - val_loss: 0.0989\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0889 - val_loss: 0.0917\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0818 - val_loss: 0.0852\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0755 - val_loss: 0.0793\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0697 - val_loss: 0.0740\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0644 - val_loss: 0.0692\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0597 - val_loss: 0.0648\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0554 - val_loss: 0.0608\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0515 - val_loss: 0.0572\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0480 - val_loss: 0.0539\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0447 - val_loss: 0.0509\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0418 - val_loss: 0.0481\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0392 - val_loss: 0.0456\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0368 - val_loss: 0.0433\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0346 - val_loss: 0.0411\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0326 - val_loss: 0.0392\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0307 - val_loss: 0.0374\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0290 - val_loss: 0.0357\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0275 - val_loss: 0.0341\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0261 - val_loss: 0.0327\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0248 - val_loss: 0.0313\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0235 - val_loss: 0.0301\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0224 - val_loss: 0.0289\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0214 - val_loss: 0.0278\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0204 - val_loss: 0.0267\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0196 - val_loss: 0.0258\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0187 - val_loss: 0.0249\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0180 - val_loss: 0.0240\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0173 - val_loss: 0.0232\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0166 - val_loss: 0.0225\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0160 - val_loss: 0.0218\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0155 - val_loss: 0.0212\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0149 - val_loss: 0.0205\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0145 - val_loss: 0.0200\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0140 - val_loss: 0.0194\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0136 - val_loss: 0.0189\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0132 - val_loss: 0.0184\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0128 - val_loss: 0.0179\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0125 - val_loss: 0.0175\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0121 - val_loss: 0.0171\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0118 - val_loss: 0.0167\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0115 - val_loss: 0.0163\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0112 - val_loss: 0.0160\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0110 - val_loss: 0.0157\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0107 - val_loss: 0.0154\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0105 - val_loss: 0.0151\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0103 - val_loss: 0.0149\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0100 - val_loss: 0.0146\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0098 - val_loss: 0.0144\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0096 - val_loss: 0.0141\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0095 - val_loss: 0.0139\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0093 - val_loss: 0.0137\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0091 - val_loss: 0.0135\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0090 - val_loss: 0.0133\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0088 - val_loss: 0.0131\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0087 - val_loss: 0.0129\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0085 - val_loss: 0.0128\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0084 - val_loss: 0.0126\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0083 - val_loss: 0.0125\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0082 - val_loss: 0.0123\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0081 - val_loss: 0.0122\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0080 - val_loss: 0.0120\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0079 - val_loss: 0.0119\n",
            "6/6 [==============================] - 1s 7ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 9 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 4s 21ms/step - loss: 0.1949 - val_loss: 0.1585\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.1401 - val_loss: 0.1212\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.1084 - val_loss: 0.0968\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0870 - val_loss: 0.0797\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0716 - val_loss: 0.0671\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0602 - val_loss: 0.0575\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0516 - val_loss: 0.0502\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0449 - val_loss: 0.0444\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0397 - val_loss: 0.0398\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0355 - val_loss: 0.0361\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0321 - val_loss: 0.0330\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0294 - val_loss: 0.0304\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0270 - val_loss: 0.0282\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0251 - val_loss: 0.0263\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0234 - val_loss: 0.0247\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0219 - val_loss: 0.0233\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0206 - val_loss: 0.0220\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0195 - val_loss: 0.0210\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0185 - val_loss: 0.0200\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0175 - val_loss: 0.0191\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0167 - val_loss: 0.0183\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0159 - val_loss: 0.0176\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0152 - val_loss: 0.0170\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0146 - val_loss: 0.0165\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0140 - val_loss: 0.0160\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0135 - val_loss: 0.0156\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0130 - val_loss: 0.0152\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0125 - val_loss: 0.0149\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0121 - val_loss: 0.0146\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0117 - val_loss: 0.0143\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0114 - val_loss: 0.0141\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0110 - val_loss: 0.0139\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0107 - val_loss: 0.0137\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0105 - val_loss: 0.0135\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0102 - val_loss: 0.0134\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0099 - val_loss: 0.0133\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0097 - val_loss: 0.0131\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0095 - val_loss: 0.0131\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0093 - val_loss: 0.0130\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0091 - val_loss: 0.0129\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0089 - val_loss: 0.0128\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0088 - val_loss: 0.0128\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0086 - val_loss: 0.0127\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0084 - val_loss: 0.0127\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0083 - val_loss: 0.0127\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0082 - val_loss: 0.0126\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0080 - val_loss: 0.0126\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0079 - val_loss: 0.0126\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0078 - val_loss: 0.0126\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0077 - val_loss: 0.0126\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0076 - val_loss: 0.0126\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0075 - val_loss: 0.0125\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0074 - val_loss: 0.0125\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0073 - val_loss: 0.0125\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0072 - val_loss: 0.0125\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0071 - val_loss: 0.0125\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0070 - val_loss: 0.0125\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0069 - val_loss: 0.0125\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0069 - val_loss: 0.0125\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0068 - val_loss: 0.0125\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0067 - val_loss: 0.0125\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0066 - val_loss: 0.0125\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0066 - val_loss: 0.0125\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0065 - val_loss: 0.0125\n",
            "6/6 [==============================] - 0s 5ms/step\n",
            "Best_hyper_parameters(LSTM): \n",
            " {'model': [32], 'optimizer': 'Adagrad', 'learning_rate': 0.01, 'batch_size': 8, 'best_avg_rmse': 2090.856698054022}\n",
            "all_avg_rmse(LSTM): \n",
            " [[[3975.40560762 2914.30569127 2599.29913848]\n",
            "  [2308.79539184 2206.69362012 2181.54641355]\n",
            "  [2183.16372113 2171.14934003 2203.39996292]]\n",
            "\n",
            " [[2161.78658876 2178.6745578  2134.29531138]\n",
            "  [2096.91215259 2090.85669805 2135.29319645]\n",
            "  [2283.6049336  2440.02479124 2582.72605508]]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model': [32],\n",
              " 'optimizer': 'Adagrad',\n",
              " 'learning_rate': 0.01,\n",
              " 'batch_size': 8,\n",
              " 'best_avg_rmse': 2090.856698054022}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Case IV: 64N-LSTM**"
      ],
      "metadata": {
        "id": "uYfHSiZfxKUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layers = [64]\n",
        "time_step = 5\n",
        "optimizers_names = ['Adam', 'Adagrad']\n",
        "learning_rates =  [0.1, 0.01, 0.001]\n",
        "batch_sizes =  [4, 8, 16]\n",
        "epochs = 75\n",
        "num_replicates = 10\n",
        "test_split = 0.2\n",
        "val_split = 0.2\n",
        "\n",
        "train_data, test_data = data_split(data, test_split)\n",
        "\n",
        "lstm_N64_best_hyper_parameters = LSTM_Hyper_Parameter_Tuning(layers, train_data, time_step, val_split, optimizers_names, learning_rates, batch_sizes, epochs = epochs, num_replicates = num_replicates)\n",
        "lstm_N64_best_hyper_parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdnOjQNHwt41",
        "outputId": "39320e5f-9a7f-46e2-a4cc-a9e48322ba09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 5.7036e-04 - val_loss: 0.0036\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 5.5909e-04 - val_loss: 0.0036\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 5.4675e-04 - val_loss: 0.0035\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 5.3696e-04 - val_loss: 0.0034\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 5.2595e-04 - val_loss: 0.0032\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5.1898e-04 - val_loss: 0.0032\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5.0706e-04 - val_loss: 0.0032\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 4.9738e-04 - val_loss: 0.0033\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 4.9008e-04 - val_loss: 0.0032\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 4.8094e-04 - val_loss: 0.0031\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adagrad optimizer 0.01 learning_rate 16 batch_size and 4 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 4s 23ms/step - loss: 0.0232 - val_loss: 0.0189\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0058 - val_loss: 0.0179\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0049 - val_loss: 0.0185\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0042 - val_loss: 0.0189\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0036 - val_loss: 0.0175\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0032 - val_loss: 0.0181\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0029 - val_loss: 0.0180\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0163\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0025 - val_loss: 0.0173\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0023 - val_loss: 0.0162\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0022 - val_loss: 0.0164\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0021 - val_loss: 0.0160\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0020 - val_loss: 0.0148\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0019 - val_loss: 0.0152\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0018 - val_loss: 0.0146\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0018 - val_loss: 0.0135\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0017 - val_loss: 0.0129\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0017 - val_loss: 0.0133\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0016 - val_loss: 0.0131\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0016 - val_loss: 0.0129\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 0.0118\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 0.0116\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 0.0122\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0014 - val_loss: 0.0113\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 0.0108\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0013 - val_loss: 0.0105\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0013 - val_loss: 0.0103\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0012 - val_loss: 0.0097\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 0.0097\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.0012 - val_loss: 0.0093\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0011 - val_loss: 0.0090\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0011 - val_loss: 0.0092\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 0.0088\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0011 - val_loss: 0.0083\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0011 - val_loss: 0.0084\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0010 - val_loss: 0.0078\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 9.9395e-04 - val_loss: 0.0077\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 9.6379e-04 - val_loss: 0.0076\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 9.4598e-04 - val_loss: 0.0073\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 9.1359e-04 - val_loss: 0.0072\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 8.9816e-04 - val_loss: 0.0071\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 8.8031e-04 - val_loss: 0.0070\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 8.5526e-04 - val_loss: 0.0065\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 8.3532e-04 - val_loss: 0.0069\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 8.2248e-04 - val_loss: 0.0063\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 8.0628e-04 - val_loss: 0.0060\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 7.8641e-04 - val_loss: 0.0059\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 7.6853e-04 - val_loss: 0.0060\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 7.5041e-04 - val_loss: 0.0060\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 7.3903e-04 - val_loss: 0.0057\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 7.1846e-04 - val_loss: 0.0056\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 7.1047e-04 - val_loss: 0.0056\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.9784e-04 - val_loss: 0.0053\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.8409e-04 - val_loss: 0.0050\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.6813e-04 - val_loss: 0.0049\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.6128e-04 - val_loss: 0.0049\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 6.4153e-04 - val_loss: 0.0047\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.2712e-04 - val_loss: 0.0050\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.2498e-04 - val_loss: 0.0046\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.1072e-04 - val_loss: 0.0043\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 6.0665e-04 - val_loss: 0.0044\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5.8761e-04 - val_loss: 0.0043\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5.7995e-04 - val_loss: 0.0042\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 5.7080e-04 - val_loss: 0.0042\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 5.6378e-04 - val_loss: 0.0041\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 12ms/step - loss: 5.5399e-04 - val_loss: 0.0042\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 5.4583e-04 - val_loss: 0.0039\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5.3512e-04 - val_loss: 0.0039\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5.3076e-04 - val_loss: 0.0038\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 5.2464e-04 - val_loss: 0.0037\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5.1249e-04 - val_loss: 0.0035\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5.0325e-04 - val_loss: 0.0034\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 5.0043e-04 - val_loss: 0.0034\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 4.8805e-04 - val_loss: 0.0033\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 4.8367e-04 - val_loss: 0.0034\n",
            "6/6 [==============================] - 1s 5ms/step\n",
            "Running for Adagrad optimizer 0.01 learning_rate 16 batch_size and 5 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 4s 28ms/step - loss: 0.0758 - val_loss: 0.0436\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0127 - val_loss: 0.0272\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0059 - val_loss: 0.0201\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0045 - val_loss: 0.0185\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0040 - val_loss: 0.0176\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0038 - val_loss: 0.0171\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0036 - val_loss: 0.0172\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0034 - val_loss: 0.0158\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0033 - val_loss: 0.0158\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0032 - val_loss: 0.0155\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0031 - val_loss: 0.0155\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0029 - val_loss: 0.0149\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0028 - val_loss: 0.0144\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0027 - val_loss: 0.0141\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0026 - val_loss: 0.0144\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0026 - val_loss: 0.0131\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0025 - val_loss: 0.0134\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0024 - val_loss: 0.0131\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0024 - val_loss: 0.0128\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0023 - val_loss: 0.0121\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0022 - val_loss: 0.0116\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0022 - val_loss: 0.0115\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0110\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0020 - val_loss: 0.0112\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0020 - val_loss: 0.0105\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0019 - val_loss: 0.0105\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0019 - val_loss: 0.0100\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0099\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0093\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0091\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0017 - val_loss: 0.0087\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0016 - val_loss: 0.0084\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0016 - val_loss: 0.0081\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 0.0080\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 0.0082\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0014 - val_loss: 0.0077\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0014 - val_loss: 0.0074\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0014 - val_loss: 0.0076\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0013 - val_loss: 0.0069\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0013 - val_loss: 0.0071\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0013 - val_loss: 0.0065\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0012 - val_loss: 0.0067\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0012 - val_loss: 0.0064\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0011 - val_loss: 0.0059\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0011 - val_loss: 0.0058\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 0.0056\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0010 - val_loss: 0.0052\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0010 - val_loss: 0.0053\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 9.9415e-04 - val_loss: 0.0051\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 9.6536e-04 - val_loss: 0.0048\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 9.4403e-04 - val_loss: 0.0047\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 9.1403e-04 - val_loss: 0.0046\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 8.8556e-04 - val_loss: 0.0046\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 8.6593e-04 - val_loss: 0.0045\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 8.4425e-04 - val_loss: 0.0043\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 12ms/step - loss: 8.2161e-04 - val_loss: 0.0041\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 7.9760e-04 - val_loss: 0.0037\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 7.8243e-04 - val_loss: 0.0039\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 7.5864e-04 - val_loss: 0.0038\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 7.3534e-04 - val_loss: 0.0035\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 7.1475e-04 - val_loss: 0.0036\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 7.0048e-04 - val_loss: 0.0035\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.8475e-04 - val_loss: 0.0033\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 6.6038e-04 - val_loss: 0.0033\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.5464e-04 - val_loss: 0.0034\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.3665e-04 - val_loss: 0.0032\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.1591e-04 - val_loss: 0.0031\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.0464e-04 - val_loss: 0.0029\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5.9258e-04 - val_loss: 0.0028\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5.8144e-04 - val_loss: 0.0028\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5.6755e-04 - val_loss: 0.0028\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 5.5189e-04 - val_loss: 0.0025\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5.4384e-04 - val_loss: 0.0025\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 5.3226e-04 - val_loss: 0.0025\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5.2266e-04 - val_loss: 0.0025\n",
            "6/6 [==============================] - 0s 4ms/step\n",
            "Running for Adagrad optimizer 0.01 learning_rate 16 batch_size and 6 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 3s 20ms/step - loss: 0.0336 - val_loss: 0.0138\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0052 - val_loss: 0.0105\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0044 - val_loss: 0.0101\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0040 - val_loss: 0.0099\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0038 - val_loss: 0.0098\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0035 - val_loss: 0.0101\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0034 - val_loss: 0.0100\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0032 - val_loss: 0.0100\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0031 - val_loss: 0.0098\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0103\n",
            "6/6 [==============================] - 0s 5ms/step\n",
            "Running for Adagrad optimizer 0.01 learning_rate 16 batch_size and 7 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 4s 29ms/step - loss: 0.0755 - val_loss: 0.0241\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0139 - val_loss: 0.0152\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0073 - val_loss: 0.0139\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0056 - val_loss: 0.0120\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0049 - val_loss: 0.0120\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0043 - val_loss: 0.0125\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0039 - val_loss: 0.0118\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0035 - val_loss: 0.0119\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0032 - val_loss: 0.0116\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0030 - val_loss: 0.0113\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0028 - val_loss: 0.0105\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0108\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0024 - val_loss: 0.0099\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0023 - val_loss: 0.0095\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0022 - val_loss: 0.0097\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0021 - val_loss: 0.0103\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0020 - val_loss: 0.0094\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0019 - val_loss: 0.0094\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0091\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0089\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0082\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0016 - val_loss: 0.0082\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0084\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0015 - val_loss: 0.0072\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0014 - val_loss: 0.0075\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0014 - val_loss: 0.0073\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0013 - val_loss: 0.0068\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0013 - val_loss: 0.0071\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0013 - val_loss: 0.0067\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0012 - val_loss: 0.0065\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0012 - val_loss: 0.0062\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0059\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0011 - val_loss: 0.0062\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0011 - val_loss: 0.0058\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0011 - val_loss: 0.0059\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0010 - val_loss: 0.0054\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 9.9199e-04 - val_loss: 0.0050\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 9.6439e-04 - val_loss: 0.0048\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 9.3882e-04 - val_loss: 0.0049\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 9.1857e-04 - val_loss: 0.0045\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 8.9474e-04 - val_loss: 0.0049\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 8.7192e-04 - val_loss: 0.0044\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 8.5288e-04 - val_loss: 0.0042\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 8.3107e-04 - val_loss: 0.0040\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 8.0302e-04 - val_loss: 0.0040\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 7.7739e-04 - val_loss: 0.0041\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 7.6903e-04 - val_loss: 0.0037\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 7.4326e-04 - val_loss: 0.0039\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 7.3063e-04 - val_loss: 0.0038\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 7.1043e-04 - val_loss: 0.0034\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 6.9703e-04 - val_loss: 0.0036\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.7864e-04 - val_loss: 0.0031\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 6.6072e-04 - val_loss: 0.0033\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 6.4620e-04 - val_loss: 0.0029\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 6.3539e-04 - val_loss: 0.0030\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 6.1835e-04 - val_loss: 0.0030\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 6.0565e-04 - val_loss: 0.0029\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 5.8822e-04 - val_loss: 0.0031\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 5.8899e-04 - val_loss: 0.0027\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 5.7444e-04 - val_loss: 0.0027\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 5.6423e-04 - val_loss: 0.0026\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 5.5200e-04 - val_loss: 0.0026\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 5.4453e-04 - val_loss: 0.0024\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 5.3511e-04 - val_loss: 0.0024\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 5.2719e-04 - val_loss: 0.0023\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 5.1222e-04 - val_loss: 0.0024\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 5.0413e-04 - val_loss: 0.0021\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5.0386e-04 - val_loss: 0.0022\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 4.9118e-04 - val_loss: 0.0021\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 4.8444e-04 - val_loss: 0.0019\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 4.7777e-04 - val_loss: 0.0019\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 4.7408e-04 - val_loss: 0.0020\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 4.6581e-04 - val_loss: 0.0020\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.5719e-04 - val_loss: 0.0020\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 4.5181e-04 - val_loss: 0.0020\n",
            "6/6 [==============================] - 0s 4ms/step\n",
            "Running for Adagrad optimizer 0.01 learning_rate 16 batch_size and 8 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 4s 29ms/step - loss: 0.0841 - val_loss: 0.0442\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0192 - val_loss: 0.0308\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0101 - val_loss: 0.0243\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0064 - val_loss: 0.0205\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0050 - val_loss: 0.0194\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 4s 103ms/step - loss: 0.0044 - val_loss: 0.0177\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0041 - val_loss: 0.0181\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0037 - val_loss: 0.0174\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0035 - val_loss: 0.0168\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0034 - val_loss: 0.0162\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0033 - val_loss: 0.0162\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0032 - val_loss: 0.0166\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0031 - val_loss: 0.0158\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0030 - val_loss: 0.0156\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0029 - val_loss: 0.0160\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0029 - val_loss: 0.0158\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0028 - val_loss: 0.0152\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0027 - val_loss: 0.0153\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0027 - val_loss: 0.0148\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0146\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0146\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0025 - val_loss: 0.0143\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0025 - val_loss: 0.0141\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0024 - val_loss: 0.0140\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0024 - val_loss: 0.0135\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0023 - val_loss: 0.0136\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0023 - val_loss: 0.0131\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0022 - val_loss: 0.0129\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0022 - val_loss: 0.0130\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0022 - val_loss: 0.0127\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0021 - val_loss: 0.0122\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0021 - val_loss: 0.0121\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0020 - val_loss: 0.0121\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0020 - val_loss: 0.0119\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0020 - val_loss: 0.0117\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.0019 - val_loss: 0.0110\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.0019 - val_loss: 0.0118\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0019 - val_loss: 0.0113\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0018 - val_loss: 0.0108\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0018 - val_loss: 0.0108\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0018 - val_loss: 0.0106\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0105\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0017 - val_loss: 0.0100\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0017 - val_loss: 0.0100\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0016 - val_loss: 0.0100\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0016 - val_loss: 0.0094\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0016 - val_loss: 0.0093\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 0.0090\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 0.0092\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 0.0087\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0014 - val_loss: 0.0082\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0014 - val_loss: 0.0084\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0014 - val_loss: 0.0084\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 0.0082\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 0.0079\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0013 - val_loss: 0.0078\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0013 - val_loss: 0.0080\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0013 - val_loss: 0.0073\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0013 - val_loss: 0.0074\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0012 - val_loss: 0.0072\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0012 - val_loss: 0.0073\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0012 - val_loss: 0.0070\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0012 - val_loss: 0.0067\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 0.0066\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 0.0065\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 0.0062\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0011 - val_loss: 0.0062\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0011 - val_loss: 0.0060\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0010 - val_loss: 0.0059\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0010 - val_loss: 0.0058\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 9.8861e-04 - val_loss: 0.0056\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 9.7368e-04 - val_loss: 0.0057\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 9.5623e-04 - val_loss: 0.0055\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 9.3887e-04 - val_loss: 0.0054\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 9.1623e-04 - val_loss: 0.0052\n",
            "6/6 [==============================] - 1s 6ms/step\n",
            "Running for Adagrad optimizer 0.01 learning_rate 16 batch_size and 9 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 4s 23ms/step - loss: 0.0405 - val_loss: 0.0213\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0126 - val_loss: 0.0144\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0072 - val_loss: 0.0131\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0056 - val_loss: 0.0118\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0049 - val_loss: 0.0124\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0044 - val_loss: 0.0121\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0040 - val_loss: 0.0113\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0037 - val_loss: 0.0112\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0035 - val_loss: 0.0110\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0033 - val_loss: 0.0112\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0031 - val_loss: 0.0107\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0030 - val_loss: 0.0102\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0028 - val_loss: 0.0104\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0027 - val_loss: 0.0100\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0026 - val_loss: 0.0104\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0025 - val_loss: 0.0100\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0024 - val_loss: 0.0098\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0024 - val_loss: 0.0102\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0023 - val_loss: 0.0096\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0022 - val_loss: 0.0092\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0022 - val_loss: 0.0091\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0095\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0087\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0020 - val_loss: 0.0089\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0020 - val_loss: 0.0083\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0019 - val_loss: 0.0081\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0019 - val_loss: 0.0080\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0018 - val_loss: 0.0078\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0018 - val_loss: 0.0076\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0017 - val_loss: 0.0078\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.0017 - val_loss: 0.0074\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0017 - val_loss: 0.0075\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0016 - val_loss: 0.0070\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0016 - val_loss: 0.0068\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0016 - val_loss: 0.0066\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0015 - val_loss: 0.0069\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0015 - val_loss: 0.0064\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0015 - val_loss: 0.0061\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0014 - val_loss: 0.0063\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0014 - val_loss: 0.0059\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0014 - val_loss: 0.0058\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.0014 - val_loss: 0.0055\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0013 - val_loss: 0.0056\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0013 - val_loss: 0.0054\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0013 - val_loss: 0.0053\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0012 - val_loss: 0.0051\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0012 - val_loss: 0.0048\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 0.0048\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0012 - val_loss: 0.0048\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0012 - val_loss: 0.0045\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0011 - val_loss: 0.0044\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 0.0042\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 0.0043\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 0.0043\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0010 - val_loss: 0.0040\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0010 - val_loss: 0.0040\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0010 - val_loss: 0.0039\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 9.8863e-04 - val_loss: 0.0037\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 9.6313e-04 - val_loss: 0.0035\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 9.4618e-04 - val_loss: 0.0035\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 9.2784e-04 - val_loss: 0.0034\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 9.0670e-04 - val_loss: 0.0033\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 8.9157e-04 - val_loss: 0.0032\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 8.7016e-04 - val_loss: 0.0031\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 8.5820e-04 - val_loss: 0.0031\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 8.4386e-04 - val_loss: 0.0031\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 8.2345e-04 - val_loss: 0.0030\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 8.1882e-04 - val_loss: 0.0029\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 8.0289e-04 - val_loss: 0.0028\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 7.8946e-04 - val_loss: 0.0027\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 7.7319e-04 - val_loss: 0.0027\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 7.5416e-04 - val_loss: 0.0026\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 7.5277e-04 - val_loss: 0.0026\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 7.3467e-04 - val_loss: 0.0024\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 7.2291e-04 - val_loss: 0.0024\n",
            "6/6 [==============================] - 0s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 0 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 4s 10ms/step - loss: 0.2346 - val_loss: 0.1699\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.1193 - val_loss: 0.0990\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0694 - val_loss: 0.0609\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0444 - val_loss: 0.0449\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0329 - val_loss: 0.0372\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0266 - val_loss: 0.0328\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0224 - val_loss: 0.0295\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0192 - val_loss: 0.0270\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0166 - val_loss: 0.0248\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0145 - val_loss: 0.0230\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0128 - val_loss: 0.0215\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0114 - val_loss: 0.0202\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0102 - val_loss: 0.0190\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0092 - val_loss: 0.0180\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0084 - val_loss: 0.0172\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0077 - val_loss: 0.0166\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0072 - val_loss: 0.0160\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0067 - val_loss: 0.0155\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0063 - val_loss: 0.0151\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0060 - val_loss: 0.0147\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0057 - val_loss: 0.0143\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0055 - val_loss: 0.0141\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0052 - val_loss: 0.0139\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0051 - val_loss: 0.0136\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0049 - val_loss: 0.0135\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0048 - val_loss: 0.0133\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0047 - val_loss: 0.0132\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0046 - val_loss: 0.0130\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0045 - val_loss: 0.0129\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0044 - val_loss: 0.0128\n",
            "Epoch 31/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0043 - val_loss: 0.0127\n",
            "Epoch 32/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0042 - val_loss: 0.0127\n",
            "Epoch 33/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0042 - val_loss: 0.0126\n",
            "Epoch 34/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0041 - val_loss: 0.0125\n",
            "Epoch 35/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0040 - val_loss: 0.0124\n",
            "Epoch 36/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0040 - val_loss: 0.0123\n",
            "Epoch 37/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0039 - val_loss: 0.0123\n",
            "Epoch 38/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0039 - val_loss: 0.0123\n",
            "Epoch 39/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0038 - val_loss: 0.0122\n",
            "Epoch 40/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0038 - val_loss: 0.0122\n",
            "Epoch 41/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0122\n",
            "Epoch 42/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0121\n",
            "Epoch 43/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0036 - val_loss: 0.0121\n",
            "Epoch 44/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0036 - val_loss: 0.0120\n",
            "Epoch 45/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0120\n",
            "Epoch 46/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0035 - val_loss: 0.0119\n",
            "Epoch 47/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0035 - val_loss: 0.0119\n",
            "Epoch 48/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0035 - val_loss: 0.0119\n",
            "Epoch 49/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0034 - val_loss: 0.0119\n",
            "Epoch 50/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0034 - val_loss: 0.0118\n",
            "Epoch 51/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0034 - val_loss: 0.0118\n",
            "Epoch 52/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0033 - val_loss: 0.0118\n",
            "Epoch 53/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0033 - val_loss: 0.0118\n",
            "Epoch 54/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0118\n",
            "Epoch 55/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0032 - val_loss: 0.0117\n",
            "Epoch 56/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0032 - val_loss: 0.0117\n",
            "Epoch 57/75\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0032 - val_loss: 0.0117\n",
            "Epoch 58/75\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0031 - val_loss: 0.0117\n",
            "Epoch 59/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0031 - val_loss: 0.0117\n",
            "Epoch 60/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0116\n",
            "Epoch 61/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0117\n",
            "Epoch 62/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0116\n",
            "Epoch 63/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0116\n",
            "Epoch 64/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0116\n",
            "Epoch 65/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0116\n",
            "Epoch 66/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0116\n",
            "Epoch 67/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0115\n",
            "Epoch 68/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0029 - val_loss: 0.0115\n",
            "Epoch 69/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0029 - val_loss: 0.0115\n",
            "Epoch 70/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0115\n",
            "Epoch 71/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0028 - val_loss: 0.0114\n",
            "Epoch 72/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0028 - val_loss: 0.0115\n",
            "Epoch 73/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0028 - val_loss: 0.0114\n",
            "Epoch 74/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0114\n",
            "Epoch 75/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0114\n",
            "6/6 [==============================] - 0s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 1 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 4s 11ms/step - loss: 0.0617 - val_loss: 0.0389\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0160 - val_loss: 0.0272\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0095 - val_loss: 0.0218\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0067 - val_loss: 0.0188\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0053 - val_loss: 0.0171\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0046 - val_loss: 0.0160\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0042 - val_loss: 0.0153\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0039 - val_loss: 0.0150\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0037 - val_loss: 0.0147\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0036 - val_loss: 0.0146\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0035 - val_loss: 0.0144\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0144\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0142\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0141\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0140\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0141\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0138\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0138\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0136\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0028 - val_loss: 0.0137\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0027 - val_loss: 0.0136\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0026 - val_loss: 0.0136\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0026 - val_loss: 0.0135\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0025 - val_loss: 0.0134\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0025 - val_loss: 0.0134\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0024 - val_loss: 0.0133\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0024 - val_loss: 0.0133\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0024 - val_loss: 0.0132\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0023 - val_loss: 0.0131\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0023 - val_loss: 0.0130\n",
            "Epoch 31/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0023 - val_loss: 0.0130\n",
            "Epoch 32/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0022 - val_loss: 0.0129\n",
            "Epoch 33/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0022 - val_loss: 0.0129\n",
            "Epoch 34/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0022 - val_loss: 0.0128\n",
            "Epoch 35/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0021 - val_loss: 0.0127\n",
            "Epoch 36/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0021 - val_loss: 0.0126\n",
            "Epoch 37/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0126\n",
            "Epoch 38/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0125\n",
            "Epoch 39/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0020 - val_loss: 0.0124\n",
            "Epoch 40/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0124\n",
            "Epoch 41/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0123\n",
            "Epoch 42/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0122\n",
            "Epoch 43/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0020 - val_loss: 0.0121\n",
            "Epoch 44/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0120\n",
            "Epoch 45/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0120\n",
            "Epoch 46/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0120\n",
            "Epoch 47/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0119\n",
            "Epoch 48/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0018 - val_loss: 0.0118\n",
            "Epoch 49/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0018 - val_loss: 0.0117\n",
            "Epoch 50/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0018 - val_loss: 0.0117\n",
            "Epoch 51/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0018 - val_loss: 0.0116\n",
            "Epoch 52/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0018 - val_loss: 0.0116\n",
            "Epoch 53/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0017 - val_loss: 0.0115\n",
            "Epoch 54/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0017 - val_loss: 0.0114\n",
            "Epoch 55/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0017 - val_loss: 0.0113\n",
            "Epoch 56/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0017 - val_loss: 0.0113\n",
            "Epoch 57/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0017 - val_loss: 0.0112\n",
            "Epoch 58/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0017 - val_loss: 0.0112\n",
            "Epoch 59/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0016 - val_loss: 0.0110\n",
            "Epoch 60/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0016 - val_loss: 0.0110\n",
            "Epoch 61/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0016 - val_loss: 0.0110\n",
            "Epoch 62/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0016 - val_loss: 0.0110\n",
            "Epoch 63/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0016 - val_loss: 0.0108\n",
            "Epoch 64/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0016 - val_loss: 0.0108\n",
            "Epoch 65/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0016 - val_loss: 0.0107\n",
            "Epoch 66/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0015 - val_loss: 0.0107\n",
            "Epoch 67/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0015 - val_loss: 0.0105\n",
            "Epoch 68/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0015 - val_loss: 0.0105\n",
            "Epoch 69/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0015 - val_loss: 0.0105\n",
            "Epoch 70/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0015 - val_loss: 0.0104\n",
            "Epoch 71/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0015 - val_loss: 0.0104\n",
            "Epoch 72/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0014 - val_loss: 0.0103\n",
            "Epoch 73/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0014 - val_loss: 0.0103\n",
            "Epoch 74/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0014 - val_loss: 0.0101\n",
            "Epoch 75/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0014 - val_loss: 0.0101\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 2 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 5s 13ms/step - loss: 0.0307 - val_loss: 0.0160\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0071 - val_loss: 0.0154\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0061 - val_loss: 0.0150\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0055 - val_loss: 0.0147\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0051 - val_loss: 0.0145\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0048 - val_loss: 0.0144\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0045 - val_loss: 0.0143\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0042 - val_loss: 0.0143\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0040 - val_loss: 0.0142\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0037 - val_loss: 0.0143\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0035 - val_loss: 0.0141\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0141\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0140\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0140\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0137\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0136\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0135\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0133\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0025 - val_loss: 0.0131\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0024 - val_loss: 0.0130\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0023 - val_loss: 0.0129\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0023 - val_loss: 0.0127\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0022 - val_loss: 0.0127\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0021 - val_loss: 0.0125\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0124\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0123\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0122\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0119\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0018 - val_loss: 0.0118\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0018 - val_loss: 0.0117\n",
            "Epoch 31/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0017 - val_loss: 0.0116\n",
            "Epoch 32/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0017 - val_loss: 0.0115\n",
            "Epoch 33/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0017 - val_loss: 0.0114\n",
            "Epoch 34/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0016 - val_loss: 0.0113\n",
            "Epoch 35/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0016 - val_loss: 0.0111\n",
            "Epoch 36/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0016 - val_loss: 0.0111\n",
            "Epoch 37/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0016 - val_loss: 0.0110\n",
            "Epoch 38/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0015 - val_loss: 0.0108\n",
            "Epoch 39/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0015 - val_loss: 0.0107\n",
            "Epoch 40/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0015 - val_loss: 0.0107\n",
            "Epoch 41/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0015 - val_loss: 0.0105\n",
            "Epoch 42/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0014 - val_loss: 0.0104\n",
            "Epoch 43/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0014 - val_loss: 0.0104\n",
            "Epoch 44/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0014 - val_loss: 0.0102\n",
            "Epoch 45/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0014 - val_loss: 0.0102\n",
            "Epoch 46/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0014 - val_loss: 0.0100\n",
            "Epoch 47/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0013 - val_loss: 0.0100\n",
            "Epoch 48/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0013 - val_loss: 0.0099\n",
            "Epoch 49/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0013 - val_loss: 0.0098\n",
            "Epoch 50/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0013 - val_loss: 0.0097\n",
            "Epoch 51/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0013 - val_loss: 0.0097\n",
            "Epoch 52/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0013 - val_loss: 0.0096\n",
            "Epoch 53/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0013 - val_loss: 0.0094\n",
            "Epoch 54/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0012 - val_loss: 0.0094\n",
            "Epoch 55/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0012 - val_loss: 0.0093\n",
            "Epoch 56/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0012 - val_loss: 0.0092\n",
            "Epoch 57/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0012 - val_loss: 0.0092\n",
            "Epoch 58/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0012 - val_loss: 0.0091\n",
            "Epoch 59/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0012 - val_loss: 0.0090\n",
            "Epoch 60/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0012 - val_loss: 0.0090\n",
            "Epoch 61/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0012 - val_loss: 0.0088\n",
            "Epoch 62/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0011 - val_loss: 0.0088\n",
            "Epoch 63/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0011 - val_loss: 0.0087\n",
            "Epoch 64/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0011 - val_loss: 0.0086\n",
            "Epoch 65/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0011 - val_loss: 0.0085\n",
            "Epoch 66/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0011 - val_loss: 0.0085\n",
            "Epoch 67/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0011 - val_loss: 0.0084\n",
            "Epoch 68/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0011 - val_loss: 0.0083\n",
            "Epoch 69/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0011 - val_loss: 0.0082\n",
            "Epoch 70/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0010 - val_loss: 0.0082\n",
            "Epoch 71/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0010 - val_loss: 0.0081\n",
            "Epoch 72/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0010 - val_loss: 0.0080\n",
            "Epoch 73/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0010 - val_loss: 0.0080\n",
            "Epoch 74/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0010 - val_loss: 0.0079\n",
            "Epoch 75/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 9.9706e-04 - val_loss: 0.0079\n",
            "6/6 [==============================] - 0s 6ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 3 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 4s 10ms/step - loss: 0.0978 - val_loss: 0.0384\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0250 - val_loss: 0.0215\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0149 - val_loss: 0.0177\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0114 - val_loss: 0.0155\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0092 - val_loss: 0.0140\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0079 - val_loss: 0.0131\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0069 - val_loss: 0.0126\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0063 - val_loss: 0.0122\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0058 - val_loss: 0.0119\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0054 - val_loss: 0.0116\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0051 - val_loss: 0.0115\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0049 - val_loss: 0.0114\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0046 - val_loss: 0.0113\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0045 - val_loss: 0.0111\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0043 - val_loss: 0.0111\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0042 - val_loss: 0.0110\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0040 - val_loss: 0.0111\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0039 - val_loss: 0.0111\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0038 - val_loss: 0.0110\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0109\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0111\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0035 - val_loss: 0.0110\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0034 - val_loss: 0.0110\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0033 - val_loss: 0.0110\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0033 - val_loss: 0.0110\n",
            "6/6 [==============================] - 0s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 4 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 4s 10ms/step - loss: 0.2115 - val_loss: 0.1127\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0811 - val_loss: 0.0581\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0410 - val_loss: 0.0368\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0243 - val_loss: 0.0286\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0169 - val_loss: 0.0246\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0128 - val_loss: 0.0220\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0102 - val_loss: 0.0201\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0085 - val_loss: 0.0187\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0074 - val_loss: 0.0177\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0066 - val_loss: 0.0169\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0061 - val_loss: 0.0163\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0056 - val_loss: 0.0158\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0053 - val_loss: 0.0155\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0050 - val_loss: 0.0153\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0048 - val_loss: 0.0151\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0046 - val_loss: 0.0148\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0044 - val_loss: 0.0147\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0043 - val_loss: 0.0147\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0042 - val_loss: 0.0145\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0041 - val_loss: 0.0145\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0040 - val_loss: 0.0144\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0039 - val_loss: 0.0143\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0038 - val_loss: 0.0144\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0038 - val_loss: 0.0144\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0037 - val_loss: 0.0143\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0141\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0142\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0035 - val_loss: 0.0141\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0035 - val_loss: 0.0141\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0034 - val_loss: 0.0141\n",
            "Epoch 31/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0138\n",
            "Epoch 32/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0139\n",
            "Epoch 33/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0139\n",
            "Epoch 34/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0140\n",
            "Epoch 35/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0138\n",
            "Epoch 36/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0137\n",
            "Epoch 37/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0031 - val_loss: 0.0137\n",
            "Epoch 38/75\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0031 - val_loss: 0.0137\n",
            "Epoch 39/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0030 - val_loss: 0.0137\n",
            "Epoch 40/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0030 - val_loss: 0.0137\n",
            "Epoch 41/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0030 - val_loss: 0.0137\n",
            "Epoch 42/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0030 - val_loss: 0.0137\n",
            "Epoch 43/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0136\n",
            "Epoch 44/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0029 - val_loss: 0.0135\n",
            "Epoch 45/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0029 - val_loss: 0.0135\n",
            "Epoch 46/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0028 - val_loss: 0.0136\n",
            "Epoch 47/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0135\n",
            "Epoch 48/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0135\n",
            "Epoch 49/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0134\n",
            "Epoch 50/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0133\n",
            "Epoch 51/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0135\n",
            "Epoch 52/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0027 - val_loss: 0.0134\n",
            "Epoch 53/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0027 - val_loss: 0.0133\n",
            "Epoch 54/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0132\n",
            "Epoch 55/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0026 - val_loss: 0.0133\n",
            "Epoch 56/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0132\n",
            "Epoch 57/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0026 - val_loss: 0.0131\n",
            "Epoch 58/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0130\n",
            "Epoch 59/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0025 - val_loss: 0.0131\n",
            "Epoch 60/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0025 - val_loss: 0.0130\n",
            "Epoch 61/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0129\n",
            "Epoch 62/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0129\n",
            "Epoch 63/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0024 - val_loss: 0.0128\n",
            "Epoch 64/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0024 - val_loss: 0.0127\n",
            "Epoch 65/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0024 - val_loss: 0.0127\n",
            "Epoch 66/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0024 - val_loss: 0.0126\n",
            "Epoch 67/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0024 - val_loss: 0.0126\n",
            "Epoch 68/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0024 - val_loss: 0.0126\n",
            "Epoch 69/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0024 - val_loss: 0.0125\n",
            "Epoch 70/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0023 - val_loss: 0.0124\n",
            "Epoch 71/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0023 - val_loss: 0.0125\n",
            "Epoch 72/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0023 - val_loss: 0.0124\n",
            "Epoch 73/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0023 - val_loss: 0.0124\n",
            "Epoch 74/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0023 - val_loss: 0.0123\n",
            "Epoch 75/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0023 - val_loss: 0.0122\n",
            "6/6 [==============================] - 0s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 5 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 4s 10ms/step - loss: 0.1432 - val_loss: 0.0624\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0397 - val_loss: 0.0325\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0230 - val_loss: 0.0259\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0175 - val_loss: 0.0225\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0141 - val_loss: 0.0199\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0117 - val_loss: 0.0179\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0099 - val_loss: 0.0164\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0086 - val_loss: 0.0152\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0076 - val_loss: 0.0142\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0069 - val_loss: 0.0134\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0063 - val_loss: 0.0128\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0058 - val_loss: 0.0124\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0055 - val_loss: 0.0120\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0052 - val_loss: 0.0117\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0050 - val_loss: 0.0113\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0048 - val_loss: 0.0111\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0046 - val_loss: 0.0110\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0045 - val_loss: 0.0108\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0043 - val_loss: 0.0106\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0042 - val_loss: 0.0105\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0041 - val_loss: 0.0103\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0040 - val_loss: 0.0103\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0039 - val_loss: 0.0101\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0038 - val_loss: 0.0100\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0037 - val_loss: 0.0099\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0036 - val_loss: 0.0097\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0036 - val_loss: 0.0096\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0035 - val_loss: 0.0095\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0034 - val_loss: 0.0094\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0093\n",
            "Epoch 31/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0092\n",
            "Epoch 32/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0092\n",
            "Epoch 33/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0091\n",
            "Epoch 34/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0090\n",
            "Epoch 35/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0090\n",
            "Epoch 36/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0089\n",
            "Epoch 37/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0089\n",
            "Epoch 38/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0029 - val_loss: 0.0089\n",
            "Epoch 39/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0029 - val_loss: 0.0088\n",
            "Epoch 40/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0028 - val_loss: 0.0089\n",
            "Epoch 41/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0028 - val_loss: 0.0088\n",
            "Epoch 42/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0088\n",
            "Epoch 43/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0087\n",
            "Epoch 44/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0087\n",
            "Epoch 45/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0087\n",
            "Epoch 46/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0087\n",
            "Epoch 47/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0087\n",
            "Epoch 48/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0086\n",
            "Epoch 49/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0086\n",
            "Epoch 50/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0085\n",
            "Epoch 51/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0025 - val_loss: 0.0085\n",
            "Epoch 52/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0085\n",
            "Epoch 53/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0024 - val_loss: 0.0085\n",
            "Epoch 54/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0024 - val_loss: 0.0085\n",
            "Epoch 55/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0024 - val_loss: 0.0085\n",
            "Epoch 56/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0024 - val_loss: 0.0085\n",
            "Epoch 57/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0023 - val_loss: 0.0084\n",
            "Epoch 58/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0023 - val_loss: 0.0084\n",
            "Epoch 59/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0023 - val_loss: 0.0084\n",
            "Epoch 60/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0023 - val_loss: 0.0084\n",
            "Epoch 61/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0022 - val_loss: 0.0083\n",
            "Epoch 62/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0022 - val_loss: 0.0083\n",
            "Epoch 63/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0022 - val_loss: 0.0083\n",
            "Epoch 64/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0022 - val_loss: 0.0082\n",
            "Epoch 65/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0022 - val_loss: 0.0082\n",
            "Epoch 66/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0082\n",
            "Epoch 67/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0082\n",
            "Epoch 68/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0082\n",
            "Epoch 69/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0082\n",
            "Epoch 70/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0021 - val_loss: 0.0081\n",
            "Epoch 71/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0021 - val_loss: 0.0081\n",
            "Epoch 72/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0020 - val_loss: 0.0081\n",
            "Epoch 73/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0020 - val_loss: 0.0081\n",
            "Epoch 74/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0020 - val_loss: 0.0081\n",
            "Epoch 75/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0020 - val_loss: 0.0080\n",
            "6/6 [==============================] - 0s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 6 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 4s 10ms/step - loss: 0.2046 - val_loss: 0.1229\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0828 - val_loss: 0.0640\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0406 - val_loss: 0.0418\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0243 - val_loss: 0.0334\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0180 - val_loss: 0.0298\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0149 - val_loss: 0.0276\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0130 - val_loss: 0.0262\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0117 - val_loss: 0.0250\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0106 - val_loss: 0.0241\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0098 - val_loss: 0.0233\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0091 - val_loss: 0.0227\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0086 - val_loss: 0.0222\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0081 - val_loss: 0.0217\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0077 - val_loss: 0.0214\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0074 - val_loss: 0.0210\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0072 - val_loss: 0.0207\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0069 - val_loss: 0.0205\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0067 - val_loss: 0.0202\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0066 - val_loss: 0.0200\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0064 - val_loss: 0.0198\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0063 - val_loss: 0.0197\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0061 - val_loss: 0.0195\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0060 - val_loss: 0.0194\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0059 - val_loss: 0.0192\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0058 - val_loss: 0.0192\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0057 - val_loss: 0.0191\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0056 - val_loss: 0.0189\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0056 - val_loss: 0.0188\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0055 - val_loss: 0.0188\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0054 - val_loss: 0.0186\n",
            "Epoch 31/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0053 - val_loss: 0.0186\n",
            "Epoch 32/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0053 - val_loss: 0.0185\n",
            "Epoch 33/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0052 - val_loss: 0.0184\n",
            "Epoch 34/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0052 - val_loss: 0.0183\n",
            "Epoch 35/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0051 - val_loss: 0.0182\n",
            "Epoch 36/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0050 - val_loss: 0.0182\n",
            "Epoch 37/75\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0050 - val_loss: 0.0181\n",
            "Epoch 38/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0049 - val_loss: 0.0181\n",
            "Epoch 39/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0049 - val_loss: 0.0181\n",
            "Epoch 40/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0048 - val_loss: 0.0179\n",
            "Epoch 41/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0048 - val_loss: 0.0179\n",
            "Epoch 42/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0047 - val_loss: 0.0178\n",
            "Epoch 43/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0047 - val_loss: 0.0177\n",
            "Epoch 44/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0046 - val_loss: 0.0176\n",
            "Epoch 45/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0046 - val_loss: 0.0176\n",
            "Epoch 46/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0045 - val_loss: 0.0175\n",
            "Epoch 47/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0045 - val_loss: 0.0174\n",
            "Epoch 48/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0045 - val_loss: 0.0174\n",
            "Epoch 49/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0044 - val_loss: 0.0173\n",
            "Epoch 50/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0044 - val_loss: 0.0173\n",
            "Epoch 51/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0043 - val_loss: 0.0173\n",
            "Epoch 52/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0043 - val_loss: 0.0172\n",
            "Epoch 53/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0043 - val_loss: 0.0171\n",
            "Epoch 54/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0042 - val_loss: 0.0170\n",
            "Epoch 55/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0042 - val_loss: 0.0170\n",
            "Epoch 56/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0041 - val_loss: 0.0170\n",
            "Epoch 57/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0041 - val_loss: 0.0170\n",
            "Epoch 58/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0041 - val_loss: 0.0169\n",
            "Epoch 59/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0040 - val_loss: 0.0169\n",
            "Epoch 60/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0040 - val_loss: 0.0168\n",
            "Epoch 61/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0040 - val_loss: 0.0168\n",
            "Epoch 62/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0040 - val_loss: 0.0167\n",
            "Epoch 63/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0039 - val_loss: 0.0166\n",
            "Epoch 64/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0039 - val_loss: 0.0166\n",
            "Epoch 65/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0039 - val_loss: 0.0166\n",
            "Epoch 66/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0038 - val_loss: 0.0165\n",
            "Epoch 67/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0038 - val_loss: 0.0165\n",
            "Epoch 68/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0038 - val_loss: 0.0165\n",
            "Epoch 69/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0038 - val_loss: 0.0164\n",
            "Epoch 70/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0038 - val_loss: 0.0164\n",
            "Epoch 71/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0163\n",
            "Epoch 72/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0037 - val_loss: 0.0164\n",
            "Epoch 73/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0037 - val_loss: 0.0163\n",
            "Epoch 74/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0162\n",
            "Epoch 75/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0162\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 7 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 5s 13ms/step - loss: 0.0878 - val_loss: 0.0450\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0275 - val_loss: 0.0281\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0172 - val_loss: 0.0217\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0126 - val_loss: 0.0185\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0102 - val_loss: 0.0167\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0087 - val_loss: 0.0155\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0078 - val_loss: 0.0148\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0073 - val_loss: 0.0143\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0069 - val_loss: 0.0139\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0066 - val_loss: 0.0137\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0063 - val_loss: 0.0136\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0061 - val_loss: 0.0134\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0059 - val_loss: 0.0133\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0058 - val_loss: 0.0132\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0056 - val_loss: 0.0130\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0055 - val_loss: 0.0130\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0053 - val_loss: 0.0128\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0052 - val_loss: 0.0127\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0051 - val_loss: 0.0127\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0049 - val_loss: 0.0126\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0048 - val_loss: 0.0125\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0046 - val_loss: 0.0125\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0045 - val_loss: 0.0124\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0044 - val_loss: 0.0123\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0043 - val_loss: 0.0123\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0042 - val_loss: 0.0122\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0041 - val_loss: 0.0121\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0040 - val_loss: 0.0121\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0039 - val_loss: 0.0120\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0039 - val_loss: 0.0120\n",
            "Epoch 31/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0038 - val_loss: 0.0119\n",
            "Epoch 32/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0118\n",
            "Epoch 33/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0037 - val_loss: 0.0118\n",
            "Epoch 34/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0117\n",
            "Epoch 35/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0035 - val_loss: 0.0117\n",
            "Epoch 36/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0035 - val_loss: 0.0116\n",
            "Epoch 37/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0034 - val_loss: 0.0115\n",
            "Epoch 38/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0034 - val_loss: 0.0115\n",
            "Epoch 39/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0033 - val_loss: 0.0114\n",
            "Epoch 40/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0113\n",
            "Epoch 41/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0032 - val_loss: 0.0113\n",
            "Epoch 42/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0112\n",
            "Epoch 43/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0111\n",
            "Epoch 44/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0111\n",
            "Epoch 45/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0110\n",
            "Epoch 46/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0110\n",
            "Epoch 47/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0109\n",
            "Epoch 48/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0108\n",
            "Epoch 49/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0107\n",
            "Epoch 50/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0107\n",
            "Epoch 51/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0106\n",
            "Epoch 52/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0028 - val_loss: 0.0106\n",
            "Epoch 53/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0105\n",
            "Epoch 54/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0104\n",
            "Epoch 55/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0104\n",
            "Epoch 56/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0027 - val_loss: 0.0103\n",
            "Epoch 57/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0027 - val_loss: 0.0103\n",
            "Epoch 58/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0102\n",
            "Epoch 59/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0101\n",
            "Epoch 60/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0101\n",
            "Epoch 61/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0100\n",
            "Epoch 62/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0100\n",
            "Epoch 63/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0099\n",
            "Epoch 64/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0099\n",
            "Epoch 65/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0025 - val_loss: 0.0098\n",
            "Epoch 66/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0098\n",
            "Epoch 67/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0024 - val_loss: 0.0097\n",
            "Epoch 68/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0024 - val_loss: 0.0096\n",
            "Epoch 69/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0024 - val_loss: 0.0096\n",
            "Epoch 70/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0024 - val_loss: 0.0095\n",
            "Epoch 71/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0023 - val_loss: 0.0095\n",
            "Epoch 72/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0023 - val_loss: 0.0094\n",
            "Epoch 73/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0023 - val_loss: 0.0093\n",
            "Epoch 74/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0023 - val_loss: 0.0093\n",
            "Epoch 75/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0023 - val_loss: 0.0092\n",
            "6/6 [==============================] - 0s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 8 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 4s 10ms/step - loss: 0.1054 - val_loss: 0.0411\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0206 - val_loss: 0.0246\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0126 - val_loss: 0.0211\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0100 - val_loss: 0.0189\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0083 - val_loss: 0.0172\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0071 - val_loss: 0.0159\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0063 - val_loss: 0.0150\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 2s 12ms/step - loss: 0.0057 - val_loss: 0.0144\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0053 - val_loss: 0.0139\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0049 - val_loss: 0.0135\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0047 - val_loss: 0.0132\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0045 - val_loss: 0.0129\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0043 - val_loss: 0.0128\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0042 - val_loss: 0.0126\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0041 - val_loss: 0.0125\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0040 - val_loss: 0.0124\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0039 - val_loss: 0.0123\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0038 - val_loss: 0.0122\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0121\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0121\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0120\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0035 - val_loss: 0.0120\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0035 - val_loss: 0.0120\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0034 - val_loss: 0.0119\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0034 - val_loss: 0.0119\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0033 - val_loss: 0.0118\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0033 - val_loss: 0.0119\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0118\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0117\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0117\n",
            "Epoch 31/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0117\n",
            "Epoch 32/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0116\n",
            "Epoch 33/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0116\n",
            "Epoch 34/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0116\n",
            "Epoch 35/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0116\n",
            "Epoch 36/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0115\n",
            "Epoch 37/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0115\n",
            "Epoch 38/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0115\n",
            "Epoch 39/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0114\n",
            "Epoch 40/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0114\n",
            "Epoch 41/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0028 - val_loss: 0.0114\n",
            "Epoch 42/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0113\n",
            "Epoch 43/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0112\n",
            "Epoch 44/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0112\n",
            "Epoch 45/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0112\n",
            "Epoch 46/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0026 - val_loss: 0.0112\n",
            "Epoch 47/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0026 - val_loss: 0.0111\n",
            "Epoch 48/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0111\n",
            "Epoch 49/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0110\n",
            "Epoch 50/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0110\n",
            "Epoch 51/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0025 - val_loss: 0.0109\n",
            "Epoch 52/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0025 - val_loss: 0.0109\n",
            "Epoch 53/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0024 - val_loss: 0.0109\n",
            "Epoch 54/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0024 - val_loss: 0.0109\n",
            "Epoch 55/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0024 - val_loss: 0.0108\n",
            "Epoch 56/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0024 - val_loss: 0.0107\n",
            "Epoch 57/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0024 - val_loss: 0.0107\n",
            "Epoch 58/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0023 - val_loss: 0.0107\n",
            "Epoch 59/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0023 - val_loss: 0.0107\n",
            "Epoch 60/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0023 - val_loss: 0.0106\n",
            "Epoch 61/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0023 - val_loss: 0.0106\n",
            "Epoch 62/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0023 - val_loss: 0.0105\n",
            "Epoch 63/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0023 - val_loss: 0.0104\n",
            "Epoch 64/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0022 - val_loss: 0.0104\n",
            "Epoch 65/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0022 - val_loss: 0.0104\n",
            "Epoch 66/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0022 - val_loss: 0.0104\n",
            "Epoch 67/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0022 - val_loss: 0.0104\n",
            "Epoch 68/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0103\n",
            "Epoch 69/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0102\n",
            "Epoch 70/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0103\n",
            "Epoch 71/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0102\n",
            "Epoch 72/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0101\n",
            "Epoch 73/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0101\n",
            "Epoch 74/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0100\n",
            "Epoch 75/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0100\n",
            "6/6 [==============================] - 1s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 9 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 5s 12ms/step - loss: 0.0798 - val_loss: 0.0260\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0123 - val_loss: 0.0158\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0072 - val_loss: 0.0148\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0062 - val_loss: 0.0143\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0056 - val_loss: 0.0140\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0052 - val_loss: 0.0137\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0050 - val_loss: 0.0136\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0048 - val_loss: 0.0134\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0047 - val_loss: 0.0135\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0046 - val_loss: 0.0134\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0045 - val_loss: 0.0133\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0044 - val_loss: 0.0134\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0043 - val_loss: 0.0133\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0042 - val_loss: 0.0134\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0042 - val_loss: 0.0134\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0041 - val_loss: 0.0133\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0040 - val_loss: 0.0134\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0134\n",
            "6/6 [==============================] - 0s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 0 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 3s 13ms/step - loss: 0.1554 - val_loss: 0.1106\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0724 - val_loss: 0.0670\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0426 - val_loss: 0.0482\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0286 - val_loss: 0.0379\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0203 - val_loss: 0.0313\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0151 - val_loss: 0.0264\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0116 - val_loss: 0.0229\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0093 - val_loss: 0.0203\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0077 - val_loss: 0.0184\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0066 - val_loss: 0.0168\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0058 - val_loss: 0.0157\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0051 - val_loss: 0.0150\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0047 - val_loss: 0.0144\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0044 - val_loss: 0.0138\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0041 - val_loss: 0.0134\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0039 - val_loss: 0.0131\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0037 - val_loss: 0.0128\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0036 - val_loss: 0.0126\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0034 - val_loss: 0.0123\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0033 - val_loss: 0.0121\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0033 - val_loss: 0.0120\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0032 - val_loss: 0.0119\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0031 - val_loss: 0.0117\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0117\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0115\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0029 - val_loss: 0.0115\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0114\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0113\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0113\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0027 - val_loss: 0.0113\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0112\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0112\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0111\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0111\n",
            "Epoch 35/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0110\n",
            "Epoch 36/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0110\n",
            "Epoch 37/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0025 - val_loss: 0.0110\n",
            "Epoch 38/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0025 - val_loss: 0.0109\n",
            "Epoch 39/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0024 - val_loss: 0.0108\n",
            "Epoch 40/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0024 - val_loss: 0.0110\n",
            "Epoch 41/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0024 - val_loss: 0.0109\n",
            "Epoch 42/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0024 - val_loss: 0.0108\n",
            "Epoch 43/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0023 - val_loss: 0.0107\n",
            "Epoch 44/75\n",
            "86/86 [==============================] - 1s 11ms/step - loss: 0.0023 - val_loss: 0.0108\n",
            "Epoch 45/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0023 - val_loss: 0.0107\n",
            "Epoch 46/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0022 - val_loss: 0.0106\n",
            "Epoch 47/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0022 - val_loss: 0.0105\n",
            "Epoch 48/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0022 - val_loss: 0.0105\n",
            "Epoch 49/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0021 - val_loss: 0.0105\n",
            "Epoch 50/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0021 - val_loss: 0.0104\n",
            "Epoch 51/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0021 - val_loss: 0.0105\n",
            "Epoch 52/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0021 - val_loss: 0.0103\n",
            "Epoch 53/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0021 - val_loss: 0.0104\n",
            "Epoch 54/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0103\n",
            "Epoch 55/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0103\n",
            "Epoch 56/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0102\n",
            "Epoch 57/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0102\n",
            "Epoch 58/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0101\n",
            "Epoch 59/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0100\n",
            "Epoch 60/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0019 - val_loss: 0.0101\n",
            "Epoch 61/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0100\n",
            "Epoch 62/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0098\n",
            "Epoch 63/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0098\n",
            "Epoch 64/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0098\n",
            "Epoch 65/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0098\n",
            "Epoch 66/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0098\n",
            "Epoch 67/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0097\n",
            "Epoch 68/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0018 - val_loss: 0.0097\n",
            "Epoch 69/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0018 - val_loss: 0.0095\n",
            "Epoch 70/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0018 - val_loss: 0.0096\n",
            "Epoch 71/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0018 - val_loss: 0.0095\n",
            "Epoch 72/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0018 - val_loss: 0.0095\n",
            "Epoch 73/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0018 - val_loss: 0.0094\n",
            "Epoch 74/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0018 - val_loss: 0.0093\n",
            "Epoch 75/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0018 - val_loss: 0.0093\n",
            "6/6 [==============================] - 0s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 1 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 3s 16ms/step - loss: 0.1373 - val_loss: 0.0517\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0361 - val_loss: 0.0282\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0213 - val_loss: 0.0234\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0165 - val_loss: 0.0208\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0136 - val_loss: 0.0191\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0116 - val_loss: 0.0179\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0101 - val_loss: 0.0168\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0090 - val_loss: 0.0164\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0082 - val_loss: 0.0160\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0076 - val_loss: 0.0157\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0072 - val_loss: 0.0154\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0068 - val_loss: 0.0152\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0065 - val_loss: 0.0152\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0062 - val_loss: 0.0152\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0060 - val_loss: 0.0152\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0058 - val_loss: 0.0151\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0056 - val_loss: 0.0150\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0055 - val_loss: 0.0149\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0053 - val_loss: 0.0149\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0052 - val_loss: 0.0150\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0051 - val_loss: 0.0150\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0050 - val_loss: 0.0150\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0049 - val_loss: 0.0149\n",
            "6/6 [==============================] - 0s 6ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 2 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 4s 17ms/step - loss: 0.1324 - val_loss: 0.0541\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0406 - val_loss: 0.0251\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0206 - val_loss: 0.0189\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0145 - val_loss: 0.0172\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0119 - val_loss: 0.0163\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0103 - val_loss: 0.0156\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0091 - val_loss: 0.0150\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0082 - val_loss: 0.0144\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0075 - val_loss: 0.0139\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0070 - val_loss: 0.0136\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0065 - val_loss: 0.0132\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0061 - val_loss: 0.0130\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0058 - val_loss: 0.0128\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0056 - val_loss: 0.0126\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0054 - val_loss: 0.0124\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0052 - val_loss: 0.0123\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0050 - val_loss: 0.0121\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0049 - val_loss: 0.0120\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0047 - val_loss: 0.0119\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0046 - val_loss: 0.0118\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0045 - val_loss: 0.0117\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0044 - val_loss: 0.0116\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0043 - val_loss: 0.0115\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0043 - val_loss: 0.0115\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0042 - val_loss: 0.0114\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0041 - val_loss: 0.0113\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0040 - val_loss: 0.0113\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0040 - val_loss: 0.0112\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0039 - val_loss: 0.0112\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0038 - val_loss: 0.0111\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0038 - val_loss: 0.0111\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0110\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 11ms/step - loss: 0.0037 - val_loss: 0.0111\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0036 - val_loss: 0.0110\n",
            "Epoch 35/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0036 - val_loss: 0.0110\n",
            "Epoch 36/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0035 - val_loss: 0.0109\n",
            "Epoch 37/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0035 - val_loss: 0.0109\n",
            "Epoch 38/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0034 - val_loss: 0.0109\n",
            "Epoch 39/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0034 - val_loss: 0.0109\n",
            "Epoch 40/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0033 - val_loss: 0.0108\n",
            "Epoch 41/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0033 - val_loss: 0.0107\n",
            "Epoch 42/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0033 - val_loss: 0.0107\n",
            "Epoch 43/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0032 - val_loss: 0.0107\n",
            "Epoch 44/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0106\n",
            "Epoch 45/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0107\n",
            "Epoch 46/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0107\n",
            "Epoch 47/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0106\n",
            "Epoch 48/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0106\n",
            "Epoch 49/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0105\n",
            "Epoch 50/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0105\n",
            "Epoch 51/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0105\n",
            "Epoch 52/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0105\n",
            "Epoch 53/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0105\n",
            "Epoch 54/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0105\n",
            "Epoch 55/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0104\n",
            "Epoch 56/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0104\n",
            "Epoch 57/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0104\n",
            "Epoch 58/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0104\n",
            "Epoch 59/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0027 - val_loss: 0.0104\n",
            "Epoch 60/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0027 - val_loss: 0.0104\n",
            "Epoch 61/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0104\n",
            "Epoch 62/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0027 - val_loss: 0.0104\n",
            "Epoch 63/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0026 - val_loss: 0.0103\n",
            "Epoch 64/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0026 - val_loss: 0.0103\n",
            "Epoch 65/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0026 - val_loss: 0.0103\n",
            "Epoch 66/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0026 - val_loss: 0.0103\n",
            "Epoch 67/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0025 - val_loss: 0.0102\n",
            "Epoch 68/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0102\n",
            "Epoch 69/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0102\n",
            "Epoch 70/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0102\n",
            "Epoch 71/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0025 - val_loss: 0.0102\n",
            "Epoch 72/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0024 - val_loss: 0.0101\n",
            "Epoch 73/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0024 - val_loss: 0.0101\n",
            "Epoch 74/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0024 - val_loss: 0.0101\n",
            "Epoch 75/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0024 - val_loss: 0.0101\n",
            "6/6 [==============================] - 1s 6ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 3 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 4s 15ms/step - loss: 0.2784 - val_loss: 0.2236\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.1990 - val_loss: 0.1716\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.1530 - val_loss: 0.1342\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.1187 - val_loss: 0.1052\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0919 - val_loss: 0.0827\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0710 - val_loss: 0.0655\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0550 - val_loss: 0.0524\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0429 - val_loss: 0.0428\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0338 - val_loss: 0.0358\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0272 - val_loss: 0.0307\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0223 - val_loss: 0.0270\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0187 - val_loss: 0.0244\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0160 - val_loss: 0.0226\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0140 - val_loss: 0.0211\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0126 - val_loss: 0.0200\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0114 - val_loss: 0.0190\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0105 - val_loss: 0.0183\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0098 - val_loss: 0.0176\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0092 - val_loss: 0.0170\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0086 - val_loss: 0.0164\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0081 - val_loss: 0.0160\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0077 - val_loss: 0.0156\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0073 - val_loss: 0.0152\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0070 - val_loss: 0.0148\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0067 - val_loss: 0.0144\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0064 - val_loss: 0.0142\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0062 - val_loss: 0.0139\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0060 - val_loss: 0.0136\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0058 - val_loss: 0.0133\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0056 - val_loss: 0.0131\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0054 - val_loss: 0.0128\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0053 - val_loss: 0.0126\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0051 - val_loss: 0.0124\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0050 - val_loss: 0.0121\n",
            "Epoch 35/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0049 - val_loss: 0.0119\n",
            "Epoch 36/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0048 - val_loss: 0.0117\n",
            "Epoch 37/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0047 - val_loss: 0.0115\n",
            "Epoch 38/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0046 - val_loss: 0.0114\n",
            "Epoch 39/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0045 - val_loss: 0.0112\n",
            "Epoch 40/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0044 - val_loss: 0.0111\n",
            "Epoch 41/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0043 - val_loss: 0.0110\n",
            "Epoch 42/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0043 - val_loss: 0.0109\n",
            "Epoch 43/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0042 - val_loss: 0.0107\n",
            "Epoch 44/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0042 - val_loss: 0.0106\n",
            "Epoch 45/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0041 - val_loss: 0.0106\n",
            "Epoch 46/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0040 - val_loss: 0.0105\n",
            "Epoch 47/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0040 - val_loss: 0.0104\n",
            "Epoch 48/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0039 - val_loss: 0.0103\n",
            "Epoch 49/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0039 - val_loss: 0.0102\n",
            "Epoch 50/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0039 - val_loss: 0.0103\n",
            "Epoch 51/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0038 - val_loss: 0.0102\n",
            "Epoch 52/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0038 - val_loss: 0.0101\n",
            "Epoch 53/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0037 - val_loss: 0.0101\n",
            "Epoch 54/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0037 - val_loss: 0.0101\n",
            "Epoch 55/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0037 - val_loss: 0.0100\n",
            "Epoch 56/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0036 - val_loss: 0.0100\n",
            "Epoch 57/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0036 - val_loss: 0.0100\n",
            "Epoch 58/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0099\n",
            "Epoch 59/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0035 - val_loss: 0.0099\n",
            "Epoch 60/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0035 - val_loss: 0.0099\n",
            "Epoch 61/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0035 - val_loss: 0.0098\n",
            "Epoch 62/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0034 - val_loss: 0.0098\n",
            "Epoch 63/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0034 - val_loss: 0.0097\n",
            "Epoch 64/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0034 - val_loss: 0.0097\n",
            "Epoch 65/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0034 - val_loss: 0.0096\n",
            "Epoch 66/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0096\n",
            "Epoch 67/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0095\n",
            "Epoch 68/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0094\n",
            "Epoch 69/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0094\n",
            "Epoch 70/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0094\n",
            "Epoch 71/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0094\n",
            "Epoch 72/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0093\n",
            "Epoch 73/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0094\n",
            "Epoch 74/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0093\n",
            "Epoch 75/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0093\n",
            "6/6 [==============================] - 1s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 4 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 5s 19ms/step - loss: 0.1436 - val_loss: 0.0793\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0473 - val_loss: 0.0394\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0217 - val_loss: 0.0289\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0141 - val_loss: 0.0259\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0114 - val_loss: 0.0246\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0099 - val_loss: 0.0235\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0089 - val_loss: 0.0226\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0081 - val_loss: 0.0218\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0074 - val_loss: 0.0210\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0069 - val_loss: 0.0204\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0065 - val_loss: 0.0198\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0061 - val_loss: 0.0194\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0058 - val_loss: 0.0190\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0056 - val_loss: 0.0185\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0053 - val_loss: 0.0183\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0051 - val_loss: 0.0181\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0050 - val_loss: 0.0179\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0048 - val_loss: 0.0177\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0047 - val_loss: 0.0175\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0046 - val_loss: 0.0174\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0045 - val_loss: 0.0173\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0044 - val_loss: 0.0172\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0043 - val_loss: 0.0171\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0042 - val_loss: 0.0171\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0041 - val_loss: 0.0170\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0041 - val_loss: 0.0170\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0170\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0039 - val_loss: 0.0169\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0039 - val_loss: 0.0169\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0038 - val_loss: 0.0169\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0038 - val_loss: 0.0170\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0037 - val_loss: 0.0169\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0169\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0036 - val_loss: 0.0169\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 5 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 4s 17ms/step - loss: 0.2358 - val_loss: 0.1327\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0815 - val_loss: 0.0644\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0441 - val_loss: 0.0469\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0325 - val_loss: 0.0407\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0271 - val_loss: 0.0375\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0236 - val_loss: 0.0352\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0209 - val_loss: 0.0335\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0188 - val_loss: 0.0321\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0170 - val_loss: 0.0308\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0155 - val_loss: 0.0295\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0141 - val_loss: 0.0282\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0129 - val_loss: 0.0270\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0119 - val_loss: 0.0259\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0110 - val_loss: 0.0250\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0103 - val_loss: 0.0242\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0097 - val_loss: 0.0234\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0091 - val_loss: 0.0228\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0087 - val_loss: 0.0222\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0083 - val_loss: 0.0217\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0079 - val_loss: 0.0212\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0077 - val_loss: 0.0208\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0074 - val_loss: 0.0205\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0072 - val_loss: 0.0202\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0069 - val_loss: 0.0199\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0068 - val_loss: 0.0196\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0066 - val_loss: 0.0195\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0065 - val_loss: 0.0192\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0063 - val_loss: 0.0191\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0062 - val_loss: 0.0189\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0061 - val_loss: 0.0187\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0060 - val_loss: 0.0185\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0059 - val_loss: 0.0185\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0058 - val_loss: 0.0184\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0057 - val_loss: 0.0183\n",
            "Epoch 35/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0056 - val_loss: 0.0182\n",
            "Epoch 36/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0055 - val_loss: 0.0180\n",
            "Epoch 37/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0055 - val_loss: 0.0179\n",
            "Epoch 38/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0054 - val_loss: 0.0179\n",
            "Epoch 39/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0053 - val_loss: 0.0179\n",
            "Epoch 40/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0053 - val_loss: 0.0177\n",
            "Epoch 41/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0052 - val_loss: 0.0177\n",
            "Epoch 42/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0051 - val_loss: 0.0176\n",
            "Epoch 43/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0051 - val_loss: 0.0176\n",
            "Epoch 44/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0050 - val_loss: 0.0175\n",
            "Epoch 45/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0050 - val_loss: 0.0175\n",
            "Epoch 46/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0049 - val_loss: 0.0174\n",
            "Epoch 47/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0049 - val_loss: 0.0173\n",
            "Epoch 48/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0048 - val_loss: 0.0173\n",
            "Epoch 49/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0048 - val_loss: 0.0172\n",
            "Epoch 50/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0047 - val_loss: 0.0173\n",
            "Epoch 51/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0047 - val_loss: 0.0172\n",
            "Epoch 52/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0046 - val_loss: 0.0171\n",
            "Epoch 53/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0046 - val_loss: 0.0171\n",
            "Epoch 54/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0046 - val_loss: 0.0170\n",
            "Epoch 55/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0045 - val_loss: 0.0170\n",
            "Epoch 56/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0045 - val_loss: 0.0170\n",
            "Epoch 57/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0045 - val_loss: 0.0170\n",
            "Epoch 58/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0044 - val_loss: 0.0169\n",
            "Epoch 59/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0044 - val_loss: 0.0169\n",
            "Epoch 60/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0044 - val_loss: 0.0169\n",
            "Epoch 61/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0043 - val_loss: 0.0168\n",
            "Epoch 62/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0043 - val_loss: 0.0168\n",
            "Epoch 63/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0043 - val_loss: 0.0168\n",
            "Epoch 64/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0043 - val_loss: 0.0167\n",
            "Epoch 65/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0042 - val_loss: 0.0167\n",
            "Epoch 66/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0042 - val_loss: 0.0166\n",
            "Epoch 67/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0042 - val_loss: 0.0166\n",
            "Epoch 68/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0041 - val_loss: 0.0166\n",
            "Epoch 69/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0041 - val_loss: 0.0167\n",
            "Epoch 70/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0041 - val_loss: 0.0167\n",
            "Epoch 71/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0041 - val_loss: 0.0166\n",
            "6/6 [==============================] - 0s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 6 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 3s 13ms/step - loss: 0.1904 - val_loss: 0.1369\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.1007 - val_loss: 0.0842\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0579 - val_loss: 0.0522\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0360 - val_loss: 0.0393\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0265 - val_loss: 0.0326\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0207 - val_loss: 0.0277\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0167 - val_loss: 0.0240\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0138 - val_loss: 0.0213\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0116 - val_loss: 0.0192\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 11ms/step - loss: 0.0099 - val_loss: 0.0177\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0086 - val_loss: 0.0165\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0076 - val_loss: 0.0157\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0068 - val_loss: 0.0150\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0062 - val_loss: 0.0145\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0058 - val_loss: 0.0142\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0054 - val_loss: 0.0139\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0051 - val_loss: 0.0136\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0049 - val_loss: 0.0135\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0047 - val_loss: 0.0133\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0045 - val_loss: 0.0132\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0044 - val_loss: 0.0131\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0042 - val_loss: 0.0130\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0041 - val_loss: 0.0130\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0040 - val_loss: 0.0129\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0039 - val_loss: 0.0127\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0039 - val_loss: 0.0127\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0038 - val_loss: 0.0127\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0126\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0125\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0124\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0124\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0035 - val_loss: 0.0123\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0035 - val_loss: 0.0123\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0034 - val_loss: 0.0123\n",
            "Epoch 35/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0034 - val_loss: 0.0122\n",
            "Epoch 36/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0034 - val_loss: 0.0121\n",
            "Epoch 37/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0121\n",
            "Epoch 38/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0121\n",
            "Epoch 39/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0121\n",
            "Epoch 40/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0032 - val_loss: 0.0120\n",
            "Epoch 41/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0119\n",
            "Epoch 42/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0120\n",
            "Epoch 43/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0119\n",
            "Epoch 44/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0119\n",
            "Epoch 45/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0031 - val_loss: 0.0118\n",
            "Epoch 46/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0118\n",
            "Epoch 47/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0031 - val_loss: 0.0117\n",
            "Epoch 48/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0030 - val_loss: 0.0117\n",
            "Epoch 49/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0030 - val_loss: 0.0117\n",
            "Epoch 50/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0030 - val_loss: 0.0117\n",
            "Epoch 51/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0030 - val_loss: 0.0116\n",
            "Epoch 52/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0030 - val_loss: 0.0116\n",
            "Epoch 53/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0029 - val_loss: 0.0116\n",
            "Epoch 54/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0116\n",
            "Epoch 55/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0116\n",
            "Epoch 56/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0029 - val_loss: 0.0115\n",
            "Epoch 57/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0115\n",
            "Epoch 58/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0114\n",
            "Epoch 59/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0114\n",
            "Epoch 60/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0114\n",
            "Epoch 61/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0113\n",
            "Epoch 62/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0113\n",
            "Epoch 63/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0112\n",
            "Epoch 64/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0112\n",
            "Epoch 65/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0112\n",
            "Epoch 66/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0112\n",
            "Epoch 67/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0112\n",
            "Epoch 68/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0111\n",
            "Epoch 69/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0111\n",
            "Epoch 70/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0110\n",
            "Epoch 71/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0110\n",
            "Epoch 72/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0110\n",
            "Epoch 73/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0026 - val_loss: 0.0110\n",
            "Epoch 74/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0026 - val_loss: 0.0109\n",
            "Epoch 75/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0026 - val_loss: 0.0109\n",
            "6/6 [==============================] - 0s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 7 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 4s 17ms/step - loss: 0.1596 - val_loss: 0.0888\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0581 - val_loss: 0.0487\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0305 - val_loss: 0.0334\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0195 - val_loss: 0.0266\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0143 - val_loss: 0.0229\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0113 - val_loss: 0.0206\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0095 - val_loss: 0.0191\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0082 - val_loss: 0.0182\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0073 - val_loss: 0.0174\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0066 - val_loss: 0.0168\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0061 - val_loss: 0.0164\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0057 - val_loss: 0.0160\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0054 - val_loss: 0.0157\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0051 - val_loss: 0.0155\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0048 - val_loss: 0.0153\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0046 - val_loss: 0.0151\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0045 - val_loss: 0.0149\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0043 - val_loss: 0.0148\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0042 - val_loss: 0.0147\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0041 - val_loss: 0.0145\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0040 - val_loss: 0.0144\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0039 - val_loss: 0.0144\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0039 - val_loss: 0.0143\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0038 - val_loss: 0.0143\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0037 - val_loss: 0.0142\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0037 - val_loss: 0.0141\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0036 - val_loss: 0.0141\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0140\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0035 - val_loss: 0.0140\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0035 - val_loss: 0.0139\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0034 - val_loss: 0.0138\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0034 - val_loss: 0.0139\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0033 - val_loss: 0.0138\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0137\n",
            "Epoch 35/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0137\n",
            "Epoch 36/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0136\n",
            "Epoch 37/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0032 - val_loss: 0.0136\n",
            "Epoch 38/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0032 - val_loss: 0.0136\n",
            "Epoch 39/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0031 - val_loss: 0.0136\n",
            "Epoch 40/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0031 - val_loss: 0.0135\n",
            "Epoch 41/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0135\n",
            "Epoch 42/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0031 - val_loss: 0.0135\n",
            "Epoch 43/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0134\n",
            "Epoch 44/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0134\n",
            "Epoch 45/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0133\n",
            "Epoch 46/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0134\n",
            "Epoch 47/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0133\n",
            "Epoch 48/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0132\n",
            "Epoch 49/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0133\n",
            "Epoch 50/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0132\n",
            "Epoch 51/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0131\n",
            "Epoch 52/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0029 - val_loss: 0.0132\n",
            "Epoch 53/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0131\n",
            "Epoch 54/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0028 - val_loss: 0.0131\n",
            "Epoch 55/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0130\n",
            "Epoch 56/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0130\n",
            "Epoch 57/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0028 - val_loss: 0.0130\n",
            "Epoch 58/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0028 - val_loss: 0.0130\n",
            "Epoch 59/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0130\n",
            "Epoch 60/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0027 - val_loss: 0.0129\n",
            "Epoch 61/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0027 - val_loss: 0.0130\n",
            "Epoch 62/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0129\n",
            "Epoch 63/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0027 - val_loss: 0.0128\n",
            "Epoch 64/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0128\n",
            "Epoch 65/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0127\n",
            "Epoch 66/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0026 - val_loss: 0.0127\n",
            "Epoch 67/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0026 - val_loss: 0.0127\n",
            "Epoch 68/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0026 - val_loss: 0.0127\n",
            "Epoch 69/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0026 - val_loss: 0.0126\n",
            "Epoch 70/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0026 - val_loss: 0.0126\n",
            "Epoch 71/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0026 - val_loss: 0.0126\n",
            "Epoch 72/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0026 - val_loss: 0.0126\n",
            "Epoch 73/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0025 - val_loss: 0.0125\n",
            "Epoch 74/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0125\n",
            "Epoch 75/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0124\n",
            "6/6 [==============================] - 1s 3ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 8 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 4s 16ms/step - loss: 0.0980 - val_loss: 0.0389\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0233 - val_loss: 0.0219\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0129 - val_loss: 0.0177\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0094 - val_loss: 0.0156\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0075 - val_loss: 0.0143\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0064 - val_loss: 0.0134\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0056 - val_loss: 0.0130\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0052 - val_loss: 0.0127\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0048 - val_loss: 0.0124\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0045 - val_loss: 0.0122\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0043 - val_loss: 0.0121\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 11ms/step - loss: 0.0042 - val_loss: 0.0121\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0040 - val_loss: 0.0121\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0039 - val_loss: 0.0120\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0038 - val_loss: 0.0120\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0037 - val_loss: 0.0120\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0036 - val_loss: 0.0120\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0035 - val_loss: 0.0121\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0034 - val_loss: 0.0118\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0033 - val_loss: 0.0118\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0033 - val_loss: 0.0118\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0032 - val_loss: 0.0116\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0117\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0117\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0115\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0030 - val_loss: 0.0115\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0115\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0115\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0115\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0115\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0113\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0113\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0113\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0112\n",
            "Epoch 35/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0112\n",
            "Epoch 36/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0026 - val_loss: 0.0112\n",
            "Epoch 37/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0112\n",
            "Epoch 38/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0112\n",
            "Epoch 39/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0025 - val_loss: 0.0110\n",
            "Epoch 40/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0025 - val_loss: 0.0110\n",
            "Epoch 41/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0024 - val_loss: 0.0110\n",
            "Epoch 42/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0024 - val_loss: 0.0109\n",
            "Epoch 43/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0024 - val_loss: 0.0109\n",
            "Epoch 44/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0024 - val_loss: 0.0108\n",
            "Epoch 45/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0023 - val_loss: 0.0107\n",
            "Epoch 46/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0023 - val_loss: 0.0106\n",
            "Epoch 47/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0023 - val_loss: 0.0106\n",
            "Epoch 48/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0023 - val_loss: 0.0105\n",
            "Epoch 49/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0022 - val_loss: 0.0106\n",
            "Epoch 50/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0022 - val_loss: 0.0105\n",
            "Epoch 51/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0022 - val_loss: 0.0103\n",
            "Epoch 52/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0022 - val_loss: 0.0103\n",
            "Epoch 53/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0022 - val_loss: 0.0102\n",
            "Epoch 54/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0102\n",
            "Epoch 55/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0102\n",
            "Epoch 56/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0101\n",
            "Epoch 57/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0100\n",
            "Epoch 58/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0099\n",
            "Epoch 59/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0099\n",
            "Epoch 60/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0098\n",
            "Epoch 61/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0098\n",
            "Epoch 62/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0097\n",
            "Epoch 63/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0097\n",
            "Epoch 64/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0097\n",
            "Epoch 65/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0020 - val_loss: 0.0096\n",
            "Epoch 66/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0020 - val_loss: 0.0096\n",
            "Epoch 67/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0019 - val_loss: 0.0096\n",
            "Epoch 68/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0095\n",
            "Epoch 69/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0094\n",
            "Epoch 70/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0019 - val_loss: 0.0094\n",
            "Epoch 71/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0019 - val_loss: 0.0093\n",
            "Epoch 72/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0019 - val_loss: 0.0093\n",
            "Epoch 73/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0092\n",
            "Epoch 74/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0019 - val_loss: 0.0091\n",
            "Epoch 75/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0018 - val_loss: 0.0092\n",
            "6/6 [==============================] - 1s 6ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 9 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 5s 19ms/step - loss: 0.1671 - val_loss: 0.0742\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0577 - val_loss: 0.0366\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0304 - val_loss: 0.0266\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0211 - val_loss: 0.0234\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0169 - val_loss: 0.0218\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0143 - val_loss: 0.0206\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0124 - val_loss: 0.0196\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0109 - val_loss: 0.0188\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0098 - val_loss: 0.0182\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0089 - val_loss: 0.0179\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0081 - val_loss: 0.0176\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0075 - val_loss: 0.0172\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0070 - val_loss: 0.0170\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0066 - val_loss: 0.0168\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0063 - val_loss: 0.0166\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0060 - val_loss: 0.0163\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0057 - val_loss: 0.0161\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0055 - val_loss: 0.0160\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0054 - val_loss: 0.0158\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0052 - val_loss: 0.0156\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0051 - val_loss: 0.0156\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0049 - val_loss: 0.0154\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0048 - val_loss: 0.0154\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0047 - val_loss: 0.0152\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0046 - val_loss: 0.0151\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0045 - val_loss: 0.0150\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0044 - val_loss: 0.0149\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0043 - val_loss: 0.0149\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0043 - val_loss: 0.0149\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0042 - val_loss: 0.0149\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0041 - val_loss: 0.0149\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0041 - val_loss: 0.0148\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0040 - val_loss: 0.0146\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0039 - val_loss: 0.0146\n",
            "Epoch 35/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0039 - val_loss: 0.0146\n",
            "Epoch 36/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0038 - val_loss: 0.0147\n",
            "Epoch 37/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0038 - val_loss: 0.0146\n",
            "Epoch 38/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0145\n",
            "Epoch 39/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0145\n",
            "Epoch 40/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0036 - val_loss: 0.0145\n",
            "Epoch 41/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0036 - val_loss: 0.0144\n",
            "Epoch 42/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0035 - val_loss: 0.0144\n",
            "Epoch 43/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0035 - val_loss: 0.0145\n",
            "Epoch 44/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0035 - val_loss: 0.0144\n",
            "Epoch 45/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0034 - val_loss: 0.0144\n",
            "Epoch 46/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0034 - val_loss: 0.0143\n",
            "Epoch 47/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0144\n",
            "Epoch 48/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0143\n",
            "Epoch 49/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0143\n",
            "Epoch 50/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0143\n",
            "Epoch 51/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0143\n",
            "Epoch 52/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0032 - val_loss: 0.0143\n",
            "Epoch 53/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0142\n",
            "Epoch 54/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0143\n",
            "Epoch 55/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0031 - val_loss: 0.0142\n",
            "Epoch 56/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0141\n",
            "Epoch 57/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0030 - val_loss: 0.0141\n",
            "Epoch 58/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0030 - val_loss: 0.0140\n",
            "Epoch 59/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0030 - val_loss: 0.0141\n",
            "Epoch 60/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0030 - val_loss: 0.0140\n",
            "Epoch 61/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0029 - val_loss: 0.0140\n",
            "Epoch 62/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0029 - val_loss: 0.0140\n",
            "Epoch 63/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0029 - val_loss: 0.0139\n",
            "Epoch 64/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0029 - val_loss: 0.0140\n",
            "Epoch 65/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0139\n",
            "Epoch 66/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0139\n",
            "Epoch 67/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0028 - val_loss: 0.0139\n",
            "Epoch 68/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0028 - val_loss: 0.0137\n",
            "Epoch 69/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0138\n",
            "Epoch 70/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0138\n",
            "Epoch 71/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0138\n",
            "Epoch 72/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0138\n",
            "Epoch 73/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0137\n",
            "Epoch 74/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0137\n",
            "Epoch 75/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0136\n",
            "6/6 [==============================] - 0s 8ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 0 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 3s 19ms/step - loss: 0.3071 - val_loss: 0.2104\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.1325 - val_loss: 0.1029\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0563 - val_loss: 0.0552\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0276 - val_loss: 0.0369\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0176 - val_loss: 0.0301\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0139 - val_loss: 0.0271\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0121 - val_loss: 0.0255\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0112 - val_loss: 0.0246\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0105 - val_loss: 0.0238\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0100 - val_loss: 0.0233\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0095 - val_loss: 0.0228\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0092 - val_loss: 0.0224\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0088 - val_loss: 0.0221\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0086 - val_loss: 0.0218\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0083 - val_loss: 0.0215\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0081 - val_loss: 0.0213\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0080 - val_loss: 0.0210\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0078 - val_loss: 0.0208\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0077 - val_loss: 0.0206\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0076 - val_loss: 0.0204\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0074 - val_loss: 0.0203\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0073 - val_loss: 0.0201\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0073 - val_loss: 0.0199\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0072 - val_loss: 0.0198\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0071 - val_loss: 0.0196\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0070 - val_loss: 0.0195\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0069 - val_loss: 0.0193\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0069 - val_loss: 0.0192\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0068 - val_loss: 0.0190\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0067 - val_loss: 0.0188\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0066 - val_loss: 0.0186\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0065 - val_loss: 0.0184\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0064 - val_loss: 0.0183\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0063 - val_loss: 0.0181\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0062 - val_loss: 0.0180\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0062 - val_loss: 0.0179\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0061 - val_loss: 0.0177\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0060 - val_loss: 0.0176\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0060 - val_loss: 0.0175\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0059 - val_loss: 0.0174\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0058 - val_loss: 0.0173\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0058 - val_loss: 0.0172\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0057 - val_loss: 0.0171\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0170\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0056 - val_loss: 0.0170\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0055 - val_loss: 0.0169\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0055 - val_loss: 0.0168\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0054 - val_loss: 0.0167\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0054 - val_loss: 0.0167\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0053 - val_loss: 0.0166\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0053 - val_loss: 0.0166\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0053 - val_loss: 0.0165\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0052 - val_loss: 0.0165\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0052 - val_loss: 0.0164\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0051 - val_loss: 0.0163\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0051 - val_loss: 0.0163\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0051 - val_loss: 0.0163\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0050 - val_loss: 0.0162\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0050 - val_loss: 0.0162\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0049 - val_loss: 0.0161\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0049 - val_loss: 0.0161\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0049 - val_loss: 0.0161\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0048 - val_loss: 0.0160\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0048 - val_loss: 0.0160\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0048 - val_loss: 0.0159\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0047 - val_loss: 0.0159\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0047 - val_loss: 0.0159\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0047 - val_loss: 0.0158\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0047 - val_loss: 0.0158\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0046 - val_loss: 0.0158\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0046 - val_loss: 0.0158\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0046 - val_loss: 0.0157\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0045 - val_loss: 0.0157\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0045 - val_loss: 0.0157\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0045 - val_loss: 0.0156\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 1 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 4s 29ms/step - loss: 0.2024 - val_loss: 0.0925\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0781 - val_loss: 0.0416\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0419 - val_loss: 0.0261\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0279 - val_loss: 0.0211\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0222 - val_loss: 0.0198\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0194 - val_loss: 0.0194\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0178 - val_loss: 0.0192\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0166 - val_loss: 0.0191\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0157 - val_loss: 0.0190\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0150 - val_loss: 0.0190\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0144 - val_loss: 0.0188\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0139 - val_loss: 0.0188\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0134 - val_loss: 0.0187\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0130 - val_loss: 0.0186\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0126 - val_loss: 0.0185\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0122 - val_loss: 0.0184\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0119 - val_loss: 0.0183\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0116 - val_loss: 0.0181\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0113 - val_loss: 0.0180\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0111 - val_loss: 0.0179\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0108 - val_loss: 0.0178\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0106 - val_loss: 0.0177\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0104 - val_loss: 0.0176\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0102 - val_loss: 0.0175\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0100 - val_loss: 0.0174\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0098 - val_loss: 0.0173\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0097 - val_loss: 0.0172\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0095 - val_loss: 0.0172\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0094 - val_loss: 0.0171\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0092 - val_loss: 0.0170\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0091 - val_loss: 0.0170\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0090 - val_loss: 0.0169\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0088 - val_loss: 0.0168\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0087 - val_loss: 0.0167\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0086 - val_loss: 0.0167\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0085 - val_loss: 0.0166\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0084 - val_loss: 0.0165\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0083 - val_loss: 0.0164\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0081 - val_loss: 0.0164\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0080 - val_loss: 0.0163\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0079 - val_loss: 0.0162\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0078 - val_loss: 0.0162\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0077 - val_loss: 0.0161\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0077 - val_loss: 0.0161\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0076 - val_loss: 0.0160\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0075 - val_loss: 0.0160\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0074 - val_loss: 0.0160\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0073 - val_loss: 0.0159\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0072 - val_loss: 0.0159\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0071 - val_loss: 0.0158\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0070 - val_loss: 0.0158\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0070 - val_loss: 0.0157\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0069 - val_loss: 0.0157\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0068 - val_loss: 0.0157\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0067 - val_loss: 0.0156\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0067 - val_loss: 0.0156\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0066 - val_loss: 0.0156\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0065 - val_loss: 0.0155\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0065 - val_loss: 0.0155\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0064 - val_loss: 0.0154\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0063 - val_loss: 0.0154\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0063 - val_loss: 0.0154\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0062 - val_loss: 0.0153\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0061 - val_loss: 0.0153\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0061 - val_loss: 0.0153\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0060 - val_loss: 0.0152\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0060 - val_loss: 0.0152\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0059 - val_loss: 0.0152\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0059 - val_loss: 0.0151\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0058 - val_loss: 0.0151\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0058 - val_loss: 0.0151\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0057 - val_loss: 0.0150\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0057 - val_loss: 0.0150\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0056 - val_loss: 0.0150\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0056 - val_loss: 0.0149\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 2 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 4s 28ms/step - loss: 0.3294 - val_loss: 0.2759\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.2366 - val_loss: 0.2166\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.1811 - val_loss: 0.1702\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.1341 - val_loss: 0.1289\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0945 - val_loss: 0.0970\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0690 - val_loss: 0.0779\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0529 - val_loss: 0.0660\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0424 - val_loss: 0.0582\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0350 - val_loss: 0.0526\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0296 - val_loss: 0.0483\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0253 - val_loss: 0.0451\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0221 - val_loss: 0.0424\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0195 - val_loss: 0.0401\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0175 - val_loss: 0.0380\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0158 - val_loss: 0.0360\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0143 - val_loss: 0.0342\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0130 - val_loss: 0.0325\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0119 - val_loss: 0.0310\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0109 - val_loss: 0.0297\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0101 - val_loss: 0.0285\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0093 - val_loss: 0.0274\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0087 - val_loss: 0.0264\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0082 - val_loss: 0.0256\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0077 - val_loss: 0.0248\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0073 - val_loss: 0.0241\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0069 - val_loss: 0.0235\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0066 - val_loss: 0.0229\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0063 - val_loss: 0.0224\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0061 - val_loss: 0.0219\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0057 - val_loss: 0.0212\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0055 - val_loss: 0.0208\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0053 - val_loss: 0.0205\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0052 - val_loss: 0.0203\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0051 - val_loss: 0.0201\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0050 - val_loss: 0.0199\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0049 - val_loss: 0.0196\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0048 - val_loss: 0.0195\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0047 - val_loss: 0.0193\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0046 - val_loss: 0.0192\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0046 - val_loss: 0.0190\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0045 - val_loss: 0.0189\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0044 - val_loss: 0.0188\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0044 - val_loss: 0.0186\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0043 - val_loss: 0.0186\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0043 - val_loss: 0.0185\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0042 - val_loss: 0.0184\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0042 - val_loss: 0.0183\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0042 - val_loss: 0.0183\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0041 - val_loss: 0.0182\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0041 - val_loss: 0.0182\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0041 - val_loss: 0.0181\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0040 - val_loss: 0.0181\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0040 - val_loss: 0.0180\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0040 - val_loss: 0.0180\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0039 - val_loss: 0.0180\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0039 - val_loss: 0.0179\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0039 - val_loss: 0.0179\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0039 - val_loss: 0.0178\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0038 - val_loss: 0.0178\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0038 - val_loss: 0.0178\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0038 - val_loss: 0.0178\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0038 - val_loss: 0.0177\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0037 - val_loss: 0.0177\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0037 - val_loss: 0.0177\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0037 - val_loss: 0.0177\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0037 - val_loss: 0.0176\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0037 - val_loss: 0.0176\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0036 - val_loss: 0.0176\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0036 - val_loss: 0.0176\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0036 - val_loss: 0.0175\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0036 - val_loss: 0.0175\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0036 - val_loss: 0.0175\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0036 - val_loss: 0.0174\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0035 - val_loss: 0.0174\n",
            "6/6 [==============================] - 0s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 3 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 3s 30ms/step - loss: 0.1860 - val_loss: 0.1615\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.1182 - val_loss: 0.1171\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0827 - val_loss: 0.0895\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 12ms/step - loss: 0.0602 - val_loss: 0.0706\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0444 - val_loss: 0.0574\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0337 - val_loss: 0.0486\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0267 - val_loss: 0.0427\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0221 - val_loss: 0.0387\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0191 - val_loss: 0.0358\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0169 - val_loss: 0.0336\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0153 - val_loss: 0.0318\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0141 - val_loss: 0.0303\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0130 - val_loss: 0.0289\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0121 - val_loss: 0.0277\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0114 - val_loss: 0.0266\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0107 - val_loss: 0.0256\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0100 - val_loss: 0.0247\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0094 - val_loss: 0.0238\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0089 - val_loss: 0.0230\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0085 - val_loss: 0.0223\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0081 - val_loss: 0.0217\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0077 - val_loss: 0.0211\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0074 - val_loss: 0.0205\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0071 - val_loss: 0.0200\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0069 - val_loss: 0.0195\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0067 - val_loss: 0.0191\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0065 - val_loss: 0.0187\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0063 - val_loss: 0.0184\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0061 - val_loss: 0.0181\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0060 - val_loss: 0.0178\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0058 - val_loss: 0.0175\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0057 - val_loss: 0.0172\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0056 - val_loss: 0.0170\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0055 - val_loss: 0.0167\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0054 - val_loss: 0.0165\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0054 - val_loss: 0.0164\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0053 - val_loss: 0.0162\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0052 - val_loss: 0.0160\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0051 - val_loss: 0.0159\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0051 - val_loss: 0.0157\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0050 - val_loss: 0.0156\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0050 - val_loss: 0.0155\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0049 - val_loss: 0.0154\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0049 - val_loss: 0.0153\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0048 - val_loss: 0.0151\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0048 - val_loss: 0.0150\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0047 - val_loss: 0.0150\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0047 - val_loss: 0.0149\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0046 - val_loss: 0.0148\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0046 - val_loss: 0.0147\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0046 - val_loss: 0.0146\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0045 - val_loss: 0.0146\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0045 - val_loss: 0.0145\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0045 - val_loss: 0.0144\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0044 - val_loss: 0.0144\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0044 - val_loss: 0.0143\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0044 - val_loss: 0.0143\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0044 - val_loss: 0.0142\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0043 - val_loss: 0.0142\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0043 - val_loss: 0.0141\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0043 - val_loss: 0.0141\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0043 - val_loss: 0.0140\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0042 - val_loss: 0.0140\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0042 - val_loss: 0.0139\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0042 - val_loss: 0.0139\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0042 - val_loss: 0.0139\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0042 - val_loss: 0.0138\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0041 - val_loss: 0.0138\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0041 - val_loss: 0.0138\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0041 - val_loss: 0.0137\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0041 - val_loss: 0.0137\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0041 - val_loss: 0.0137\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0040 - val_loss: 0.0136\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0040 - val_loss: 0.0136\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0040 - val_loss: 0.0136\n",
            "6/6 [==============================] - 1s 7ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 4 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 4s 28ms/step - loss: 0.2231 - val_loss: 0.1743\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.1547 - val_loss: 0.1310\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.1166 - val_loss: 0.1018\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0900 - val_loss: 0.0804\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0704 - val_loss: 0.0649\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0561 - val_loss: 0.0535\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0455 - val_loss: 0.0451\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0377 - val_loss: 0.0390\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0320 - val_loss: 0.0346\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0278 - val_loss: 0.0314\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0247 - val_loss: 0.0289\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0223 - val_loss: 0.0271\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0204 - val_loss: 0.0256\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0190 - val_loss: 0.0245\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0178 - val_loss: 0.0235\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0167 - val_loss: 0.0226\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0158 - val_loss: 0.0219\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0151 - val_loss: 0.0212\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0144 - val_loss: 0.0206\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0138 - val_loss: 0.0201\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0132 - val_loss: 0.0196\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0127 - val_loss: 0.0191\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0123 - val_loss: 0.0187\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0119 - val_loss: 0.0184\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0115 - val_loss: 0.0180\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0111 - val_loss: 0.0177\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0107 - val_loss: 0.0174\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0104 - val_loss: 0.0171\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0101 - val_loss: 0.0168\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0098 - val_loss: 0.0165\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0095 - val_loss: 0.0163\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0093 - val_loss: 0.0160\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0090 - val_loss: 0.0158\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0088 - val_loss: 0.0156\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0085 - val_loss: 0.0154\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0083 - val_loss: 0.0152\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0081 - val_loss: 0.0150\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0079 - val_loss: 0.0148\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0077 - val_loss: 0.0146\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0074 - val_loss: 0.0145\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0072 - val_loss: 0.0143\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0070 - val_loss: 0.0142\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0068 - val_loss: 0.0141\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0066 - val_loss: 0.0140\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0063 - val_loss: 0.0138\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0062 - val_loss: 0.0137\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0060 - val_loss: 0.0136\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.0058 - val_loss: 0.0136\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0057 - val_loss: 0.0135\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0055 - val_loss: 0.0134\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0054 - val_loss: 0.0134\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0053 - val_loss: 0.0133\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0052 - val_loss: 0.0132\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0051 - val_loss: 0.0131\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0050 - val_loss: 0.0130\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0049 - val_loss: 0.0130\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0048 - val_loss: 0.0129\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0047 - val_loss: 0.0128\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0047 - val_loss: 0.0128\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0046 - val_loss: 0.0127\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0046 - val_loss: 0.0127\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0045 - val_loss: 0.0127\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0045 - val_loss: 0.0126\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0045 - val_loss: 0.0126\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0044 - val_loss: 0.0126\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0044 - val_loss: 0.0125\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0044 - val_loss: 0.0125\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0043 - val_loss: 0.0125\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0043 - val_loss: 0.0125\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0043 - val_loss: 0.0124\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0043 - val_loss: 0.0124\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0042 - val_loss: 0.0124\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0042 - val_loss: 0.0124\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0042 - val_loss: 0.0124\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0042 - val_loss: 0.0123\n",
            "6/6 [==============================] - 1s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 5 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 5s 31ms/step - loss: 0.1259 - val_loss: 0.1072\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0541 - val_loss: 0.0725\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0342 - val_loss: 0.0587\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0259 - val_loss: 0.0511\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0212 - val_loss: 0.0459\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0179 - val_loss: 0.0418\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0154 - val_loss: 0.0386\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0135 - val_loss: 0.0359\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0120 - val_loss: 0.0337\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0108 - val_loss: 0.0319\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0098 - val_loss: 0.0304\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0089 - val_loss: 0.0290\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0082 - val_loss: 0.0278\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0076 - val_loss: 0.0267\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0071 - val_loss: 0.0258\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0067 - val_loss: 0.0250\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0063 - val_loss: 0.0242\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0060 - val_loss: 0.0235\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0057 - val_loss: 0.0229\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0054 - val_loss: 0.0224\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0052 - val_loss: 0.0219\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0050 - val_loss: 0.0214\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0049 - val_loss: 0.0210\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0047 - val_loss: 0.0206\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0046 - val_loss: 0.0202\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0045 - val_loss: 0.0199\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0044 - val_loss: 0.0196\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0043 - val_loss: 0.0194\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0042 - val_loss: 0.0191\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0042 - val_loss: 0.0189\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0041 - val_loss: 0.0187\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0040 - val_loss: 0.0185\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0040 - val_loss: 0.0183\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0039 - val_loss: 0.0181\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0039 - val_loss: 0.0180\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0038 - val_loss: 0.0178\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0038 - val_loss: 0.0177\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.0037 - val_loss: 0.0175\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0037 - val_loss: 0.0174\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0037 - val_loss: 0.0173\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0036 - val_loss: 0.0172\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0036 - val_loss: 0.0171\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0036 - val_loss: 0.0170\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.0036 - val_loss: 0.0169\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0035 - val_loss: 0.0169\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0035 - val_loss: 0.0168\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0035 - val_loss: 0.0167\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0035 - val_loss: 0.0166\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0034 - val_loss: 0.0166\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.0034 - val_loss: 0.0165\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0034 - val_loss: 0.0164\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0034 - val_loss: 0.0164\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0033 - val_loss: 0.0163\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0033 - val_loss: 0.0163\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0033 - val_loss: 0.0162\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0033 - val_loss: 0.0162\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0033 - val_loss: 0.0161\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.0032 - val_loss: 0.0161\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0032 - val_loss: 0.0160\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0032 - val_loss: 0.0160\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0032 - val_loss: 0.0160\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0032 - val_loss: 0.0159\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0032 - val_loss: 0.0159\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0031 - val_loss: 0.0159\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0031 - val_loss: 0.0158\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0031 - val_loss: 0.0158\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0031 - val_loss: 0.0158\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0031 - val_loss: 0.0157\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0031 - val_loss: 0.0157\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0031 - val_loss: 0.0156\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0030 - val_loss: 0.0156\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0030 - val_loss: 0.0156\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0030 - val_loss: 0.0155\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0030 - val_loss: 0.0155\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0030 - val_loss: 0.0155\n",
            "6/6 [==============================] - 1s 9ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 6 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 3s 21ms/step - loss: 0.1888 - val_loss: 0.1181\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0890 - val_loss: 0.0657\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0496 - val_loss: 0.0440\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0324 - val_loss: 0.0348\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0244 - val_loss: 0.0308\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0204 - val_loss: 0.0288\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0181 - val_loss: 0.0275\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0165 - val_loss: 0.0266\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0152 - val_loss: 0.0257\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0142 - val_loss: 0.0248\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0133 - val_loss: 0.0240\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0124 - val_loss: 0.0232\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0117 - val_loss: 0.0225\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0110 - val_loss: 0.0219\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0104 - val_loss: 0.0213\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0099 - val_loss: 0.0207\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0094 - val_loss: 0.0202\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0089 - val_loss: 0.0197\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0085 - val_loss: 0.0193\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0082 - val_loss: 0.0189\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0079 - val_loss: 0.0186\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 12ms/step - loss: 0.0076 - val_loss: 0.0182\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0073 - val_loss: 0.0179\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0071 - val_loss: 0.0177\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.0069 - val_loss: 0.0174\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0067 - val_loss: 0.0172\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0065 - val_loss: 0.0169\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.0063 - val_loss: 0.0167\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0062 - val_loss: 0.0166\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0060 - val_loss: 0.0164\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0059 - val_loss: 0.0163\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.0058 - val_loss: 0.0161\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0057 - val_loss: 0.0159\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0056 - val_loss: 0.0158\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0055 - val_loss: 0.0157\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0054 - val_loss: 0.0156\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0053 - val_loss: 0.0155\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0053 - val_loss: 0.0154\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0052 - val_loss: 0.0153\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0051 - val_loss: 0.0152\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0051 - val_loss: 0.0151\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0050 - val_loss: 0.0151\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0049 - val_loss: 0.0150\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0049 - val_loss: 0.0149\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0048 - val_loss: 0.0149\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0048 - val_loss: 0.0148\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0047 - val_loss: 0.0147\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0047 - val_loss: 0.0147\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0046 - val_loss: 0.0146\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0046 - val_loss: 0.0146\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0046 - val_loss: 0.0145\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0045 - val_loss: 0.0145\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0045 - val_loss: 0.0144\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0044 - val_loss: 0.0144\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0044 - val_loss: 0.0143\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0044 - val_loss: 0.0143\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0043 - val_loss: 0.0142\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0043 - val_loss: 0.0142\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0043 - val_loss: 0.0142\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0042 - val_loss: 0.0141\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0042 - val_loss: 0.0141\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0042 - val_loss: 0.0141\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0041 - val_loss: 0.0140\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0041 - val_loss: 0.0140\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0041 - val_loss: 0.0140\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0040 - val_loss: 0.0139\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0040 - val_loss: 0.0139\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.0040 - val_loss: 0.0139\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0040 - val_loss: 0.0139\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0039 - val_loss: 0.0138\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.0039 - val_loss: 0.0138\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0039 - val_loss: 0.0138\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0038 - val_loss: 0.0138\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0038 - val_loss: 0.0137\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.0038 - val_loss: 0.0137\n",
            "6/6 [==============================] - 1s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 7 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 5s 33ms/step - loss: 0.1123 - val_loss: 0.0558\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0340 - val_loss: 0.0248\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0145 - val_loss: 0.0169\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0089 - val_loss: 0.0151\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0072 - val_loss: 0.0147\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0066 - val_loss: 0.0147\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0064 - val_loss: 0.0147\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0062 - val_loss: 0.0147\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0061 - val_loss: 0.0146\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0060 - val_loss: 0.0144\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0059 - val_loss: 0.0143\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0058 - val_loss: 0.0141\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0057 - val_loss: 0.0140\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0056 - val_loss: 0.0138\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0055 - val_loss: 0.0137\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0054 - val_loss: 0.0136\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0054 - val_loss: 0.0135\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0053 - val_loss: 0.0134\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0052 - val_loss: 0.0133\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0052 - val_loss: 0.0133\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0051 - val_loss: 0.0132\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0051 - val_loss: 0.0131\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0050 - val_loss: 0.0129\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0050 - val_loss: 0.0128\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0049 - val_loss: 0.0128\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0048 - val_loss: 0.0127\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0048 - val_loss: 0.0126\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0047 - val_loss: 0.0125\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0047 - val_loss: 0.0124\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0046 - val_loss: 0.0123\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0046 - val_loss: 0.0122\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0045 - val_loss: 0.0122\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0045 - val_loss: 0.0121\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.0045 - val_loss: 0.0120\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0044 - val_loss: 0.0119\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0044 - val_loss: 0.0119\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0043 - val_loss: 0.0118\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.0043 - val_loss: 0.0118\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0043 - val_loss: 0.0117\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0042 - val_loss: 0.0117\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.0042 - val_loss: 0.0116\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.0041 - val_loss: 0.0116\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0041 - val_loss: 0.0115\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0041 - val_loss: 0.0115\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0040 - val_loss: 0.0115\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0040 - val_loss: 0.0114\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0040 - val_loss: 0.0114\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0040 - val_loss: 0.0113\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0039 - val_loss: 0.0113\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0039 - val_loss: 0.0112\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0039 - val_loss: 0.0112\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0038 - val_loss: 0.0112\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0038 - val_loss: 0.0111\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0038 - val_loss: 0.0111\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0037 - val_loss: 0.0111\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0037 - val_loss: 0.0110\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0037 - val_loss: 0.0110\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0037 - val_loss: 0.0110\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0036 - val_loss: 0.0109\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0036 - val_loss: 0.0109\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0036 - val_loss: 0.0109\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0036 - val_loss: 0.0108\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0035 - val_loss: 0.0108\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0035 - val_loss: 0.0108\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0035 - val_loss: 0.0108\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0035 - val_loss: 0.0107\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0034 - val_loss: 0.0107\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0034 - val_loss: 0.0107\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0034 - val_loss: 0.0106\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0034 - val_loss: 0.0106\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0034 - val_loss: 0.0106\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0033 - val_loss: 0.0106\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0033 - val_loss: 0.0105\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0033 - val_loss: 0.0105\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0033 - val_loss: 0.0105\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 8 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 4s 29ms/step - loss: 0.2133 - val_loss: 0.1256\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.1025 - val_loss: 0.0695\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0585 - val_loss: 0.0466\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0351 - val_loss: 0.0341\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0237 - val_loss: 0.0288\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 1s 11ms/step - loss: 0.0185 - val_loss: 0.0259\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0154 - val_loss: 0.0240\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0133 - val_loss: 0.0225\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0116 - val_loss: 0.0211\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.0103 - val_loss: 0.0199\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0092 - val_loss: 0.0189\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.0083 - val_loss: 0.0180\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0075 - val_loss: 0.0173\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0069 - val_loss: 0.0166\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0063 - val_loss: 0.0161\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0059 - val_loss: 0.0156\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0055 - val_loss: 0.0152\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0052 - val_loss: 0.0149\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0049 - val_loss: 0.0146\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0046 - val_loss: 0.0143\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0044 - val_loss: 0.0141\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0042 - val_loss: 0.0139\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0040 - val_loss: 0.0137\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0039 - val_loss: 0.0136\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0038 - val_loss: 0.0135\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0037 - val_loss: 0.0134\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0036 - val_loss: 0.0133\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0035 - val_loss: 0.0131\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0034 - val_loss: 0.0131\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0034 - val_loss: 0.0130\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0033 - val_loss: 0.0129\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0033 - val_loss: 0.0128\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0032 - val_loss: 0.0128\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0032 - val_loss: 0.0127\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0031 - val_loss: 0.0127\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0031 - val_loss: 0.0126\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0031 - val_loss: 0.0126\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0030 - val_loss: 0.0125\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0030 - val_loss: 0.0125\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0030 - val_loss: 0.0124\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0030 - val_loss: 0.0124\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0029 - val_loss: 0.0124\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0029 - val_loss: 0.0123\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0029 - val_loss: 0.0123\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 12ms/step - loss: 0.0029 - val_loss: 0.0123\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0029 - val_loss: 0.0122\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0029 - val_loss: 0.0122\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0029 - val_loss: 0.0122\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0028 - val_loss: 0.0121\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0028 - val_loss: 0.0121\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0028 - val_loss: 0.0121\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0028 - val_loss: 0.0120\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0028 - val_loss: 0.0120\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0028 - val_loss: 0.0120\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0028 - val_loss: 0.0119\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.0028 - val_loss: 0.0119\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0027 - val_loss: 0.0119\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0027 - val_loss: 0.0119\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0027 - val_loss: 0.0118\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0027 - val_loss: 0.0118\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0027 - val_loss: 0.0118\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0027 - val_loss: 0.0118\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0027 - val_loss: 0.0118\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0027 - val_loss: 0.0117\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0027 - val_loss: 0.0117\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0027 - val_loss: 0.0117\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0026 - val_loss: 0.0116\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0026 - val_loss: 0.0116\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0026 - val_loss: 0.0116\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0026 - val_loss: 0.0116\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0116\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0026 - val_loss: 0.0116\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0116\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0026 - val_loss: 0.0115\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0115\n",
            "6/6 [==============================] - 1s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 9 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 4s 29ms/step - loss: 0.2892 - val_loss: 0.2267\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.1871 - val_loss: 0.1594\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.1378 - val_loss: 0.1236\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.1070 - val_loss: 0.1016\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0860 - val_loss: 0.0854\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0701 - val_loss: 0.0732\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0577 - val_loss: 0.0639\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0481 - val_loss: 0.0568\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0405 - val_loss: 0.0512\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0344 - val_loss: 0.0468\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0296 - val_loss: 0.0435\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0257 - val_loss: 0.0408\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0226 - val_loss: 0.0386\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0200 - val_loss: 0.0369\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0179 - val_loss: 0.0355\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0162 - val_loss: 0.0343\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0147 - val_loss: 0.0333\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0134 - val_loss: 0.0324\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0124 - val_loss: 0.0316\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0115 - val_loss: 0.0309\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0106 - val_loss: 0.0303\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0099 - val_loss: 0.0297\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0093 - val_loss: 0.0292\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0087 - val_loss: 0.0286\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0082 - val_loss: 0.0281\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0078 - val_loss: 0.0277\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0074 - val_loss: 0.0273\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0070 - val_loss: 0.0268\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0067 - val_loss: 0.0264\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0064 - val_loss: 0.0261\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0061 - val_loss: 0.0257\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0059 - val_loss: 0.0254\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0057 - val_loss: 0.0251\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0055 - val_loss: 0.0248\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0053 - val_loss: 0.0245\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0052 - val_loss: 0.0242\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0050 - val_loss: 0.0240\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0049 - val_loss: 0.0237\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 12ms/step - loss: 0.0048 - val_loss: 0.0235\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 12ms/step - loss: 0.0046 - val_loss: 0.0233\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0046 - val_loss: 0.0231\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0045 - val_loss: 0.0229\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0044 - val_loss: 0.0227\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 12ms/step - loss: 0.0043 - val_loss: 0.0226\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0043 - val_loss: 0.0224\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0042 - val_loss: 0.0223\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0041 - val_loss: 0.0221\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0041 - val_loss: 0.0220\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0041 - val_loss: 0.0219\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0040 - val_loss: 0.0218\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0040 - val_loss: 0.0217\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 12ms/step - loss: 0.0039 - val_loss: 0.0216\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 12ms/step - loss: 0.0039 - val_loss: 0.0215\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0039 - val_loss: 0.0214\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0039 - val_loss: 0.0213\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0038 - val_loss: 0.0213\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0038 - val_loss: 0.0212\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0038 - val_loss: 0.0211\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0038 - val_loss: 0.0211\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0038 - val_loss: 0.0210\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0037 - val_loss: 0.0210\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0037 - val_loss: 0.0209\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0037 - val_loss: 0.0209\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0037 - val_loss: 0.0208\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0037 - val_loss: 0.0208\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0037 - val_loss: 0.0208\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0036 - val_loss: 0.0207\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0036 - val_loss: 0.0207\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0036 - val_loss: 0.0207\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0036 - val_loss: 0.0207\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0036 - val_loss: 0.0206\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0036 - val_loss: 0.0206\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0036 - val_loss: 0.0206\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0036 - val_loss: 0.0205\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0036 - val_loss: 0.0205\n",
            "6/6 [==============================] - 0s 5ms/step\n",
            "Best_hyper_parameters(LSTM): \n",
            " {'model': [64], 'optimizer': 'Adagrad', 'learning_rate': 0.01, 'batch_size': 16, 'best_avg_rmse': 2423.8052354890297}\n",
            "all_avg_rmse(LSTM): \n",
            " [[[5962.00517544 4719.6058583  4373.55067154]\n",
            "  [3622.02927437 3256.72282462 3058.80241573]\n",
            "  [2936.94755113 2850.75465457 2781.7348498 ]]\n",
            "\n",
            " [[2673.47001148 2595.53882175 2534.47299529]\n",
            "  [2472.64332423 2429.62108447 2423.80523549]\n",
            "  [2531.74644154 2641.83743494 2761.47771436]]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model': [64],\n",
              " 'optimizer': 'Adagrad',\n",
              " 'learning_rate': 0.01,\n",
              " 'batch_size': 16,\n",
              " 'best_avg_rmse': 2423.8052354890297}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Case V: 128N-LSTM**"
      ],
      "metadata": {
        "id": "_LDxUWPgxRTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layers = [128]\n",
        "time_step = 5\n",
        "optimizers_names = ['Adam', 'Adagrad']\n",
        "learning_rates =  [0.1, 0.01, 0.001]\n",
        "batch_sizes =  [4, 8, 16]\n",
        "epochs = 75\n",
        "num_replicates = 10\n",
        "test_split = 0.2\n",
        "val_split = 0.2\n",
        "\n",
        "train_data, test_data = data_split(data, test_split)\n",
        "\n",
        "lstm_N128_best_hyper_parameters = LSTM_Hyper_Parameter_Tuning(layers, train_data, time_step, val_split, optimizers_names, learning_rates, batch_sizes, epochs = epochs, num_replicates = num_replicates)\n",
        "lstm_N128_best_hyper_parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgLCe-1aw4ez",
        "outputId": "488071ba-a590-452c-eb04-11a8d2d9620f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0010 - val_loss: 0.0059\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0010 - val_loss: 0.0050\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 9.6710e-04 - val_loss: 0.0052\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 9.3051e-04 - val_loss: 0.0050\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 9.0631e-04 - val_loss: 0.0048\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 8.5698e-04 - val_loss: 0.0045\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 8.3143e-04 - val_loss: 0.0046\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 8.1097e-04 - val_loss: 0.0041\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 7.7613e-04 - val_loss: 0.0042\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 7.5567e-04 - val_loss: 0.0036\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 7.2948e-04 - val_loss: 0.0035\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 7.0081e-04 - val_loss: 0.0036\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 6.7491e-04 - val_loss: 0.0035\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.5987e-04 - val_loss: 0.0034\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.3807e-04 - val_loss: 0.0032\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.1899e-04 - val_loss: 0.0030\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.0346e-04 - val_loss: 0.0028\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5.8677e-04 - val_loss: 0.0029\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5.6307e-04 - val_loss: 0.0030\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5.5191e-04 - val_loss: 0.0028\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5.3571e-04 - val_loss: 0.0025\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5.2816e-04 - val_loss: 0.0024\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 5.1118e-04 - val_loss: 0.0025\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 4.9736e-04 - val_loss: 0.0023\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 4.8531e-04 - val_loss: 0.0025\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.6865e-04 - val_loss: 0.0024\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 4.7145e-04 - val_loss: 0.0022\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 4.5658e-04 - val_loss: 0.0021\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.4656e-04 - val_loss: 0.0021\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.3496e-04 - val_loss: 0.0022\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.2536e-04 - val_loss: 0.0020\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.1942e-04 - val_loss: 0.0019\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 4.1101e-04 - val_loss: 0.0020\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.0578e-04 - val_loss: 0.0018\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.0146e-04 - val_loss: 0.0018\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 3.9629e-04 - val_loss: 0.0017\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 3.8373e-04 - val_loss: 0.0019\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 3.8281e-04 - val_loss: 0.0018\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 3.7825e-04 - val_loss: 0.0018\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 3.7561e-04 - val_loss: 0.0018\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 3.6874e-04 - val_loss: 0.0017\n",
            "6/6 [==============================] - 0s 5ms/step\n",
            "Running for Adagrad optimizer 0.01 learning_rate 16 batch_size and 6 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 2s 17ms/step - loss: 0.0245 - val_loss: 0.0189\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0056 - val_loss: 0.0180\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0045 - val_loss: 0.0169\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0040 - val_loss: 0.0164\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0035 - val_loss: 0.0150\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0032 - val_loss: 0.0156\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0030 - val_loss: 0.0137\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0028 - val_loss: 0.0140\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0027 - val_loss: 0.0127\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0025 - val_loss: 0.0132\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0023 - val_loss: 0.0119\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0023 - val_loss: 0.0111\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0103\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0020 - val_loss: 0.0103\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0019 - val_loss: 0.0099\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0099\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0085\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0081\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0016 - val_loss: 0.0077\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 0.0072\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0015 - val_loss: 0.0066\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0014 - val_loss: 0.0063\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0014 - val_loss: 0.0063\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0013 - val_loss: 0.0060\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0012 - val_loss: 0.0054\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0012 - val_loss: 0.0051\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0011 - val_loss: 0.0047\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0011 - val_loss: 0.0045\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0010 - val_loss: 0.0042\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0010 - val_loss: 0.0040\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 9.5832e-04 - val_loss: 0.0038\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 9.3058e-04 - val_loss: 0.0037\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 8.8179e-04 - val_loss: 0.0037\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 8.6262e-04 - val_loss: 0.0033\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 8.2456e-04 - val_loss: 0.0031\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 7.9904e-04 - val_loss: 0.0030\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 7.7031e-04 - val_loss: 0.0028\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 7.3699e-04 - val_loss: 0.0026\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 7.2001e-04 - val_loss: 0.0026\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 6.8892e-04 - val_loss: 0.0026\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 6.7596e-04 - val_loss: 0.0023\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 6.4913e-04 - val_loss: 0.0023\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 6.3190e-04 - val_loss: 0.0022\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 6.1092e-04 - val_loss: 0.0021\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 5.8593e-04 - val_loss: 0.0019\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 5.8237e-04 - val_loss: 0.0018\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 5.5778e-04 - val_loss: 0.0019\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 5.4914e-04 - val_loss: 0.0018\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 5.3004e-04 - val_loss: 0.0016\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 5.2390e-04 - val_loss: 0.0015\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 5.0441e-04 - val_loss: 0.0015\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 5.0026e-04 - val_loss: 0.0015\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 4.9715e-04 - val_loss: 0.0015\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 4.8233e-04 - val_loss: 0.0014\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 4.6218e-04 - val_loss: 0.0015\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.6253e-04 - val_loss: 0.0013\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 4.5652e-04 - val_loss: 0.0013\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 4.4873e-04 - val_loss: 0.0013\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 4.4091e-04 - val_loss: 0.0013\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.3971e-04 - val_loss: 0.0013\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.3013e-04 - val_loss: 0.0013\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 4.1860e-04 - val_loss: 0.0012\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 4.1854e-04 - val_loss: 0.0012\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 4.0418e-04 - val_loss: 0.0012\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.0307e-04 - val_loss: 0.0012\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 3.9480e-04 - val_loss: 0.0016\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 4.1197e-04 - val_loss: 0.0011\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 3.8945e-04 - val_loss: 0.0011\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 3.8449e-04 - val_loss: 0.0012\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 3.8631e-04 - val_loss: 0.0011\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 3.7677e-04 - val_loss: 0.0011\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 3.7564e-04 - val_loss: 0.0011\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 3.7513e-04 - val_loss: 0.0011\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 3.6936e-04 - val_loss: 0.0011\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 3.6289e-04 - val_loss: 0.0011\n",
            "6/6 [==============================] - 1s 6ms/step\n",
            "Running for Adagrad optimizer 0.01 learning_rate 16 batch_size and 7 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 4s 21ms/step - loss: 0.0924 - val_loss: 0.0376\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0137 - val_loss: 0.0195\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0064 - val_loss: 0.0136\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0045 - val_loss: 0.0115\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0037 - val_loss: 0.0109\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0033 - val_loss: 0.0102\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0030 - val_loss: 0.0098\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0028 - val_loss: 0.0099\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0026 - val_loss: 0.0092\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0024 - val_loss: 0.0092\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0023 - val_loss: 0.0090\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0022 - val_loss: 0.0087\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0021 - val_loss: 0.0087\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0020 - val_loss: 0.0081\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0019 - val_loss: 0.0081\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0076\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0018 - val_loss: 0.0075\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0072\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0017 - val_loss: 0.0071\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0069\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0016 - val_loss: 0.0069\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0065\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0015 - val_loss: 0.0063\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0015 - val_loss: 0.0060\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0061\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0013 - val_loss: 0.0057\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0013 - val_loss: 0.0056\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0056\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0052\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0051\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0012 - val_loss: 0.0051\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0012 - val_loss: 0.0050\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 0.0045\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0011 - val_loss: 0.0045\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0011 - val_loss: 0.0043\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0010 - val_loss: 0.0042\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0010 - val_loss: 0.0040\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 9.8503e-04 - val_loss: 0.0040\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 9.6115e-04 - val_loss: 0.0038\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 9.2664e-04 - val_loss: 0.0036\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 9.1539e-04 - val_loss: 0.0035\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 8.8052e-04 - val_loss: 0.0033\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 8.6842e-04 - val_loss: 0.0033\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 8.4395e-04 - val_loss: 0.0033\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 8.1654e-04 - val_loss: 0.0032\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 7.9868e-04 - val_loss: 0.0029\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 7.8319e-04 - val_loss: 0.0028\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 7.6398e-04 - val_loss: 0.0027\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 7.4540e-04 - val_loss: 0.0027\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 7.2912e-04 - val_loss: 0.0028\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 7.1130e-04 - val_loss: 0.0024\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.9848e-04 - val_loss: 0.0025\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.8461e-04 - val_loss: 0.0025\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.6284e-04 - val_loss: 0.0025\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 6.6058e-04 - val_loss: 0.0022\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 6.4125e-04 - val_loss: 0.0023\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 6.3044e-04 - val_loss: 0.0022\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 6.1450e-04 - val_loss: 0.0022\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 5.9938e-04 - val_loss: 0.0020\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 5.9090e-04 - val_loss: 0.0019\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 5.7770e-04 - val_loss: 0.0019\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 5.6487e-04 - val_loss: 0.0019\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 5.5950e-04 - val_loss: 0.0019\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 5.4509e-04 - val_loss: 0.0018\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 5.3413e-04 - val_loss: 0.0018\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 5.2781e-04 - val_loss: 0.0017\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 5.1805e-04 - val_loss: 0.0017\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 5.1438e-04 - val_loss: 0.0016\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5.0746e-04 - val_loss: 0.0017\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.9394e-04 - val_loss: 0.0016\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.8946e-04 - val_loss: 0.0017\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 4.8462e-04 - val_loss: 0.0016\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.7663e-04 - val_loss: 0.0016\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.7002e-04 - val_loss: 0.0015\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.6700e-04 - val_loss: 0.0014\n",
            "6/6 [==============================] - 0s 4ms/step\n",
            "Running for Adagrad optimizer 0.01 learning_rate 16 batch_size and 8 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 3s 18ms/step - loss: 0.0817 - val_loss: 0.0408\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0093 - val_loss: 0.0244\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0044 - val_loss: 0.0185\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0034 - val_loss: 0.0167\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0032 - val_loss: 0.0163\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0030 - val_loss: 0.0151\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0028 - val_loss: 0.0150\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0027 - val_loss: 0.0143\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0132\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0025 - val_loss: 0.0130\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0024 - val_loss: 0.0122\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0023 - val_loss: 0.0119\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0022 - val_loss: 0.0117\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0021 - val_loss: 0.0114\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0020 - val_loss: 0.0106\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0019 - val_loss: 0.0110\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0019 - val_loss: 0.0102\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0101\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0102\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0098\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0091\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0016 - val_loss: 0.0090\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0016 - val_loss: 0.0086\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0015 - val_loss: 0.0082\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0015 - val_loss: 0.0079\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 0.0078\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0079\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0070\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0068\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0068\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0066\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0062\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0061\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0011 - val_loss: 0.0063\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0011 - val_loss: 0.0056\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0011 - val_loss: 0.0057\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0010 - val_loss: 0.0058\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0010 - val_loss: 0.0050\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 9.7710e-04 - val_loss: 0.0049\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 9.5111e-04 - val_loss: 0.0048\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 9.2384e-04 - val_loss: 0.0048\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 8.9423e-04 - val_loss: 0.0044\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 8.8552e-04 - val_loss: 0.0044\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 8.5475e-04 - val_loss: 0.0043\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 8.2482e-04 - val_loss: 0.0041\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 8.0701e-04 - val_loss: 0.0040\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 7.8814e-04 - val_loss: 0.0040\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 7.6563e-04 - val_loss: 0.0039\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 7.4215e-04 - val_loss: 0.0036\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 7.1512e-04 - val_loss: 0.0037\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 7.0213e-04 - val_loss: 0.0035\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 6.9092e-04 - val_loss: 0.0036\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.7370e-04 - val_loss: 0.0033\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 6.5192e-04 - val_loss: 0.0033\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.4219e-04 - val_loss: 0.0030\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 6.3033e-04 - val_loss: 0.0029\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 6.0632e-04 - val_loss: 0.0029\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 5.9589e-04 - val_loss: 0.0026\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5.8287e-04 - val_loss: 0.0027\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5.7038e-04 - val_loss: 0.0026\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 5.5229e-04 - val_loss: 0.0025\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 5.3975e-04 - val_loss: 0.0025\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 5.2804e-04 - val_loss: 0.0023\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5.1818e-04 - val_loss: 0.0023\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 5.0802e-04 - val_loss: 0.0023\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 4.9700e-04 - val_loss: 0.0023\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 4.9062e-04 - val_loss: 0.0022\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 4.7553e-04 - val_loss: 0.0023\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 4.6979e-04 - val_loss: 0.0021\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 4.5774e-04 - val_loss: 0.0020\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 4.5208e-04 - val_loss: 0.0019\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 4.4442e-04 - val_loss: 0.0018\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 4.4147e-04 - val_loss: 0.0020\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 4.3128e-04 - val_loss: 0.0018\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.2164e-04 - val_loss: 0.0019\n",
            "6/6 [==============================] - 0s 5ms/step\n",
            "Running for Adagrad optimizer 0.01 learning_rate 16 batch_size and 9 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 4s 24ms/step - loss: 0.0509 - val_loss: 0.0253\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0046 - val_loss: 0.0184\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0031 - val_loss: 0.0157\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0028 - val_loss: 0.0154\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0027 - val_loss: 0.0152\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0026 - val_loss: 0.0144\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0025 - val_loss: 0.0145\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0024 - val_loss: 0.0137\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0023 - val_loss: 0.0131\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0022 - val_loss: 0.0130\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0022 - val_loss: 0.0121\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0021 - val_loss: 0.0115\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0108\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0115\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0019 - val_loss: 0.0107\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0018 - val_loss: 0.0103\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0101\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0099\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0016 - val_loss: 0.0093\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0086\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0087\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0015 - val_loss: 0.0081\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0014 - val_loss: 0.0081\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0014 - val_loss: 0.0074\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0014 - val_loss: 0.0069\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0071\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0072\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0012 - val_loss: 0.0067\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0012 - val_loss: 0.0064\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0011 - val_loss: 0.0060\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0011 - val_loss: 0.0060\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0011 - val_loss: 0.0054\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0010 - val_loss: 0.0055\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0010 - val_loss: 0.0052\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 9.7678e-04 - val_loss: 0.0049\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 9.5860e-04 - val_loss: 0.0049\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 9.2897e-04 - val_loss: 0.0046\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 8.9748e-04 - val_loss: 0.0045\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 8.7331e-04 - val_loss: 0.0045\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 8.4116e-04 - val_loss: 0.0040\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 8.2105e-04 - val_loss: 0.0041\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 7.9777e-04 - val_loss: 0.0039\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 7.7703e-04 - val_loss: 0.0039\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 7.5797e-04 - val_loss: 0.0040\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 7.3533e-04 - val_loss: 0.0034\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 7.1660e-04 - val_loss: 0.0033\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.8966e-04 - val_loss: 0.0034\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 6.7906e-04 - val_loss: 0.0035\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.6651e-04 - val_loss: 0.0029\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.4473e-04 - val_loss: 0.0031\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 6.2318e-04 - val_loss: 0.0028\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6.1479e-04 - val_loss: 0.0030\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 5.9872e-04 - val_loss: 0.0026\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 5.8018e-04 - val_loss: 0.0026\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 5.6764e-04 - val_loss: 0.0024\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5.6034e-04 - val_loss: 0.0025\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5.4242e-04 - val_loss: 0.0022\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 5.3282e-04 - val_loss: 0.0022\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 5.1947e-04 - val_loss: 0.0024\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 5.1231e-04 - val_loss: 0.0021\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5.0047e-04 - val_loss: 0.0023\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 4.8903e-04 - val_loss: 0.0023\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.8513e-04 - val_loss: 0.0020\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 4.7483e-04 - val_loss: 0.0021\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.6801e-04 - val_loss: 0.0021\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 4.5791e-04 - val_loss: 0.0019\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.4937e-04 - val_loss: 0.0018\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.4567e-04 - val_loss: 0.0017\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.3427e-04 - val_loss: 0.0019\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 4.2932e-04 - val_loss: 0.0018\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.2346e-04 - val_loss: 0.0017\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.1684e-04 - val_loss: 0.0017\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.1032e-04 - val_loss: 0.0017\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.0956e-04 - val_loss: 0.0017\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4.0441e-04 - val_loss: 0.0017\n",
            "6/6 [==============================] - 1s 3ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 0 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 3s 9ms/step - loss: 0.0979 - val_loss: 0.0515\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0313 - val_loss: 0.0319\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0196 - val_loss: 0.0234\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0133 - val_loss: 0.0184\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0096 - val_loss: 0.0155\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0074 - val_loss: 0.0137\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0061 - val_loss: 0.0126\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0053 - val_loss: 0.0120\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0048 - val_loss: 0.0114\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0045 - val_loss: 0.0111\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0042 - val_loss: 0.0108\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0106\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0038 - val_loss: 0.0106\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0037 - val_loss: 0.0104\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0036 - val_loss: 0.0101\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0035 - val_loss: 0.0102\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0034 - val_loss: 0.0101\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0100\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0032 - val_loss: 0.0100\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0098\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0031 - val_loss: 0.0098\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0097\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0029 - val_loss: 0.0097\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0029 - val_loss: 0.0096\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0095\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0096\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0095\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0094\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0026 - val_loss: 0.0095\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0026 - val_loss: 0.0094\n",
            "Epoch 31/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0025 - val_loss: 0.0093\n",
            "Epoch 32/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0025 - val_loss: 0.0093\n",
            "Epoch 33/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0025 - val_loss: 0.0092\n",
            "Epoch 34/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0024 - val_loss: 0.0092\n",
            "Epoch 35/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0024 - val_loss: 0.0091\n",
            "Epoch 36/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0024 - val_loss: 0.0091\n",
            "Epoch 37/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0023 - val_loss: 0.0090\n",
            "Epoch 38/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0023 - val_loss: 0.0090\n",
            "Epoch 39/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0023 - val_loss: 0.0090\n",
            "Epoch 40/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0022 - val_loss: 0.0089\n",
            "Epoch 41/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0022 - val_loss: 0.0088\n",
            "Epoch 42/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0022 - val_loss: 0.0089\n",
            "Epoch 43/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0022 - val_loss: 0.0087\n",
            "Epoch 44/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0021 - val_loss: 0.0087\n",
            "Epoch 45/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0021 - val_loss: 0.0086\n",
            "Epoch 46/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0021 - val_loss: 0.0086\n",
            "Epoch 47/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0021 - val_loss: 0.0086\n",
            "Epoch 48/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0020 - val_loss: 0.0085\n",
            "Epoch 49/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0020 - val_loss: 0.0084\n",
            "Epoch 50/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0020 - val_loss: 0.0084\n",
            "Epoch 51/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0020 - val_loss: 0.0084\n",
            "Epoch 52/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0020 - val_loss: 0.0083\n",
            "Epoch 53/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0082\n",
            "Epoch 54/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0019 - val_loss: 0.0082\n",
            "Epoch 55/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0082\n",
            "Epoch 56/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0019 - val_loss: 0.0081\n",
            "Epoch 57/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0019 - val_loss: 0.0080\n",
            "Epoch 58/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0018 - val_loss: 0.0080\n",
            "Epoch 59/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0018 - val_loss: 0.0080\n",
            "Epoch 60/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0018 - val_loss: 0.0079\n",
            "Epoch 61/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0018 - val_loss: 0.0078\n",
            "Epoch 62/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0018 - val_loss: 0.0078\n",
            "Epoch 63/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0018 - val_loss: 0.0078\n",
            "Epoch 64/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0017 - val_loss: 0.0077\n",
            "Epoch 65/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0017 - val_loss: 0.0076\n",
            "Epoch 66/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0017 - val_loss: 0.0076\n",
            "Epoch 67/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0017 - val_loss: 0.0075\n",
            "Epoch 68/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0017 - val_loss: 0.0074\n",
            "Epoch 69/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0017 - val_loss: 0.0074\n",
            "Epoch 70/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0017 - val_loss: 0.0074\n",
            "Epoch 71/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0016 - val_loss: 0.0072\n",
            "Epoch 72/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0016 - val_loss: 0.0072\n",
            "Epoch 73/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0016 - val_loss: 0.0072\n",
            "Epoch 74/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0016 - val_loss: 0.0071\n",
            "Epoch 75/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0016 - val_loss: 0.0071\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 1 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 3s 9ms/step - loss: 0.1343 - val_loss: 0.0380\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0255 - val_loss: 0.0230\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0146 - val_loss: 0.0210\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0107 - val_loss: 0.0189\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0085 - val_loss: 0.0176\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0071 - val_loss: 0.0170\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0062 - val_loss: 0.0163\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0056 - val_loss: 0.0159\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0052 - val_loss: 0.0158\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0049 - val_loss: 0.0156\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0046 - val_loss: 0.0154\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0044 - val_loss: 0.0152\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0042 - val_loss: 0.0154\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0041 - val_loss: 0.0151\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0149\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0039 - val_loss: 0.0150\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0038 - val_loss: 0.0147\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0037 - val_loss: 0.0148\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0036 - val_loss: 0.0151\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0035 - val_loss: 0.0149\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0034 - val_loss: 0.0147\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0034 - val_loss: 0.0147\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0145\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0147\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0032 - val_loss: 0.0147\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0144\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0144\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0030 - val_loss: 0.0144\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0144\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0143\n",
            "Epoch 31/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0142\n",
            "Epoch 32/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0028 - val_loss: 0.0142\n",
            "Epoch 33/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0028 - val_loss: 0.0142\n",
            "Epoch 34/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0028 - val_loss: 0.0141\n",
            "Epoch 35/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0141\n",
            "Epoch 36/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0141\n",
            "Epoch 37/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0139\n",
            "Epoch 38/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0026 - val_loss: 0.0139\n",
            "Epoch 39/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0026 - val_loss: 0.0138\n",
            "Epoch 40/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0137\n",
            "Epoch 41/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0025 - val_loss: 0.0137\n",
            "Epoch 42/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0135\n",
            "Epoch 43/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0025 - val_loss: 0.0135\n",
            "Epoch 44/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0025 - val_loss: 0.0134\n",
            "Epoch 45/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0024 - val_loss: 0.0135\n",
            "Epoch 46/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0024 - val_loss: 0.0132\n",
            "Epoch 47/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0024 - val_loss: 0.0131\n",
            "Epoch 48/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0024 - val_loss: 0.0131\n",
            "Epoch 49/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0023 - val_loss: 0.0131\n",
            "Epoch 50/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0023 - val_loss: 0.0131\n",
            "Epoch 51/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0023 - val_loss: 0.0130\n",
            "Epoch 52/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0023 - val_loss: 0.0129\n",
            "Epoch 53/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0022 - val_loss: 0.0129\n",
            "Epoch 54/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0022 - val_loss: 0.0127\n",
            "Epoch 55/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0022 - val_loss: 0.0127\n",
            "Epoch 56/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0022 - val_loss: 0.0126\n",
            "Epoch 57/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0022 - val_loss: 0.0125\n",
            "Epoch 58/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0021 - val_loss: 0.0125\n",
            "Epoch 59/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0123\n",
            "Epoch 60/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0123\n",
            "Epoch 61/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0122\n",
            "Epoch 62/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0121\n",
            "Epoch 63/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0020 - val_loss: 0.0120\n",
            "Epoch 64/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0120\n",
            "Epoch 65/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0020 - val_loss: 0.0118\n",
            "Epoch 66/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0020 - val_loss: 0.0116\n",
            "Epoch 67/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0020 - val_loss: 0.0117\n",
            "Epoch 68/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0020 - val_loss: 0.0115\n",
            "Epoch 69/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0019 - val_loss: 0.0115\n",
            "Epoch 70/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0019 - val_loss: 0.0113\n",
            "Epoch 71/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0019 - val_loss: 0.0113\n",
            "Epoch 72/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0019 - val_loss: 0.0112\n",
            "Epoch 73/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0111\n",
            "Epoch 74/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0018 - val_loss: 0.0110\n",
            "Epoch 75/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0018 - val_loss: 0.0109\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 2 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 4s 9ms/step - loss: 0.1890 - val_loss: 0.0861\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0460 - val_loss: 0.0328\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0167 - val_loss: 0.0247\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0112 - val_loss: 0.0223\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0090 - val_loss: 0.0208\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0078 - val_loss: 0.0198\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0070 - val_loss: 0.0189\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0064 - val_loss: 0.0183\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0060 - val_loss: 0.0178\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0056 - val_loss: 0.0174\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0053 - val_loss: 0.0171\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0050 - val_loss: 0.0169\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0048 - val_loss: 0.0166\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0046 - val_loss: 0.0165\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0045 - val_loss: 0.0163\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0044 - val_loss: 0.0161\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0043 - val_loss: 0.0161\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0042 - val_loss: 0.0160\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0041 - val_loss: 0.0160\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0041 - val_loss: 0.0160\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0159\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0039 - val_loss: 0.0159\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0039 - val_loss: 0.0159\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0038 - val_loss: 0.0159\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0038 - val_loss: 0.0158\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0037 - val_loss: 0.0160\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0037 - val_loss: 0.0159\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0036 - val_loss: 0.0159\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0036 - val_loss: 0.0158\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0035 - val_loss: 0.0158\n",
            "6/6 [==============================] - 1s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 3 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 7s 9ms/step - loss: 0.0638 - val_loss: 0.0325\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0138 - val_loss: 0.0242\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0083 - val_loss: 0.0197\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0058 - val_loss: 0.0170\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0046 - val_loss: 0.0155\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0039 - val_loss: 0.0146\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0035 - val_loss: 0.0141\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0137\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0134\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0134\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0134\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0132\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0132\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0133\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0131\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0026 - val_loss: 0.0129\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0026 - val_loss: 0.0129\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0026 - val_loss: 0.0129\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0025 - val_loss: 0.0127\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0025 - val_loss: 0.0128\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0024 - val_loss: 0.0126\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0024 - val_loss: 0.0126\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0024 - val_loss: 0.0124\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0023 - val_loss: 0.0125\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0023 - val_loss: 0.0123\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0023 - val_loss: 0.0123\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0023 - val_loss: 0.0121\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0022 - val_loss: 0.0122\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0022 - val_loss: 0.0120\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0022 - val_loss: 0.0118\n",
            "Epoch 31/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0022 - val_loss: 0.0119\n",
            "Epoch 32/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0021 - val_loss: 0.0118\n",
            "Epoch 33/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0021 - val_loss: 0.0117\n",
            "Epoch 34/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0021 - val_loss: 0.0116\n",
            "Epoch 35/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0021 - val_loss: 0.0116\n",
            "Epoch 36/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0021 - val_loss: 0.0115\n",
            "Epoch 37/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0020 - val_loss: 0.0114\n",
            "Epoch 38/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0020 - val_loss: 0.0115\n",
            "Epoch 39/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0113\n",
            "Epoch 40/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0112\n",
            "Epoch 41/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0113\n",
            "Epoch 42/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0019 - val_loss: 0.0110\n",
            "Epoch 43/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0109\n",
            "Epoch 44/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0019 - val_loss: 0.0110\n",
            "Epoch 45/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0109\n",
            "Epoch 46/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0019 - val_loss: 0.0107\n",
            "Epoch 47/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0019 - val_loss: 0.0106\n",
            "Epoch 48/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0018 - val_loss: 0.0105\n",
            "Epoch 49/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0018 - val_loss: 0.0104\n",
            "Epoch 50/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0018 - val_loss: 0.0104\n",
            "Epoch 51/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0018 - val_loss: 0.0102\n",
            "Epoch 52/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0018 - val_loss: 0.0101\n",
            "Epoch 53/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0018 - val_loss: 0.0101\n",
            "Epoch 54/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0017 - val_loss: 0.0100\n",
            "Epoch 55/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0017 - val_loss: 0.0100\n",
            "Epoch 56/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0017 - val_loss: 0.0100\n",
            "Epoch 57/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0017 - val_loss: 0.0097\n",
            "Epoch 58/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0017 - val_loss: 0.0096\n",
            "Epoch 59/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0017 - val_loss: 0.0096\n",
            "Epoch 60/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0017 - val_loss: 0.0097\n",
            "Epoch 61/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0016 - val_loss: 0.0095\n",
            "Epoch 62/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0016 - val_loss: 0.0094\n",
            "Epoch 63/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0016 - val_loss: 0.0094\n",
            "Epoch 64/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0016 - val_loss: 0.0092\n",
            "Epoch 65/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0016 - val_loss: 0.0092\n",
            "Epoch 66/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0016 - val_loss: 0.0091\n",
            "Epoch 67/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0016 - val_loss: 0.0090\n",
            "Epoch 68/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0015 - val_loss: 0.0090\n",
            "Epoch 69/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0015 - val_loss: 0.0090\n",
            "Epoch 70/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0015 - val_loss: 0.0090\n",
            "Epoch 71/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0015 - val_loss: 0.0089\n",
            "Epoch 72/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0015 - val_loss: 0.0087\n",
            "Epoch 73/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0015 - val_loss: 0.0086\n",
            "Epoch 74/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0015 - val_loss: 0.0086\n",
            "Epoch 75/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0015 - val_loss: 0.0084\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 4 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 4s 9ms/step - loss: 0.0911 - val_loss: 0.0279\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0179 - val_loss: 0.0177\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0114 - val_loss: 0.0142\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0084 - val_loss: 0.0118\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0068 - val_loss: 0.0106\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0058 - val_loss: 0.0098\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0052 - val_loss: 0.0092\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0047 - val_loss: 0.0089\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0044 - val_loss: 0.0088\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0042 - val_loss: 0.0087\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0084\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0038 - val_loss: 0.0083\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0082\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0035 - val_loss: 0.0082\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0081\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0081\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0080\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0030 - val_loss: 0.0080\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0079\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0028 - val_loss: 0.0079\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0028 - val_loss: 0.0079\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0078\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0026 - val_loss: 0.0078\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0025 - val_loss: 0.0077\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0078\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0024 - val_loss: 0.0076\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0024 - val_loss: 0.0075\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0023 - val_loss: 0.0074\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0023 - val_loss: 0.0074\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0022 - val_loss: 0.0073\n",
            "Epoch 31/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0022 - val_loss: 0.0075\n",
            "Epoch 32/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0021 - val_loss: 0.0073\n",
            "Epoch 33/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0073\n",
            "Epoch 34/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0020 - val_loss: 0.0073\n",
            "Epoch 35/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0020 - val_loss: 0.0073\n",
            "Epoch 36/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0020 - val_loss: 0.0072\n",
            "Epoch 37/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0019 - val_loss: 0.0071\n",
            "Epoch 38/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0019 - val_loss: 0.0071\n",
            "Epoch 39/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0019 - val_loss: 0.0071\n",
            "Epoch 40/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0018 - val_loss: 0.0071\n",
            "Epoch 41/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0018 - val_loss: 0.0069\n",
            "Epoch 42/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0018 - val_loss: 0.0070\n",
            "Epoch 43/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0018 - val_loss: 0.0069\n",
            "Epoch 44/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0017 - val_loss: 0.0069\n",
            "Epoch 45/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0017 - val_loss: 0.0069\n",
            "Epoch 46/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0017 - val_loss: 0.0068\n",
            "Epoch 47/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0017 - val_loss: 0.0068\n",
            "Epoch 48/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0016 - val_loss: 0.0067\n",
            "Epoch 49/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0016 - val_loss: 0.0066\n",
            "Epoch 50/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0016 - val_loss: 0.0067\n",
            "Epoch 51/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0016 - val_loss: 0.0066\n",
            "Epoch 52/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0016 - val_loss: 0.0065\n",
            "Epoch 53/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0015 - val_loss: 0.0065\n",
            "Epoch 54/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0015 - val_loss: 0.0065\n",
            "Epoch 55/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0015 - val_loss: 0.0064\n",
            "Epoch 56/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0015 - val_loss: 0.0063\n",
            "Epoch 57/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0015 - val_loss: 0.0063\n",
            "Epoch 58/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0015 - val_loss: 0.0063\n",
            "Epoch 59/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0014 - val_loss: 0.0063\n",
            "Epoch 60/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0014 - val_loss: 0.0062\n",
            "Epoch 61/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0014 - val_loss: 0.0062\n",
            "Epoch 62/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0014 - val_loss: 0.0062\n",
            "Epoch 63/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0014 - val_loss: 0.0061\n",
            "Epoch 64/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0014 - val_loss: 0.0061\n",
            "Epoch 65/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0014 - val_loss: 0.0059\n",
            "Epoch 66/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0013 - val_loss: 0.0060\n",
            "Epoch 67/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0013 - val_loss: 0.0060\n",
            "Epoch 68/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0013 - val_loss: 0.0060\n",
            "Epoch 69/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0013 - val_loss: 0.0059\n",
            "Epoch 70/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0013 - val_loss: 0.0059\n",
            "Epoch 71/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0013 - val_loss: 0.0058\n",
            "Epoch 72/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0013 - val_loss: 0.0059\n",
            "Epoch 73/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0013 - val_loss: 0.0057\n",
            "Epoch 74/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0012 - val_loss: 0.0057\n",
            "Epoch 75/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0012 - val_loss: 0.0057\n",
            "6/6 [==============================] - 1s 7ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 5 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 4s 10ms/step - loss: 0.0674 - val_loss: 0.0239\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0195 - val_loss: 0.0163\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0115 - val_loss: 0.0135\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0080 - val_loss: 0.0120\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0064 - val_loss: 0.0116\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0055 - val_loss: 0.0115\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0049 - val_loss: 0.0110\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0045 - val_loss: 0.0108\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0042 - val_loss: 0.0109\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0039 - val_loss: 0.0106\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0102\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0035 - val_loss: 0.0102\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0100\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0032 - val_loss: 0.0098\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0098\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0101\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0096\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0096\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0026 - val_loss: 0.0094\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0025 - val_loss: 0.0092\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0024 - val_loss: 0.0093\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0023 - val_loss: 0.0090\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0023 - val_loss: 0.0092\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0022 - val_loss: 0.0091\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0022 - val_loss: 0.0092\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0090\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0090\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0020 - val_loss: 0.0090\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0090\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0087\n",
            "Epoch 31/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0086\n",
            "Epoch 32/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0087\n",
            "Epoch 33/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0018 - val_loss: 0.0086\n",
            "Epoch 34/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0018 - val_loss: 0.0085\n",
            "Epoch 35/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0018 - val_loss: 0.0086\n",
            "Epoch 36/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0017 - val_loss: 0.0084\n",
            "Epoch 37/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0017 - val_loss: 0.0083\n",
            "Epoch 38/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0017 - val_loss: 0.0082\n",
            "Epoch 39/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0017 - val_loss: 0.0082\n",
            "Epoch 40/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0016 - val_loss: 0.0082\n",
            "Epoch 41/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0016 - val_loss: 0.0080\n",
            "Epoch 42/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0016 - val_loss: 0.0080\n",
            "Epoch 43/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0016 - val_loss: 0.0079\n",
            "Epoch 44/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0016 - val_loss: 0.0080\n",
            "Epoch 45/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0015 - val_loss: 0.0078\n",
            "Epoch 46/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0015 - val_loss: 0.0077\n",
            "Epoch 47/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0015 - val_loss: 0.0077\n",
            "Epoch 48/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0015 - val_loss: 0.0075\n",
            "Epoch 49/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0015 - val_loss: 0.0075\n",
            "Epoch 50/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0014 - val_loss: 0.0075\n",
            "Epoch 51/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0014 - val_loss: 0.0073\n",
            "Epoch 52/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0014 - val_loss: 0.0072\n",
            "Epoch 53/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0014 - val_loss: 0.0072\n",
            "Epoch 54/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0014 - val_loss: 0.0071\n",
            "Epoch 55/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0014 - val_loss: 0.0071\n",
            "Epoch 56/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0013 - val_loss: 0.0069\n",
            "Epoch 57/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0013 - val_loss: 0.0070\n",
            "Epoch 58/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0013 - val_loss: 0.0068\n",
            "Epoch 59/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0013 - val_loss: 0.0068\n",
            "Epoch 60/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0013 - val_loss: 0.0067\n",
            "Epoch 61/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0013 - val_loss: 0.0067\n",
            "Epoch 62/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0013 - val_loss: 0.0066\n",
            "Epoch 63/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0012 - val_loss: 0.0065\n",
            "Epoch 64/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0012 - val_loss: 0.0065\n",
            "Epoch 65/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0012 - val_loss: 0.0064\n",
            "Epoch 66/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0012 - val_loss: 0.0063\n",
            "Epoch 67/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0012 - val_loss: 0.0062\n",
            "Epoch 68/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0012 - val_loss: 0.0061\n",
            "Epoch 69/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0012 - val_loss: 0.0061\n",
            "Epoch 70/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0012 - val_loss: 0.0060\n",
            "Epoch 71/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0011 - val_loss: 0.0059\n",
            "Epoch 72/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0011 - val_loss: 0.0059\n",
            "Epoch 73/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0011 - val_loss: 0.0059\n",
            "Epoch 74/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0011 - val_loss: 0.0057\n",
            "Epoch 75/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0011 - val_loss: 0.0057\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 6 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 5s 12ms/step - loss: 0.1615 - val_loss: 0.0826\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0465 - val_loss: 0.0393\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0229 - val_loss: 0.0262\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0141 - val_loss: 0.0203\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0097 - val_loss: 0.0172\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0072 - val_loss: 0.0154\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0058 - val_loss: 0.0145\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0049 - val_loss: 0.0140\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0044 - val_loss: 0.0135\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0041 - val_loss: 0.0133\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0038 - val_loss: 0.0130\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0036 - val_loss: 0.0130\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0034 - val_loss: 0.0128\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0033 - val_loss: 0.0127\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0126\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0126\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0030 - val_loss: 0.0124\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0124\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0123\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0124\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0122\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0122\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0119\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0118\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0026 - val_loss: 0.0118\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0026 - val_loss: 0.0117\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0119\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0025 - val_loss: 0.0118\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0115\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0024 - val_loss: 0.0115\n",
            "Epoch 31/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0024 - val_loss: 0.0115\n",
            "Epoch 32/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0024 - val_loss: 0.0114\n",
            "Epoch 33/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0024 - val_loss: 0.0112\n",
            "Epoch 34/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0023 - val_loss: 0.0112\n",
            "Epoch 35/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0023 - val_loss: 0.0111\n",
            "Epoch 36/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0023 - val_loss: 0.0111\n",
            "Epoch 37/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0023 - val_loss: 0.0111\n",
            "Epoch 38/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0022 - val_loss: 0.0109\n",
            "Epoch 39/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0022 - val_loss: 0.0109\n",
            "Epoch 40/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0022 - val_loss: 0.0107\n",
            "Epoch 41/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0022 - val_loss: 0.0107\n",
            "Epoch 42/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0022 - val_loss: 0.0106\n",
            "Epoch 43/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0021 - val_loss: 0.0106\n",
            "Epoch 44/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0105\n",
            "Epoch 45/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0103\n",
            "Epoch 46/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0102\n",
            "Epoch 47/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0021 - val_loss: 0.0102\n",
            "Epoch 48/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0102\n",
            "Epoch 49/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0100\n",
            "Epoch 50/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0020 - val_loss: 0.0099\n",
            "Epoch 51/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0020 - val_loss: 0.0099\n",
            "Epoch 52/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0020 - val_loss: 0.0097\n",
            "Epoch 53/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0020 - val_loss: 0.0098\n",
            "Epoch 54/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0020 - val_loss: 0.0097\n",
            "Epoch 55/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0019 - val_loss: 0.0096\n",
            "Epoch 56/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0019 - val_loss: 0.0095\n",
            "Epoch 57/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0019 - val_loss: 0.0095\n",
            "Epoch 58/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0019 - val_loss: 0.0094\n",
            "Epoch 59/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0092\n",
            "Epoch 60/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0019 - val_loss: 0.0092\n",
            "Epoch 61/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0018 - val_loss: 0.0092\n",
            "Epoch 62/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0018 - val_loss: 0.0090\n",
            "Epoch 63/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0018 - val_loss: 0.0089\n",
            "Epoch 64/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0018 - val_loss: 0.0089\n",
            "Epoch 65/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0018 - val_loss: 0.0088\n",
            "Epoch 66/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0018 - val_loss: 0.0087\n",
            "Epoch 67/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0018 - val_loss: 0.0087\n",
            "Epoch 68/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0018 - val_loss: 0.0086\n",
            "Epoch 69/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0017 - val_loss: 0.0086\n",
            "Epoch 70/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0017 - val_loss: 0.0085\n",
            "Epoch 71/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0017 - val_loss: 0.0084\n",
            "Epoch 72/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0017 - val_loss: 0.0084\n",
            "Epoch 73/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0017 - val_loss: 0.0082\n",
            "Epoch 74/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0017 - val_loss: 0.0081\n",
            "Epoch 75/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0017 - val_loss: 0.0080\n",
            "6/6 [==============================] - 1s 8ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 7 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 5s 12ms/step - loss: 0.1213 - val_loss: 0.0297\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0150 - val_loss: 0.0194\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0085 - val_loss: 0.0176\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0069 - val_loss: 0.0170\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0061 - val_loss: 0.0163\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0056 - val_loss: 0.0166\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0053 - val_loss: 0.0160\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0050 - val_loss: 0.0156\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0047 - val_loss: 0.0156\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0045 - val_loss: 0.0158\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0043 - val_loss: 0.0157\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0041 - val_loss: 0.0155\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0039 - val_loss: 0.0153\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0038 - val_loss: 0.0153\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0153\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0035 - val_loss: 0.0151\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0034 - val_loss: 0.0150\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0147\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0147\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0146\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0146\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0030 - val_loss: 0.0146\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0029 - val_loss: 0.0144\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0143\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0028 - val_loss: 0.0142\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0143\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0142\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0026 - val_loss: 0.0142\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0026 - val_loss: 0.0144\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0141\n",
            "Epoch 31/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0140\n",
            "Epoch 32/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0024 - val_loss: 0.0139\n",
            "Epoch 33/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0024 - val_loss: 0.0140\n",
            "Epoch 34/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0024 - val_loss: 0.0138\n",
            "Epoch 35/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0023 - val_loss: 0.0140\n",
            "Epoch 36/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0023 - val_loss: 0.0140\n",
            "Epoch 37/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0023 - val_loss: 0.0135\n",
            "Epoch 38/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0023 - val_loss: 0.0136\n",
            "Epoch 39/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0022 - val_loss: 0.0136\n",
            "Epoch 40/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0022 - val_loss: 0.0136\n",
            "Epoch 41/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0022 - val_loss: 0.0136\n",
            "Epoch 42/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0021 - val_loss: 0.0135\n",
            "Epoch 43/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0021 - val_loss: 0.0134\n",
            "Epoch 44/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0021 - val_loss: 0.0132\n",
            "Epoch 45/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0021 - val_loss: 0.0132\n",
            "Epoch 46/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0020 - val_loss: 0.0131\n",
            "Epoch 47/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0020 - val_loss: 0.0131\n",
            "Epoch 48/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0131\n",
            "Epoch 49/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0020 - val_loss: 0.0130\n",
            "Epoch 50/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0130\n",
            "Epoch 51/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0019 - val_loss: 0.0129\n",
            "Epoch 52/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0019 - val_loss: 0.0127\n",
            "Epoch 53/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0019 - val_loss: 0.0128\n",
            "Epoch 54/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0019 - val_loss: 0.0127\n",
            "Epoch 55/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0018 - val_loss: 0.0126\n",
            "Epoch 56/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0018 - val_loss: 0.0127\n",
            "Epoch 57/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0018 - val_loss: 0.0126\n",
            "Epoch 58/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0018 - val_loss: 0.0124\n",
            "Epoch 59/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0018 - val_loss: 0.0124\n",
            "Epoch 60/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0017 - val_loss: 0.0123\n",
            "Epoch 61/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0017 - val_loss: 0.0121\n",
            "Epoch 62/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0017 - val_loss: 0.0121\n",
            "Epoch 63/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0017 - val_loss: 0.0120\n",
            "Epoch 64/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0017 - val_loss: 0.0121\n",
            "Epoch 65/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0017 - val_loss: 0.0119\n",
            "Epoch 66/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0016 - val_loss: 0.0118\n",
            "Epoch 67/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0016 - val_loss: 0.0117\n",
            "Epoch 68/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0016 - val_loss: 0.0118\n",
            "Epoch 69/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0016 - val_loss: 0.0115\n",
            "Epoch 70/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0016 - val_loss: 0.0115\n",
            "Epoch 71/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0016 - val_loss: 0.0114\n",
            "Epoch 72/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0015 - val_loss: 0.0112\n",
            "Epoch 73/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0015 - val_loss: 0.0112\n",
            "Epoch 74/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0015 - val_loss: 0.0112\n",
            "Epoch 75/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0015 - val_loss: 0.0111\n",
            "6/6 [==============================] - 0s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 8 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 4s 10ms/step - loss: 0.1228 - val_loss: 0.0495\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0266 - val_loss: 0.0316\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0166 - val_loss: 0.0256\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0120 - val_loss: 0.0218\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0091 - val_loss: 0.0192\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0074 - val_loss: 0.0175\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0063 - val_loss: 0.0164\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0056 - val_loss: 0.0156\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0051 - val_loss: 0.0149\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0048 - val_loss: 0.0143\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0045 - val_loss: 0.0141\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0043 - val_loss: 0.0138\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0042 - val_loss: 0.0138\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0135\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0039 - val_loss: 0.0133\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0038 - val_loss: 0.0132\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0037 - val_loss: 0.0131\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0132\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0035 - val_loss: 0.0131\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0035 - val_loss: 0.0131\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0034 - val_loss: 0.0131\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0130\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0033 - val_loss: 0.0126\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0129\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0128\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0127\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0127\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0030 - val_loss: 0.0126\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0124\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0125\n",
            "Epoch 31/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0029 - val_loss: 0.0124\n",
            "Epoch 32/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0028 - val_loss: 0.0124\n",
            "Epoch 33/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0028 - val_loss: 0.0123\n",
            "Epoch 34/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0123\n",
            "Epoch 35/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0123\n",
            "Epoch 36/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0122\n",
            "Epoch 37/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0121\n",
            "Epoch 38/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0121\n",
            "Epoch 39/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0026 - val_loss: 0.0121\n",
            "Epoch 40/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0120\n",
            "Epoch 41/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0120\n",
            "Epoch 42/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0025 - val_loss: 0.0118\n",
            "Epoch 43/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0025 - val_loss: 0.0118\n",
            "Epoch 44/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0024 - val_loss: 0.0118\n",
            "Epoch 45/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0024 - val_loss: 0.0116\n",
            "Epoch 46/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0024 - val_loss: 0.0116\n",
            "Epoch 47/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0024 - val_loss: 0.0117\n",
            "Epoch 48/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0023 - val_loss: 0.0115\n",
            "Epoch 49/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0023 - val_loss: 0.0114\n",
            "Epoch 50/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0023 - val_loss: 0.0114\n",
            "Epoch 51/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0023 - val_loss: 0.0113\n",
            "Epoch 52/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0022 - val_loss: 0.0113\n",
            "Epoch 53/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0022 - val_loss: 0.0112\n",
            "Epoch 54/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0022 - val_loss: 0.0112\n",
            "Epoch 55/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0022 - val_loss: 0.0110\n",
            "Epoch 56/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0022 - val_loss: 0.0110\n",
            "Epoch 57/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0021 - val_loss: 0.0110\n",
            "Epoch 58/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0021 - val_loss: 0.0108\n",
            "Epoch 59/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0110\n",
            "Epoch 60/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0021 - val_loss: 0.0108\n",
            "Epoch 61/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0021 - val_loss: 0.0107\n",
            "Epoch 62/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0020 - val_loss: 0.0106\n",
            "Epoch 63/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0020 - val_loss: 0.0105\n",
            "Epoch 64/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0020 - val_loss: 0.0106\n",
            "Epoch 65/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0020 - val_loss: 0.0105\n",
            "Epoch 66/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0020 - val_loss: 0.0104\n",
            "Epoch 67/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0020 - val_loss: 0.0104\n",
            "Epoch 68/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0019 - val_loss: 0.0103\n",
            "Epoch 69/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0019 - val_loss: 0.0102\n",
            "Epoch 70/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0019 - val_loss: 0.0102\n",
            "Epoch 71/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0102\n",
            "Epoch 72/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0019 - val_loss: 0.0101\n",
            "Epoch 73/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0100\n",
            "Epoch 74/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0019 - val_loss: 0.0100\n",
            "Epoch 75/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0018 - val_loss: 0.0099\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 4 batch_size and 9 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "171/171 [==============================] - 3s 8ms/step - loss: 0.1580 - val_loss: 0.0585\n",
            "Epoch 2/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0382 - val_loss: 0.0276\n",
            "Epoch 3/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0199 - val_loss: 0.0208\n",
            "Epoch 4/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0139 - val_loss: 0.0183\n",
            "Epoch 5/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0110 - val_loss: 0.0168\n",
            "Epoch 6/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0092 - val_loss: 0.0157\n",
            "Epoch 7/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0081 - val_loss: 0.0151\n",
            "Epoch 8/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0073 - val_loss: 0.0146\n",
            "Epoch 9/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0067 - val_loss: 0.0144\n",
            "Epoch 10/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0062 - val_loss: 0.0140\n",
            "Epoch 11/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0058 - val_loss: 0.0139\n",
            "Epoch 12/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0055 - val_loss: 0.0138\n",
            "Epoch 13/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0052 - val_loss: 0.0137\n",
            "Epoch 14/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0050 - val_loss: 0.0135\n",
            "Epoch 15/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0048 - val_loss: 0.0135\n",
            "Epoch 16/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0047 - val_loss: 0.0134\n",
            "Epoch 17/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0045 - val_loss: 0.0134\n",
            "Epoch 18/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0044 - val_loss: 0.0133\n",
            "Epoch 19/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0043 - val_loss: 0.0132\n",
            "Epoch 20/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0042 - val_loss: 0.0133\n",
            "Epoch 21/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0041 - val_loss: 0.0132\n",
            "Epoch 22/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0040 - val_loss: 0.0131\n",
            "Epoch 23/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0039 - val_loss: 0.0133\n",
            "Epoch 24/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0038 - val_loss: 0.0132\n",
            "Epoch 25/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0037 - val_loss: 0.0131\n",
            "Epoch 26/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0036 - val_loss: 0.0131\n",
            "Epoch 27/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0036 - val_loss: 0.0131\n",
            "Epoch 28/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0035 - val_loss: 0.0131\n",
            "Epoch 29/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0034 - val_loss: 0.0131\n",
            "Epoch 30/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0034 - val_loss: 0.0130\n",
            "Epoch 31/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0130\n",
            "Epoch 32/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0130\n",
            "Epoch 33/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0130\n",
            "Epoch 34/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0032 - val_loss: 0.0130\n",
            "Epoch 35/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0031 - val_loss: 0.0130\n",
            "Epoch 36/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0031 - val_loss: 0.0129\n",
            "Epoch 37/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0030 - val_loss: 0.0128\n",
            "Epoch 38/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0030 - val_loss: 0.0129\n",
            "Epoch 39/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0029 - val_loss: 0.0129\n",
            "Epoch 40/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0029 - val_loss: 0.0128\n",
            "Epoch 41/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0029 - val_loss: 0.0127\n",
            "Epoch 42/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0028 - val_loss: 0.0129\n",
            "Epoch 43/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0127\n",
            "Epoch 44/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0127\n",
            "Epoch 45/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0127\n",
            "Epoch 46/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0127\n",
            "Epoch 47/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0126\n",
            "Epoch 48/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0126\n",
            "Epoch 49/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0125\n",
            "Epoch 50/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0026 - val_loss: 0.0125\n",
            "Epoch 51/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0025 - val_loss: 0.0124\n",
            "Epoch 52/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0025 - val_loss: 0.0125\n",
            "Epoch 53/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0025 - val_loss: 0.0124\n",
            "Epoch 54/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0024 - val_loss: 0.0124\n",
            "Epoch 55/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0024 - val_loss: 0.0124\n",
            "Epoch 56/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0024 - val_loss: 0.0123\n",
            "Epoch 57/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0024 - val_loss: 0.0122\n",
            "Epoch 58/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0023 - val_loss: 0.0122\n",
            "Epoch 59/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0023 - val_loss: 0.0121\n",
            "Epoch 60/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0023 - val_loss: 0.0121\n",
            "Epoch 61/75\n",
            "171/171 [==============================] - 1s 9ms/step - loss: 0.0023 - val_loss: 0.0120\n",
            "Epoch 62/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0022 - val_loss: 0.0119\n",
            "Epoch 63/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0022 - val_loss: 0.0119\n",
            "Epoch 64/75\n",
            "171/171 [==============================] - 1s 8ms/step - loss: 0.0022 - val_loss: 0.0119\n",
            "Epoch 65/75\n",
            "171/171 [==============================] - 2s 9ms/step - loss: 0.0022 - val_loss: 0.0118\n",
            "Epoch 66/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0022 - val_loss: 0.0117\n",
            "Epoch 67/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0021 - val_loss: 0.0117\n",
            "Epoch 68/75\n",
            "171/171 [==============================] - 1s 7ms/step - loss: 0.0021 - val_loss: 0.0115\n",
            "Epoch 69/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0021 - val_loss: 0.0116\n",
            "Epoch 70/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0021 - val_loss: 0.0115\n",
            "Epoch 71/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0021 - val_loss: 0.0114\n",
            "Epoch 72/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0020 - val_loss: 0.0114\n",
            "Epoch 73/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0020 - val_loss: 0.0113\n",
            "Epoch 74/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0020 - val_loss: 0.0112\n",
            "Epoch 75/75\n",
            "171/171 [==============================] - 1s 6ms/step - loss: 0.0020 - val_loss: 0.0112\n",
            "6/6 [==============================] - 0s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 0 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 4s 14ms/step - loss: 0.1684 - val_loss: 0.0771\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0379 - val_loss: 0.0364\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0180 - val_loss: 0.0314\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0137 - val_loss: 0.0287\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0112 - val_loss: 0.0263\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0094 - val_loss: 0.0246\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0082 - val_loss: 0.0234\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0073 - val_loss: 0.0224\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0066 - val_loss: 0.0215\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0062 - val_loss: 0.0208\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0058 - val_loss: 0.0202\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0055 - val_loss: 0.0198\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0052 - val_loss: 0.0194\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0050 - val_loss: 0.0191\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0049 - val_loss: 0.0188\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0047 - val_loss: 0.0186\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0045 - val_loss: 0.0185\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0044 - val_loss: 0.0181\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0043 - val_loss: 0.0181\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0042 - val_loss: 0.0178\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0041 - val_loss: 0.0178\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0040 - val_loss: 0.0178\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0040 - val_loss: 0.0177\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0039 - val_loss: 0.0175\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0039 - val_loss: 0.0175\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0038 - val_loss: 0.0173\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0037 - val_loss: 0.0173\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0171\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0170\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0035 - val_loss: 0.0168\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0035 - val_loss: 0.0167\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0034 - val_loss: 0.0166\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0034 - val_loss: 0.0166\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0166\n",
            "Epoch 35/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0163\n",
            "Epoch 36/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0032 - val_loss: 0.0163\n",
            "Epoch 37/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0161\n",
            "Epoch 38/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0160\n",
            "Epoch 39/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0158\n",
            "Epoch 40/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0158\n",
            "Epoch 41/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0157\n",
            "Epoch 42/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0156\n",
            "Epoch 43/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0155\n",
            "Epoch 44/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0156\n",
            "Epoch 45/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0154\n",
            "Epoch 46/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0029 - val_loss: 0.0153\n",
            "Epoch 47/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0028 - val_loss: 0.0152\n",
            "Epoch 48/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0153\n",
            "Epoch 49/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0028 - val_loss: 0.0151\n",
            "Epoch 50/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0028 - val_loss: 0.0151\n",
            "Epoch 51/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0150\n",
            "Epoch 52/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0150\n",
            "Epoch 53/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0147\n",
            "Epoch 54/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0147\n",
            "Epoch 55/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0147\n",
            "Epoch 56/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0147\n",
            "Epoch 57/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0146\n",
            "Epoch 58/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0145\n",
            "Epoch 59/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0144\n",
            "Epoch 60/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0144\n",
            "Epoch 61/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0143\n",
            "Epoch 62/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0144\n",
            "Epoch 63/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0025 - val_loss: 0.0143\n",
            "Epoch 64/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0142\n",
            "Epoch 65/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0142\n",
            "Epoch 66/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0025 - val_loss: 0.0141\n",
            "Epoch 67/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0141\n",
            "Epoch 68/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0024 - val_loss: 0.0139\n",
            "Epoch 69/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0024 - val_loss: 0.0138\n",
            "Epoch 70/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0024 - val_loss: 0.0137\n",
            "Epoch 71/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0024 - val_loss: 0.0138\n",
            "Epoch 72/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0024 - val_loss: 0.0137\n",
            "Epoch 73/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0024 - val_loss: 0.0137\n",
            "Epoch 74/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0024 - val_loss: 0.0135\n",
            "Epoch 75/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0023 - val_loss: 0.0135\n",
            "6/6 [==============================] - 0s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 1 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 3s 12ms/step - loss: 0.1859 - val_loss: 0.1091\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0827 - val_loss: 0.0571\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0445 - val_loss: 0.0386\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0278 - val_loss: 0.0305\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0197 - val_loss: 0.0270\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0158 - val_loss: 0.0250\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0133 - val_loss: 0.0235\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0114 - val_loss: 0.0223\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0100 - val_loss: 0.0211\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0089 - val_loss: 0.0205\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0080 - val_loss: 0.0197\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0073 - val_loss: 0.0191\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0067 - val_loss: 0.0186\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0062 - val_loss: 0.0182\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0058 - val_loss: 0.0181\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0054 - val_loss: 0.0180\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0051 - val_loss: 0.0178\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0048 - val_loss: 0.0178\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0046 - val_loss: 0.0178\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0044 - val_loss: 0.0177\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0042 - val_loss: 0.0177\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0041 - val_loss: 0.0176\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0039 - val_loss: 0.0175\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0039 - val_loss: 0.0174\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0038 - val_loss: 0.0174\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0037 - val_loss: 0.0170\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0036 - val_loss: 0.0170\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0036 - val_loss: 0.0169\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0035 - val_loss: 0.0169\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0035 - val_loss: 0.0168\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0034 - val_loss: 0.0167\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0034 - val_loss: 0.0167\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0033 - val_loss: 0.0165\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0164\n",
            "Epoch 35/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0033 - val_loss: 0.0165\n",
            "Epoch 36/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0032 - val_loss: 0.0165\n",
            "Epoch 37/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0032 - val_loss: 0.0163\n",
            "Epoch 38/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0032 - val_loss: 0.0163\n",
            "Epoch 39/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0160\n",
            "Epoch 40/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0159\n",
            "Epoch 41/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0031 - val_loss: 0.0159\n",
            "Epoch 42/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0160\n",
            "Epoch 43/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0157\n",
            "Epoch 44/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0156\n",
            "Epoch 45/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0158\n",
            "Epoch 46/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0157\n",
            "Epoch 47/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0030 - val_loss: 0.0158\n",
            "Epoch 48/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0029 - val_loss: 0.0156\n",
            "Epoch 49/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0029 - val_loss: 0.0156\n",
            "Epoch 50/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0029 - val_loss: 0.0155\n",
            "Epoch 51/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0029 - val_loss: 0.0154\n",
            "Epoch 52/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0155\n",
            "Epoch 53/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0152\n",
            "Epoch 54/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0153\n",
            "Epoch 55/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0152\n",
            "Epoch 56/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0152\n",
            "Epoch 57/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0151\n",
            "Epoch 58/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0151\n",
            "Epoch 59/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0150\n",
            "Epoch 60/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0149\n",
            "Epoch 61/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0150\n",
            "Epoch 62/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0149\n",
            "Epoch 63/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0149\n",
            "Epoch 64/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0149\n",
            "Epoch 65/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0026 - val_loss: 0.0148\n",
            "Epoch 66/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0026 - val_loss: 0.0148\n",
            "Epoch 67/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0026 - val_loss: 0.0146\n",
            "Epoch 68/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0026 - val_loss: 0.0145\n",
            "Epoch 69/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0026 - val_loss: 0.0146\n",
            "Epoch 70/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0145\n",
            "Epoch 71/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0025 - val_loss: 0.0143\n",
            "Epoch 72/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0025 - val_loss: 0.0143\n",
            "Epoch 73/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0143\n",
            "Epoch 74/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0025 - val_loss: 0.0142\n",
            "Epoch 75/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0025 - val_loss: 0.0140\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 2 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 3s 11ms/step - loss: 0.1771 - val_loss: 0.0751\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0361 - val_loss: 0.0323\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0178 - val_loss: 0.0249\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0130 - val_loss: 0.0213\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0104 - val_loss: 0.0189\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0086 - val_loss: 0.0172\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0074 - val_loss: 0.0159\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0064 - val_loss: 0.0149\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0058 - val_loss: 0.0142\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0053 - val_loss: 0.0137\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0050 - val_loss: 0.0133\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0048 - val_loss: 0.0130\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0046 - val_loss: 0.0126\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0044 - val_loss: 0.0125\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0042 - val_loss: 0.0123\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0041 - val_loss: 0.0120\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0119\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0039 - val_loss: 0.0118\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0038 - val_loss: 0.0117\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0037 - val_loss: 0.0116\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0115\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0114\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0035 - val_loss: 0.0114\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0034 - val_loss: 0.0113\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0034 - val_loss: 0.0113\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0112\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0111\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0111\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0032 - val_loss: 0.0110\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0111\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0110\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0109\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0109\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0108\n",
            "Epoch 35/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0108\n",
            "Epoch 36/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0107\n",
            "Epoch 37/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0107\n",
            "Epoch 38/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0107\n",
            "Epoch 39/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0028 - val_loss: 0.0106\n",
            "Epoch 40/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0105\n",
            "Epoch 41/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0106\n",
            "Epoch 42/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0105\n",
            "Epoch 43/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0026 - val_loss: 0.0104\n",
            "Epoch 44/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0026 - val_loss: 0.0104\n",
            "Epoch 45/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0026 - val_loss: 0.0104\n",
            "Epoch 46/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0026 - val_loss: 0.0104\n",
            "Epoch 47/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0025 - val_loss: 0.0103\n",
            "Epoch 48/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0025 - val_loss: 0.0103\n",
            "Epoch 49/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0025 - val_loss: 0.0102\n",
            "Epoch 50/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0025 - val_loss: 0.0102\n",
            "Epoch 51/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0024 - val_loss: 0.0102\n",
            "Epoch 52/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0024 - val_loss: 0.0102\n",
            "Epoch 53/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0024 - val_loss: 0.0101\n",
            "Epoch 54/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0024 - val_loss: 0.0101\n",
            "Epoch 55/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0023 - val_loss: 0.0100\n",
            "Epoch 56/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0023 - val_loss: 0.0100\n",
            "Epoch 57/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0023 - val_loss: 0.0100\n",
            "Epoch 58/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0023 - val_loss: 0.0099\n",
            "Epoch 59/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0023 - val_loss: 0.0099\n",
            "Epoch 60/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0023 - val_loss: 0.0098\n",
            "Epoch 61/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0022 - val_loss: 0.0098\n",
            "Epoch 62/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0022 - val_loss: 0.0097\n",
            "Epoch 63/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0022 - val_loss: 0.0097\n",
            "Epoch 64/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0022 - val_loss: 0.0097\n",
            "Epoch 65/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0022 - val_loss: 0.0097\n",
            "Epoch 66/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0022 - val_loss: 0.0097\n",
            "Epoch 67/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0021 - val_loss: 0.0096\n",
            "Epoch 68/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0096\n",
            "Epoch 69/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0095\n",
            "Epoch 70/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0095\n",
            "Epoch 71/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0021 - val_loss: 0.0095\n",
            "Epoch 72/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0021 - val_loss: 0.0094\n",
            "Epoch 73/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0021 - val_loss: 0.0093\n",
            "Epoch 74/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0020 - val_loss: 0.0093\n",
            "Epoch 75/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0020 - val_loss: 0.0093\n",
            "6/6 [==============================] - 0s 3ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 3 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 3s 11ms/step - loss: 0.1387 - val_loss: 0.0858\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0546 - val_loss: 0.0489\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0307 - val_loss: 0.0361\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0220 - val_loss: 0.0305\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0180 - val_loss: 0.0275\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0155 - val_loss: 0.0253\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0137 - val_loss: 0.0234\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0122 - val_loss: 0.0217\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0110 - val_loss: 0.0204\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0100 - val_loss: 0.0192\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0092 - val_loss: 0.0181\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0085 - val_loss: 0.0172\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0079 - val_loss: 0.0165\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0074 - val_loss: 0.0159\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0069 - val_loss: 0.0154\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0065 - val_loss: 0.0150\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0062 - val_loss: 0.0146\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0059 - val_loss: 0.0144\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0057 - val_loss: 0.0142\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0055 - val_loss: 0.0140\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0054 - val_loss: 0.0139\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0052 - val_loss: 0.0138\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0051 - val_loss: 0.0137\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0050 - val_loss: 0.0136\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0049 - val_loss: 0.0136\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0048 - val_loss: 0.0136\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0047 - val_loss: 0.0135\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0046 - val_loss: 0.0135\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0046 - val_loss: 0.0135\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0045 - val_loss: 0.0134\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0044 - val_loss: 0.0133\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0044 - val_loss: 0.0133\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0043 - val_loss: 0.0133\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0043 - val_loss: 0.0132\n",
            "Epoch 35/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0042 - val_loss: 0.0132\n",
            "Epoch 36/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0042 - val_loss: 0.0131\n",
            "Epoch 37/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0041 - val_loss: 0.0130\n",
            "Epoch 38/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0041 - val_loss: 0.0131\n",
            "Epoch 39/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0130\n",
            "Epoch 40/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0130\n",
            "Epoch 41/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0039 - val_loss: 0.0129\n",
            "Epoch 42/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0039 - val_loss: 0.0129\n",
            "Epoch 43/75\n",
            "86/86 [==============================] - 0s 6ms/step - loss: 0.0039 - val_loss: 0.0129\n",
            "Epoch 44/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0038 - val_loss: 0.0129\n",
            "Epoch 45/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0038 - val_loss: 0.0128\n",
            "Epoch 46/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0037 - val_loss: 0.0129\n",
            "Epoch 47/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0037 - val_loss: 0.0128\n",
            "Epoch 48/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0127\n",
            "Epoch 49/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0128\n",
            "Epoch 50/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0036 - val_loss: 0.0127\n",
            "Epoch 51/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0127\n",
            "Epoch 52/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0035 - val_loss: 0.0127\n",
            "Epoch 53/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0035 - val_loss: 0.0127\n",
            "Epoch 54/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0035 - val_loss: 0.0126\n",
            "Epoch 55/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0035 - val_loss: 0.0126\n",
            "Epoch 56/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0034 - val_loss: 0.0126\n",
            "Epoch 57/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0034 - val_loss: 0.0125\n",
            "Epoch 58/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0034 - val_loss: 0.0125\n",
            "Epoch 59/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0033 - val_loss: 0.0125\n",
            "Epoch 60/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0125\n",
            "Epoch 61/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0125\n",
            "Epoch 62/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0125\n",
            "Epoch 63/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0032 - val_loss: 0.0125\n",
            "Epoch 64/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0032 - val_loss: 0.0124\n",
            "Epoch 65/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0032 - val_loss: 0.0124\n",
            "Epoch 66/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0124\n",
            "Epoch 67/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0032 - val_loss: 0.0123\n",
            "Epoch 68/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0031 - val_loss: 0.0123\n",
            "Epoch 69/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0031 - val_loss: 0.0123\n",
            "Epoch 70/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0031 - val_loss: 0.0123\n",
            "Epoch 71/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0122\n",
            "Epoch 72/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0031 - val_loss: 0.0122\n",
            "Epoch 73/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0030 - val_loss: 0.0122\n",
            "Epoch 74/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0122\n",
            "Epoch 75/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0030 - val_loss: 0.0122\n",
            "6/6 [==============================] - 0s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 4 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 3s 12ms/step - loss: 0.2262 - val_loss: 0.1456\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0991 - val_loss: 0.0767\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0533 - val_loss: 0.0518\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0360 - val_loss: 0.0429\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0287 - val_loss: 0.0388\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0246 - val_loss: 0.0361\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0215 - val_loss: 0.0338\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0187 - val_loss: 0.0314\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0163 - val_loss: 0.0292\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0144 - val_loss: 0.0272\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0129 - val_loss: 0.0255\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0116 - val_loss: 0.0241\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0105 - val_loss: 0.0229\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0096 - val_loss: 0.0217\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0088 - val_loss: 0.0208\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0082 - val_loss: 0.0200\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0076 - val_loss: 0.0192\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0072 - val_loss: 0.0186\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0068 - val_loss: 0.0180\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0064 - val_loss: 0.0176\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0061 - val_loss: 0.0171\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0059 - val_loss: 0.0167\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0057 - val_loss: 0.0165\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0055 - val_loss: 0.0161\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0053 - val_loss: 0.0158\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0051 - val_loss: 0.0156\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0050 - val_loss: 0.0154\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0049 - val_loss: 0.0152\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0048 - val_loss: 0.0150\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0047 - val_loss: 0.0149\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0046 - val_loss: 0.0148\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0045 - val_loss: 0.0147\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0044 - val_loss: 0.0146\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0044 - val_loss: 0.0145\n",
            "Epoch 35/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0043 - val_loss: 0.0144\n",
            "Epoch 36/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0042 - val_loss: 0.0144\n",
            "Epoch 37/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0042 - val_loss: 0.0144\n",
            "Epoch 38/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0041 - val_loss: 0.0144\n",
            "Epoch 39/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0041 - val_loss: 0.0144\n",
            "Epoch 40/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0040 - val_loss: 0.0145\n",
            "Epoch 41/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0040 - val_loss: 0.0143\n",
            "Epoch 42/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0039 - val_loss: 0.0143\n",
            "Epoch 43/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0039 - val_loss: 0.0143\n",
            "Epoch 44/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0038 - val_loss: 0.0143\n",
            "Epoch 45/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0038 - val_loss: 0.0143\n",
            "Epoch 46/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0142\n",
            "Epoch 47/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0142\n",
            "Epoch 48/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0036 - val_loss: 0.0142\n",
            "Epoch 49/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0036 - val_loss: 0.0141\n",
            "Epoch 50/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0036 - val_loss: 0.0141\n",
            "Epoch 51/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0035 - val_loss: 0.0140\n",
            "Epoch 52/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0035 - val_loss: 0.0140\n",
            "Epoch 53/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0035 - val_loss: 0.0139\n",
            "Epoch 54/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0034 - val_loss: 0.0140\n",
            "Epoch 55/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0034 - val_loss: 0.0140\n",
            "Epoch 56/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0034 - val_loss: 0.0139\n",
            "Epoch 57/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0139\n",
            "Epoch 58/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0033 - val_loss: 0.0139\n",
            "Epoch 59/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0033 - val_loss: 0.0139\n",
            "Epoch 60/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0033 - val_loss: 0.0138\n",
            "Epoch 61/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0032 - val_loss: 0.0138\n",
            "Epoch 62/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0032 - val_loss: 0.0138\n",
            "Epoch 63/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0032 - val_loss: 0.0138\n",
            "Epoch 64/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0032 - val_loss: 0.0136\n",
            "Epoch 65/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0031 - val_loss: 0.0137\n",
            "Epoch 66/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0031 - val_loss: 0.0136\n",
            "Epoch 67/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0031 - val_loss: 0.0137\n",
            "Epoch 68/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0136\n",
            "Epoch 69/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0136\n",
            "Epoch 70/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0136\n",
            "Epoch 71/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0030 - val_loss: 0.0136\n",
            "Epoch 72/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0135\n",
            "Epoch 73/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0136\n",
            "Epoch 74/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0029 - val_loss: 0.0135\n",
            "Epoch 75/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0135\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 5 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 3s 12ms/step - loss: 0.1288 - val_loss: 0.0514\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0347 - val_loss: 0.0240\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0183 - val_loss: 0.0199\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0142 - val_loss: 0.0187\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0122 - val_loss: 0.0176\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0107 - val_loss: 0.0169\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0095 - val_loss: 0.0162\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0086 - val_loss: 0.0157\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0079 - val_loss: 0.0152\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0073 - val_loss: 0.0148\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0068 - val_loss: 0.0145\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0064 - val_loss: 0.0143\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0060 - val_loss: 0.0141\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0058 - val_loss: 0.0139\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0055 - val_loss: 0.0138\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0053 - val_loss: 0.0136\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0051 - val_loss: 0.0136\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0050 - val_loss: 0.0135\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0048 - val_loss: 0.0135\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0047 - val_loss: 0.0134\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0046 - val_loss: 0.0134\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0045 - val_loss: 0.0135\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0044 - val_loss: 0.0132\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0043 - val_loss: 0.0133\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0043 - val_loss: 0.0133\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0042 - val_loss: 0.0132\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0041 - val_loss: 0.0131\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0041 - val_loss: 0.0131\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0040 - val_loss: 0.0130\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0039 - val_loss: 0.0129\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0039 - val_loss: 0.0129\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0038 - val_loss: 0.0129\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0038 - val_loss: 0.0129\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0128\n",
            "Epoch 35/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0128\n",
            "Epoch 36/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0036 - val_loss: 0.0128\n",
            "Epoch 37/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0036 - val_loss: 0.0128\n",
            "Epoch 38/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0035 - val_loss: 0.0127\n",
            "Epoch 39/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0035 - val_loss: 0.0126\n",
            "Epoch 40/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0034 - val_loss: 0.0127\n",
            "Epoch 41/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0034 - val_loss: 0.0127\n",
            "Epoch 42/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0034 - val_loss: 0.0127\n",
            "Epoch 43/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0033 - val_loss: 0.0126\n",
            "Epoch 44/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0126\n",
            "Epoch 45/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0125\n",
            "Epoch 46/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0126\n",
            "Epoch 47/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0125\n",
            "Epoch 48/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0124\n",
            "Epoch 49/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0031 - val_loss: 0.0125\n",
            "Epoch 50/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0125\n",
            "Epoch 51/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0031 - val_loss: 0.0125\n",
            "Epoch 52/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0123\n",
            "Epoch 53/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0123\n",
            "Epoch 54/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0123\n",
            "Epoch 55/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0123\n",
            "Epoch 56/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0122\n",
            "Epoch 57/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0122\n",
            "Epoch 58/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0122\n",
            "Epoch 59/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0123\n",
            "Epoch 60/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0028 - val_loss: 0.0122\n",
            "Epoch 61/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0122\n",
            "Epoch 62/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0122\n",
            "Epoch 63/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0122\n",
            "Epoch 64/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0121\n",
            "Epoch 65/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0121\n",
            "Epoch 66/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0121\n",
            "Epoch 67/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0026 - val_loss: 0.0121\n",
            "Epoch 68/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0026 - val_loss: 0.0120\n",
            "Epoch 69/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0026 - val_loss: 0.0120\n",
            "Epoch 70/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0026 - val_loss: 0.0120\n",
            "Epoch 71/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0026 - val_loss: 0.0119\n",
            "Epoch 72/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0025 - val_loss: 0.0119\n",
            "Epoch 73/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0025 - val_loss: 0.0119\n",
            "Epoch 74/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0025 - val_loss: 0.0119\n",
            "Epoch 75/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0025 - val_loss: 0.0118\n",
            "6/6 [==============================] - 0s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 6 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 3s 15ms/step - loss: 0.1596 - val_loss: 0.0766\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0552 - val_loss: 0.0355\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0301 - val_loss: 0.0253\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0220 - val_loss: 0.0214\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0179 - val_loss: 0.0189\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0152 - val_loss: 0.0172\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0132 - val_loss: 0.0160\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0118 - val_loss: 0.0150\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0106 - val_loss: 0.0142\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0098 - val_loss: 0.0138\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0091 - val_loss: 0.0133\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0085 - val_loss: 0.0130\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0081 - val_loss: 0.0129\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0077 - val_loss: 0.0129\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0074 - val_loss: 0.0128\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0071 - val_loss: 0.0128\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0069 - val_loss: 0.0128\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0067 - val_loss: 0.0128\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0065 - val_loss: 0.0128\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0063 - val_loss: 0.0129\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0061 - val_loss: 0.0130\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0059 - val_loss: 0.0132\n",
            "6/6 [==============================] - 0s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 7 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 3s 11ms/step - loss: 0.0977 - val_loss: 0.0300\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0132 - val_loss: 0.0167\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0064 - val_loss: 0.0154\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0053 - val_loss: 0.0149\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0048 - val_loss: 0.0143\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0045 - val_loss: 0.0139\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0042 - val_loss: 0.0134\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0041 - val_loss: 0.0133\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0039 - val_loss: 0.0131\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0038 - val_loss: 0.0131\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0037 - val_loss: 0.0130\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0037 - val_loss: 0.0130\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0130\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0035 - val_loss: 0.0129\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0034 - val_loss: 0.0129\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0034 - val_loss: 0.0128\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0128\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0129\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0127\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0127\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0127\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0127\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0126\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0126\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0126\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0028 - val_loss: 0.0125\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0028 - val_loss: 0.0125\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0028 - val_loss: 0.0125\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0124\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0122\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0123\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0122\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0026 - val_loss: 0.0122\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0026 - val_loss: 0.0122\n",
            "Epoch 35/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0026 - val_loss: 0.0122\n",
            "Epoch 36/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0026 - val_loss: 0.0121\n",
            "Epoch 37/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0025 - val_loss: 0.0120\n",
            "Epoch 38/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0119\n",
            "Epoch 39/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0025 - val_loss: 0.0119\n",
            "Epoch 40/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0025 - val_loss: 0.0119\n",
            "Epoch 41/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0025 - val_loss: 0.0118\n",
            "Epoch 42/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0024 - val_loss: 0.0118\n",
            "Epoch 43/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0024 - val_loss: 0.0118\n",
            "Epoch 44/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0024 - val_loss: 0.0116\n",
            "Epoch 45/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0024 - val_loss: 0.0117\n",
            "Epoch 46/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0024 - val_loss: 0.0117\n",
            "Epoch 47/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0024 - val_loss: 0.0116\n",
            "Epoch 48/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0023 - val_loss: 0.0115\n",
            "Epoch 49/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0023 - val_loss: 0.0115\n",
            "Epoch 50/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0023 - val_loss: 0.0114\n",
            "Epoch 51/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0023 - val_loss: 0.0114\n",
            "Epoch 52/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0023 - val_loss: 0.0113\n",
            "Epoch 53/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0023 - val_loss: 0.0112\n",
            "Epoch 54/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0023 - val_loss: 0.0112\n",
            "Epoch 55/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0022 - val_loss: 0.0112\n",
            "Epoch 56/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0022 - val_loss: 0.0111\n",
            "Epoch 57/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0022 - val_loss: 0.0110\n",
            "Epoch 58/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0022 - val_loss: 0.0110\n",
            "Epoch 59/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0022 - val_loss: 0.0110\n",
            "Epoch 60/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0022 - val_loss: 0.0109\n",
            "Epoch 61/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0022 - val_loss: 0.0109\n",
            "Epoch 62/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0021 - val_loss: 0.0109\n",
            "Epoch 63/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0021 - val_loss: 0.0108\n",
            "Epoch 64/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0021 - val_loss: 0.0106\n",
            "Epoch 65/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0021 - val_loss: 0.0107\n",
            "Epoch 66/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0106\n",
            "Epoch 67/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0105\n",
            "Epoch 68/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0104\n",
            "Epoch 69/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0104\n",
            "Epoch 70/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0020 - val_loss: 0.0104\n",
            "Epoch 71/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0103\n",
            "Epoch 72/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0103\n",
            "Epoch 73/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0103\n",
            "Epoch 74/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0103\n",
            "Epoch 75/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0101\n",
            "6/6 [==============================] - 0s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 8 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 3s 14ms/step - loss: 0.1743 - val_loss: 0.0649\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0383 - val_loss: 0.0263\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0173 - val_loss: 0.0201\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0126 - val_loss: 0.0184\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0105 - val_loss: 0.0173\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0092 - val_loss: 0.0164\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0081 - val_loss: 0.0157\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0072 - val_loss: 0.0151\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0066 - val_loss: 0.0145\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0061 - val_loss: 0.0141\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0057 - val_loss: 0.0139\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0054 - val_loss: 0.0135\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0051 - val_loss: 0.0132\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0049 - val_loss: 0.0129\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0047 - val_loss: 0.0128\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0045 - val_loss: 0.0127\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0044 - val_loss: 0.0125\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0043 - val_loss: 0.0124\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0042 - val_loss: 0.0123\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0041 - val_loss: 0.0122\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0122\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0039 - val_loss: 0.0121\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0039 - val_loss: 0.0121\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0038 - val_loss: 0.0120\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0038 - val_loss: 0.0120\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0037 - val_loss: 0.0119\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0037 - val_loss: 0.0118\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0036 - val_loss: 0.0118\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0036 - val_loss: 0.0118\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0035 - val_loss: 0.0117\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0035 - val_loss: 0.0117\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0034 - val_loss: 0.0117\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0034 - val_loss: 0.0116\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0034 - val_loss: 0.0115\n",
            "Epoch 35/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0033 - val_loss: 0.0115\n",
            "Epoch 36/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0115\n",
            "Epoch 37/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0033 - val_loss: 0.0115\n",
            "Epoch 38/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0032 - val_loss: 0.0115\n",
            "Epoch 39/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0032 - val_loss: 0.0115\n",
            "Epoch 40/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0032 - val_loss: 0.0114\n",
            "Epoch 41/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0031 - val_loss: 0.0114\n",
            "Epoch 42/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0031 - val_loss: 0.0114\n",
            "Epoch 43/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0031 - val_loss: 0.0114\n",
            "Epoch 44/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0031 - val_loss: 0.0114\n",
            "Epoch 45/75\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.0030 - val_loss: 0.0114\n",
            "Epoch 46/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0113\n",
            "Epoch 47/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0030 - val_loss: 0.0113\n",
            "Epoch 48/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0030 - val_loss: 0.0113\n",
            "Epoch 49/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0113\n",
            "Epoch 50/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0112\n",
            "Epoch 51/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0112\n",
            "Epoch 52/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0112\n",
            "Epoch 53/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0112\n",
            "Epoch 54/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0112\n",
            "Epoch 55/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0112\n",
            "Epoch 56/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0111\n",
            "Epoch 57/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0028 - val_loss: 0.0112\n",
            "Epoch 58/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0112\n",
            "Epoch 59/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0111\n",
            "Epoch 60/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0111\n",
            "Epoch 61/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0111\n",
            "Epoch 62/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0110\n",
            "Epoch 63/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0110\n",
            "Epoch 64/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0026 - val_loss: 0.0110\n",
            "Epoch 65/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0026 - val_loss: 0.0110\n",
            "Epoch 66/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0026 - val_loss: 0.0110\n",
            "Epoch 67/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0026 - val_loss: 0.0110\n",
            "Epoch 68/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0110\n",
            "Epoch 69/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0109\n",
            "Epoch 70/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0025 - val_loss: 0.0109\n",
            "Epoch 71/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0025 - val_loss: 0.0108\n",
            "Epoch 72/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0025 - val_loss: 0.0108\n",
            "Epoch 73/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0109\n",
            "Epoch 74/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0025 - val_loss: 0.0108\n",
            "Epoch 75/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0108\n",
            "6/6 [==============================] - 0s 6ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 8 batch_size and 9 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "86/86 [==============================] - 3s 12ms/step - loss: 0.1295 - val_loss: 0.0607\n",
            "Epoch 2/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0417 - val_loss: 0.0304\n",
            "Epoch 3/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0231 - val_loss: 0.0227\n",
            "Epoch 4/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0171 - val_loss: 0.0196\n",
            "Epoch 5/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0140 - val_loss: 0.0177\n",
            "Epoch 6/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0118 - val_loss: 0.0163\n",
            "Epoch 7/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0102 - val_loss: 0.0152\n",
            "Epoch 8/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0090 - val_loss: 0.0144\n",
            "Epoch 9/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0081 - val_loss: 0.0137\n",
            "Epoch 10/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0075 - val_loss: 0.0133\n",
            "Epoch 11/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0069 - val_loss: 0.0128\n",
            "Epoch 12/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0065 - val_loss: 0.0125\n",
            "Epoch 13/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0062 - val_loss: 0.0123\n",
            "Epoch 14/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0059 - val_loss: 0.0120\n",
            "Epoch 15/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0057 - val_loss: 0.0117\n",
            "Epoch 16/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0055 - val_loss: 0.0117\n",
            "Epoch 17/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0053 - val_loss: 0.0117\n",
            "Epoch 18/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0052 - val_loss: 0.0115\n",
            "Epoch 19/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0050 - val_loss: 0.0114\n",
            "Epoch 20/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0049 - val_loss: 0.0113\n",
            "Epoch 21/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0048 - val_loss: 0.0112\n",
            "Epoch 22/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0047 - val_loss: 0.0112\n",
            "Epoch 23/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0046 - val_loss: 0.0111\n",
            "Epoch 24/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0045 - val_loss: 0.0110\n",
            "Epoch 25/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0044 - val_loss: 0.0109\n",
            "Epoch 26/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0043 - val_loss: 0.0109\n",
            "Epoch 27/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0042 - val_loss: 0.0109\n",
            "Epoch 28/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0041 - val_loss: 0.0108\n",
            "Epoch 29/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0107\n",
            "Epoch 30/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0040 - val_loss: 0.0107\n",
            "Epoch 31/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0039 - val_loss: 0.0107\n",
            "Epoch 32/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0038 - val_loss: 0.0107\n",
            "Epoch 33/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0038 - val_loss: 0.0107\n",
            "Epoch 34/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0037 - val_loss: 0.0106\n",
            "Epoch 35/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0037 - val_loss: 0.0106\n",
            "Epoch 36/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0036 - val_loss: 0.0106\n",
            "Epoch 37/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0036 - val_loss: 0.0106\n",
            "Epoch 38/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0035 - val_loss: 0.0105\n",
            "Epoch 39/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0034 - val_loss: 0.0105\n",
            "Epoch 40/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0034 - val_loss: 0.0105\n",
            "Epoch 41/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0034 - val_loss: 0.0105\n",
            "Epoch 42/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0105\n",
            "Epoch 43/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0104\n",
            "Epoch 44/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0104\n",
            "Epoch 45/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0032 - val_loss: 0.0103\n",
            "Epoch 46/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0031 - val_loss: 0.0103\n",
            "Epoch 47/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0031 - val_loss: 0.0102\n",
            "Epoch 48/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0031 - val_loss: 0.0103\n",
            "Epoch 49/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0104\n",
            "Epoch 50/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0030 - val_loss: 0.0103\n",
            "Epoch 51/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0030 - val_loss: 0.0103\n",
            "Epoch 52/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0102\n",
            "Epoch 53/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0102\n",
            "Epoch 54/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0102\n",
            "Epoch 55/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0102\n",
            "Epoch 56/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0028 - val_loss: 0.0102\n",
            "Epoch 57/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0028 - val_loss: 0.0102\n",
            "Epoch 58/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0101\n",
            "Epoch 59/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0101\n",
            "Epoch 60/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0101\n",
            "Epoch 61/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0027 - val_loss: 0.0100\n",
            "Epoch 62/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0026 - val_loss: 0.0100\n",
            "Epoch 63/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0026 - val_loss: 0.0100\n",
            "Epoch 64/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0026 - val_loss: 0.0100\n",
            "Epoch 65/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0026 - val_loss: 0.0100\n",
            "Epoch 66/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0025 - val_loss: 0.0099\n",
            "Epoch 67/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0025 - val_loss: 0.0099\n",
            "Epoch 68/75\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.0025 - val_loss: 0.0100\n",
            "Epoch 69/75\n",
            "86/86 [==============================] - 1s 7ms/step - loss: 0.0025 - val_loss: 0.0099\n",
            "Epoch 70/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0025 - val_loss: 0.0099\n",
            "Epoch 71/75\n",
            "86/86 [==============================] - 1s 9ms/step - loss: 0.0024 - val_loss: 0.0099\n",
            "Epoch 72/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0024 - val_loss: 0.0099\n",
            "Epoch 73/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0024 - val_loss: 0.0099\n",
            "Epoch 74/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0024 - val_loss: 0.0098\n",
            "Epoch 75/75\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.0024 - val_loss: 0.0098\n",
            "6/6 [==============================] - 1s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 0 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 3s 18ms/step - loss: 0.1436 - val_loss: 0.0694\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0469 - val_loss: 0.0391\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0278 - val_loss: 0.0312\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0207 - val_loss: 0.0277\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0168 - val_loss: 0.0254\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0142 - val_loss: 0.0237\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0123 - val_loss: 0.0222\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0108 - val_loss: 0.0210\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0096 - val_loss: 0.0201\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0087 - val_loss: 0.0193\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0080 - val_loss: 0.0188\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0075 - val_loss: 0.0183\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0070 - val_loss: 0.0179\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0066 - val_loss: 0.0176\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0063 - val_loss: 0.0173\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0060 - val_loss: 0.0170\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0058 - val_loss: 0.0168\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0056 - val_loss: 0.0166\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0054 - val_loss: 0.0165\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0053 - val_loss: 0.0164\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0051 - val_loss: 0.0163\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0050 - val_loss: 0.0163\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0049 - val_loss: 0.0162\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0048 - val_loss: 0.0162\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0047 - val_loss: 0.0162\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0046 - val_loss: 0.0161\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0045 - val_loss: 0.0162\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0045 - val_loss: 0.0161\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0044 - val_loss: 0.0161\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0043 - val_loss: 0.0161\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0042 - val_loss: 0.0161\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0042 - val_loss: 0.0161\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0041 - val_loss: 0.0161\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0041 - val_loss: 0.0161\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0040 - val_loss: 0.0160\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0040 - val_loss: 0.0161\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0039 - val_loss: 0.0161\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0039 - val_loss: 0.0161\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0038 - val_loss: 0.0161\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0038 - val_loss: 0.0161\n",
            "6/6 [==============================] - 0s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 1 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 2s 17ms/step - loss: 0.1188 - val_loss: 0.0593\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0470 - val_loss: 0.0352\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0315 - val_loss: 0.0287\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0255 - val_loss: 0.0255\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0217 - val_loss: 0.0232\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0188 - val_loss: 0.0212\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0164 - val_loss: 0.0198\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0145 - val_loss: 0.0186\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0129 - val_loss: 0.0177\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0116 - val_loss: 0.0168\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0105 - val_loss: 0.0161\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0096 - val_loss: 0.0155\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0089 - val_loss: 0.0151\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0083 - val_loss: 0.0147\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0077 - val_loss: 0.0145\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0072 - val_loss: 0.0142\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0068 - val_loss: 0.0139\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0064 - val_loss: 0.0137\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0061 - val_loss: 0.0135\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0059 - val_loss: 0.0134\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0132\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0054 - val_loss: 0.0131\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0052 - val_loss: 0.0129\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0051 - val_loss: 0.0128\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0049 - val_loss: 0.0128\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0048 - val_loss: 0.0126\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0047 - val_loss: 0.0125\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0046 - val_loss: 0.0124\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0045 - val_loss: 0.0123\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0044 - val_loss: 0.0122\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0043 - val_loss: 0.0122\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0043 - val_loss: 0.0121\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0042 - val_loss: 0.0120\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0041 - val_loss: 0.0120\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0041 - val_loss: 0.0120\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0040 - val_loss: 0.0119\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0040 - val_loss: 0.0119\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0039 - val_loss: 0.0118\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0039 - val_loss: 0.0118\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0038 - val_loss: 0.0118\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0038 - val_loss: 0.0117\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0037 - val_loss: 0.0116\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0037 - val_loss: 0.0116\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0036 - val_loss: 0.0116\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0036 - val_loss: 0.0116\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0035 - val_loss: 0.0116\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0035 - val_loss: 0.0115\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0035 - val_loss: 0.0115\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0034 - val_loss: 0.0115\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0034 - val_loss: 0.0114\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0034 - val_loss: 0.0114\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0033 - val_loss: 0.0114\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0033 - val_loss: 0.0114\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0033 - val_loss: 0.0113\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0032 - val_loss: 0.0113\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0032 - val_loss: 0.0112\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0032 - val_loss: 0.0112\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0031 - val_loss: 0.0112\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0031 - val_loss: 0.0112\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0031 - val_loss: 0.0111\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0031 - val_loss: 0.0112\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0030 - val_loss: 0.0112\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0030 - val_loss: 0.0111\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0030 - val_loss: 0.0111\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0030 - val_loss: 0.0111\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0029 - val_loss: 0.0111\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0029 - val_loss: 0.0110\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0029 - val_loss: 0.0110\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0029 - val_loss: 0.0110\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0028 - val_loss: 0.0110\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0028 - val_loss: 0.0110\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0028 - val_loss: 0.0109\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0028 - val_loss: 0.0109\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0028 - val_loss: 0.0109\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0027 - val_loss: 0.0109\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 2 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 4s 22ms/step - loss: 0.2417 - val_loss: 0.1701\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.1494 - val_loss: 0.1145\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.1016 - val_loss: 0.0795\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0710 - val_loss: 0.0566\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0515 - val_loss: 0.0427\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0394 - val_loss: 0.0341\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0315 - val_loss: 0.0288\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0261 - val_loss: 0.0252\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0222 - val_loss: 0.0225\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0193 - val_loss: 0.0204\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0171 - val_loss: 0.0187\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0153 - val_loss: 0.0174\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0138 - val_loss: 0.0162\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0126 - val_loss: 0.0154\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0116 - val_loss: 0.0146\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0107 - val_loss: 0.0140\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0099 - val_loss: 0.0134\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0093 - val_loss: 0.0130\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0087 - val_loss: 0.0126\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0082 - val_loss: 0.0124\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0078 - val_loss: 0.0121\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0074 - val_loss: 0.0119\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0071 - val_loss: 0.0117\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0068 - val_loss: 0.0116\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0066 - val_loss: 0.0115\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0064 - val_loss: 0.0113\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0062 - val_loss: 0.0112\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0060 - val_loss: 0.0111\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0058 - val_loss: 0.0111\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0057 - val_loss: 0.0110\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0055 - val_loss: 0.0110\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0054 - val_loss: 0.0110\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0053 - val_loss: 0.0109\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0052 - val_loss: 0.0108\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0051 - val_loss: 0.0108\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0050 - val_loss: 0.0107\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0049 - val_loss: 0.0107\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0048 - val_loss: 0.0107\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0047 - val_loss: 0.0107\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0046 - val_loss: 0.0106\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0045 - val_loss: 0.0106\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0045 - val_loss: 0.0106\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0044 - val_loss: 0.0106\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0043 - val_loss: 0.0105\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0042 - val_loss: 0.0105\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0042 - val_loss: 0.0104\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0041 - val_loss: 0.0104\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0040 - val_loss: 0.0104\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0040 - val_loss: 0.0103\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0039 - val_loss: 0.0104\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0038 - val_loss: 0.0103\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0038 - val_loss: 0.0103\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0037 - val_loss: 0.0103\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0037 - val_loss: 0.0102\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0036 - val_loss: 0.0102\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0036 - val_loss: 0.0101\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0035 - val_loss: 0.0101\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0034 - val_loss: 0.0101\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0034 - val_loss: 0.0101\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0033 - val_loss: 0.0101\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0033 - val_loss: 0.0101\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0033 - val_loss: 0.0101\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0032 - val_loss: 0.0100\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0032 - val_loss: 0.0100\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0031 - val_loss: 0.0100\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0031 - val_loss: 0.0100\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0031 - val_loss: 0.0100\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0030 - val_loss: 0.0099\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0030 - val_loss: 0.0100\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0030 - val_loss: 0.0099\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0099\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0099\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0099\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0028 - val_loss: 0.0099\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0028 - val_loss: 0.0099\n",
            "6/6 [==============================] - 0s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 3 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 3s 17ms/step - loss: 0.1972 - val_loss: 0.1487\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.1155 - val_loss: 0.0966\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0730 - val_loss: 0.0672\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0475 - val_loss: 0.0488\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0301 - val_loss: 0.0367\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0194 - val_loss: 0.0294\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0138 - val_loss: 0.0253\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0109 - val_loss: 0.0226\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0090 - val_loss: 0.0205\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0077 - val_loss: 0.0190\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0067 - val_loss: 0.0178\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0060 - val_loss: 0.0169\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0055 - val_loss: 0.0162\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0050 - val_loss: 0.0157\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0047 - val_loss: 0.0152\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0044 - val_loss: 0.0149\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0042 - val_loss: 0.0146\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0040 - val_loss: 0.0144\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0039 - val_loss: 0.0141\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0038 - val_loss: 0.0139\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0037 - val_loss: 0.0138\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0036 - val_loss: 0.0136\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0035 - val_loss: 0.0135\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0035 - val_loss: 0.0134\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0034 - val_loss: 0.0133\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0034 - val_loss: 0.0132\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0033 - val_loss: 0.0131\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0033 - val_loss: 0.0130\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0032 - val_loss: 0.0129\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0032 - val_loss: 0.0129\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0032 - val_loss: 0.0128\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0032 - val_loss: 0.0127\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0031 - val_loss: 0.0127\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0031 - val_loss: 0.0126\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0031 - val_loss: 0.0125\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0031 - val_loss: 0.0125\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0030 - val_loss: 0.0125\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0030 - val_loss: 0.0124\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0030 - val_loss: 0.0124\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0030 - val_loss: 0.0123\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0030 - val_loss: 0.0123\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0030 - val_loss: 0.0123\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0029 - val_loss: 0.0122\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0122\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0122\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0029 - val_loss: 0.0121\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0121\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0121\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0121\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0028 - val_loss: 0.0120\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0028 - val_loss: 0.0120\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0028 - val_loss: 0.0120\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0028 - val_loss: 0.0119\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0028 - val_loss: 0.0119\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0028 - val_loss: 0.0119\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0028 - val_loss: 0.0119\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0028 - val_loss: 0.0118\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0027 - val_loss: 0.0118\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0027 - val_loss: 0.0118\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0027 - val_loss: 0.0117\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0027 - val_loss: 0.0117\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0027 - val_loss: 0.0117\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0027 - val_loss: 0.0117\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0027 - val_loss: 0.0117\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0027 - val_loss: 0.0116\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0116\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0116\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0026 - val_loss: 0.0116\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0115\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0115\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0115\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0026 - val_loss: 0.0115\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0115\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0026 - val_loss: 0.0114\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0025 - val_loss: 0.0114\n",
            "6/6 [==============================] - 0s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 4 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 4s 24ms/step - loss: 0.1696 - val_loss: 0.1004\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0707 - val_loss: 0.0548\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0380 - val_loss: 0.0388\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0250 - val_loss: 0.0325\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0193 - val_loss: 0.0294\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0162 - val_loss: 0.0276\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0141 - val_loss: 0.0261\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0125 - val_loss: 0.0247\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0112 - val_loss: 0.0235\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0101 - val_loss: 0.0225\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0091 - val_loss: 0.0217\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0084 - val_loss: 0.0211\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0078 - val_loss: 0.0206\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0073 - val_loss: 0.0200\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0068 - val_loss: 0.0195\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0064 - val_loss: 0.0192\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0061 - val_loss: 0.0188\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0058 - val_loss: 0.0185\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0182\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0054 - val_loss: 0.0180\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0052 - val_loss: 0.0178\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0050 - val_loss: 0.0176\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0049 - val_loss: 0.0173\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0048 - val_loss: 0.0171\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0046 - val_loss: 0.0169\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0045 - val_loss: 0.0168\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0044 - val_loss: 0.0166\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0044 - val_loss: 0.0165\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0043 - val_loss: 0.0164\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0042 - val_loss: 0.0163\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0041 - val_loss: 0.0162\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0041 - val_loss: 0.0161\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0040 - val_loss: 0.0160\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0040 - val_loss: 0.0160\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0039 - val_loss: 0.0159\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0039 - val_loss: 0.0158\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0038 - val_loss: 0.0158\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0038 - val_loss: 0.0157\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0038 - val_loss: 0.0156\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0037 - val_loss: 0.0156\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0037 - val_loss: 0.0155\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0036 - val_loss: 0.0155\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0036 - val_loss: 0.0155\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0036 - val_loss: 0.0154\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0036 - val_loss: 0.0154\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0035 - val_loss: 0.0154\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0035 - val_loss: 0.0153\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0035 - val_loss: 0.0153\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0034 - val_loss: 0.0153\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0034 - val_loss: 0.0152\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0034 - val_loss: 0.0153\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0033 - val_loss: 0.0152\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 12ms/step - loss: 0.0033 - val_loss: 0.0152\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0033 - val_loss: 0.0152\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0033 - val_loss: 0.0151\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0032 - val_loss: 0.0152\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0032 - val_loss: 0.0151\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0032 - val_loss: 0.0151\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0032 - val_loss: 0.0151\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0032 - val_loss: 0.0151\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0031 - val_loss: 0.0150\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0031 - val_loss: 0.0150\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0031 - val_loss: 0.0150\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0031 - val_loss: 0.0151\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0030 - val_loss: 0.0150\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0030 - val_loss: 0.0150\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0030 - val_loss: 0.0150\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0030 - val_loss: 0.0149\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0030 - val_loss: 0.0149\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0149\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0149\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0029 - val_loss: 0.0149\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0148\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0148\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0028 - val_loss: 0.0148\n",
            "6/6 [==============================] - 0s 5ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 5 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 2s 16ms/step - loss: 0.1381 - val_loss: 0.0984\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0700 - val_loss: 0.0604\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0426 - val_loss: 0.0441\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0301 - val_loss: 0.0363\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0236 - val_loss: 0.0322\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0198 - val_loss: 0.0296\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0172 - val_loss: 0.0277\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0154 - val_loss: 0.0261\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0138 - val_loss: 0.0247\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0126 - val_loss: 0.0234\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0115 - val_loss: 0.0222\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0105 - val_loss: 0.0211\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0096 - val_loss: 0.0202\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0089 - val_loss: 0.0193\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0082 - val_loss: 0.0185\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0077 - val_loss: 0.0178\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0071 - val_loss: 0.0172\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0067 - val_loss: 0.0166\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0063 - val_loss: 0.0161\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0059 - val_loss: 0.0156\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0152\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0054 - val_loss: 0.0148\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0051 - val_loss: 0.0145\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0049 - val_loss: 0.0142\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0047 - val_loss: 0.0139\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0045 - val_loss: 0.0136\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0044 - val_loss: 0.0134\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0042 - val_loss: 0.0132\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0041 - val_loss: 0.0130\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0040 - val_loss: 0.0128\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0039 - val_loss: 0.0126\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0038 - val_loss: 0.0124\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0037 - val_loss: 0.0123\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0036 - val_loss: 0.0122\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0035 - val_loss: 0.0121\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0035 - val_loss: 0.0120\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0034 - val_loss: 0.0119\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0034 - val_loss: 0.0118\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0033 - val_loss: 0.0117\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0033 - val_loss: 0.0117\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0032 - val_loss: 0.0116\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0032 - val_loss: 0.0116\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0031 - val_loss: 0.0115\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0031 - val_loss: 0.0115\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0031 - val_loss: 0.0114\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0030 - val_loss: 0.0114\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0030 - val_loss: 0.0113\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0030 - val_loss: 0.0113\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0030 - val_loss: 0.0113\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0113\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0112\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0112\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0029 - val_loss: 0.0112\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0028 - val_loss: 0.0112\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0028 - val_loss: 0.0111\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0028 - val_loss: 0.0111\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0028 - val_loss: 0.0111\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0028 - val_loss: 0.0111\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0028 - val_loss: 0.0111\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0027 - val_loss: 0.0110\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0027 - val_loss: 0.0110\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0027 - val_loss: 0.0110\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0027 - val_loss: 0.0110\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0027 - val_loss: 0.0110\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0027 - val_loss: 0.0110\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0026 - val_loss: 0.0109\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0026 - val_loss: 0.0109\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0026 - val_loss: 0.0109\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0109\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0026 - val_loss: 0.0109\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0026 - val_loss: 0.0109\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0026 - val_loss: 0.0109\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0025 - val_loss: 0.0108\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0025 - val_loss: 0.0108\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0025 - val_loss: 0.0108\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 6 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 4s 23ms/step - loss: 0.1659 - val_loss: 0.1045\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0667 - val_loss: 0.0605\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0379 - val_loss: 0.0442\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0264 - val_loss: 0.0369\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0208 - val_loss: 0.0328\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0175 - val_loss: 0.0302\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0153 - val_loss: 0.0283\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0136 - val_loss: 0.0267\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0122 - val_loss: 0.0253\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0111 - val_loss: 0.0241\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0101 - val_loss: 0.0231\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0094 - val_loss: 0.0222\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0088 - val_loss: 0.0215\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0082 - val_loss: 0.0208\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0077 - val_loss: 0.0202\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0073 - val_loss: 0.0198\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0070 - val_loss: 0.0194\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0067 - val_loss: 0.0190\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0064 - val_loss: 0.0187\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0062 - val_loss: 0.0184\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0060 - val_loss: 0.0181\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0059 - val_loss: 0.0179\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0057 - val_loss: 0.0177\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0175\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0055 - val_loss: 0.0173\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0054 - val_loss: 0.0171\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0053 - val_loss: 0.0170\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0053 - val_loss: 0.0169\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0052 - val_loss: 0.0167\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0051 - val_loss: 0.0166\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0050 - val_loss: 0.0165\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0050 - val_loss: 0.0164\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0049 - val_loss: 0.0163\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0049 - val_loss: 0.0162\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0048 - val_loss: 0.0161\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0048 - val_loss: 0.0161\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0048 - val_loss: 0.0160\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0047 - val_loss: 0.0159\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0047 - val_loss: 0.0159\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0046 - val_loss: 0.0158\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0046 - val_loss: 0.0158\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0046 - val_loss: 0.0157\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0045 - val_loss: 0.0157\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0045 - val_loss: 0.0157\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0045 - val_loss: 0.0156\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0044 - val_loss: 0.0156\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0044 - val_loss: 0.0156\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0044 - val_loss: 0.0155\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0043 - val_loss: 0.0155\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0043 - val_loss: 0.0155\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0043 - val_loss: 0.0154\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0043 - val_loss: 0.0154\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0042 - val_loss: 0.0154\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0042 - val_loss: 0.0153\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0042 - val_loss: 0.0153\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0042 - val_loss: 0.0153\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0041 - val_loss: 0.0153\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0041 - val_loss: 0.0152\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0041 - val_loss: 0.0152\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0041 - val_loss: 0.0152\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0040 - val_loss: 0.0152\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0040 - val_loss: 0.0152\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0040 - val_loss: 0.0151\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0040 - val_loss: 0.0151\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0039 - val_loss: 0.0151\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0039 - val_loss: 0.0151\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0039 - val_loss: 0.0151\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0039 - val_loss: 0.0150\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0039 - val_loss: 0.0150\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0038 - val_loss: 0.0150\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0038 - val_loss: 0.0150\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0038 - val_loss: 0.0150\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0038 - val_loss: 0.0150\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0038 - val_loss: 0.0149\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0038 - val_loss: 0.0149\n",
            "6/6 [==============================] - 0s 3ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 7 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 2s 16ms/step - loss: 0.1682 - val_loss: 0.1148\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0883 - val_loss: 0.0713\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0560 - val_loss: 0.0507\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0399 - val_loss: 0.0405\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0314 - val_loss: 0.0350\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0264 - val_loss: 0.0315\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0231 - val_loss: 0.0290\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0206 - val_loss: 0.0271\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0186 - val_loss: 0.0254\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0169 - val_loss: 0.0239\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0154 - val_loss: 0.0227\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0142 - val_loss: 0.0216\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0130 - val_loss: 0.0205\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0120 - val_loss: 0.0196\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0111 - val_loss: 0.0188\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0103 - val_loss: 0.0181\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0096 - val_loss: 0.0174\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0090 - val_loss: 0.0168\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0084 - val_loss: 0.0163\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0079 - val_loss: 0.0158\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0075 - val_loss: 0.0154\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0071 - val_loss: 0.0150\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0067 - val_loss: 0.0146\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0064 - val_loss: 0.0143\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0061 - val_loss: 0.0140\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0059 - val_loss: 0.0138\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0057 - val_loss: 0.0135\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0055 - val_loss: 0.0133\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0053 - val_loss: 0.0131\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0051 - val_loss: 0.0129\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0050 - val_loss: 0.0128\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0048 - val_loss: 0.0126\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0047 - val_loss: 0.0125\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0046 - val_loss: 0.0124\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0045 - val_loss: 0.0123\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0044 - val_loss: 0.0121\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0043 - val_loss: 0.0120\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0042 - val_loss: 0.0119\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0042 - val_loss: 0.0118\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0041 - val_loss: 0.0117\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0040 - val_loss: 0.0117\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0040 - val_loss: 0.0116\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0039 - val_loss: 0.0115\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0039 - val_loss: 0.0114\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0038 - val_loss: 0.0114\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0038 - val_loss: 0.0113\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0037 - val_loss: 0.0113\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0037 - val_loss: 0.0112\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0036 - val_loss: 0.0112\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0036 - val_loss: 0.0111\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0036 - val_loss: 0.0111\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0035 - val_loss: 0.0110\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0035 - val_loss: 0.0110\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0034 - val_loss: 0.0109\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0034 - val_loss: 0.0109\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0034 - val_loss: 0.0108\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0033 - val_loss: 0.0108\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0033 - val_loss: 0.0107\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0033 - val_loss: 0.0107\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0033 - val_loss: 0.0107\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0032 - val_loss: 0.0106\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0032 - val_loss: 0.0106\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0032 - val_loss: 0.0106\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0032 - val_loss: 0.0106\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0031 - val_loss: 0.0106\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0031 - val_loss: 0.0105\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0031 - val_loss: 0.0105\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0031 - val_loss: 0.0105\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0030 - val_loss: 0.0105\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0030 - val_loss: 0.0105\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0030 - val_loss: 0.0105\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0030 - val_loss: 0.0104\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0030 - val_loss: 0.0104\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0104\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0029 - val_loss: 0.0104\n",
            "6/6 [==============================] - 1s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 8 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 3s 21ms/step - loss: 0.2415 - val_loss: 0.1650\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.1204 - val_loss: 0.1007\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0768 - val_loss: 0.0693\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0536 - val_loss: 0.0517\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0405 - val_loss: 0.0411\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0325 - val_loss: 0.0345\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0273 - val_loss: 0.0300\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0237 - val_loss: 0.0268\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0210 - val_loss: 0.0243\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0189 - val_loss: 0.0224\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0171 - val_loss: 0.0207\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0156 - val_loss: 0.0193\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0143 - val_loss: 0.0180\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0131 - val_loss: 0.0169\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0121 - val_loss: 0.0159\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0112 - val_loss: 0.0151\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0104 - val_loss: 0.0143\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0097 - val_loss: 0.0137\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0091 - val_loss: 0.0131\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0086 - val_loss: 0.0126\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0081 - val_loss: 0.0121\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0077 - val_loss: 0.0117\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0074 - val_loss: 0.0114\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0071 - val_loss: 0.0110\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0067 - val_loss: 0.0107\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0064 - val_loss: 0.0104\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0061 - val_loss: 0.0101\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0059 - val_loss: 0.0099\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0057 - val_loss: 0.0097\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0055 - val_loss: 0.0095\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0054 - val_loss: 0.0094\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0053 - val_loss: 0.0093\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0052 - val_loss: 0.0092\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0051 - val_loss: 0.0091\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0050 - val_loss: 0.0091\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0049 - val_loss: 0.0090\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0048 - val_loss: 0.0090\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0047 - val_loss: 0.0089\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0047 - val_loss: 0.0089\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0046 - val_loss: 0.0088\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0045 - val_loss: 0.0088\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0045 - val_loss: 0.0087\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0044 - val_loss: 0.0087\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0044 - val_loss: 0.0087\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0043 - val_loss: 0.0087\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0043 - val_loss: 0.0086\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0042 - val_loss: 0.0086\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0042 - val_loss: 0.0086\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0041 - val_loss: 0.0086\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0041 - val_loss: 0.0086\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0040 - val_loss: 0.0085\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.0040 - val_loss: 0.0085\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0040 - val_loss: 0.0085\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0039 - val_loss: 0.0085\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0039 - val_loss: 0.0085\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0039 - val_loss: 0.0085\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0038 - val_loss: 0.0084\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0038 - val_loss: 0.0084\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0038 - val_loss: 0.0084\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0037 - val_loss: 0.0084\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0037 - val_loss: 0.0084\n",
            "Epoch 62/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0037 - val_loss: 0.0084\n",
            "Epoch 63/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0037 - val_loss: 0.0084\n",
            "Epoch 64/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0036 - val_loss: 0.0084\n",
            "Epoch 65/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0036 - val_loss: 0.0084\n",
            "Epoch 66/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0036 - val_loss: 0.0084\n",
            "Epoch 67/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0035 - val_loss: 0.0083\n",
            "Epoch 68/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0035 - val_loss: 0.0083\n",
            "Epoch 69/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0035 - val_loss: 0.0083\n",
            "Epoch 70/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0035 - val_loss: 0.0083\n",
            "Epoch 71/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0035 - val_loss: 0.0083\n",
            "Epoch 72/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0034 - val_loss: 0.0083\n",
            "Epoch 73/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0034 - val_loss: 0.0083\n",
            "Epoch 74/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0034 - val_loss: 0.0083\n",
            "Epoch 75/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0034 - val_loss: 0.0083\n",
            "6/6 [==============================] - 4s 4ms/step\n",
            "Running for Adagrad optimizer 0.001 learning_rate 16 batch_size and 9 replicate \n",
            "\n",
            "Epoch 1/75\n",
            "43/43 [==============================] - 4s 23ms/step - loss: 0.2902 - val_loss: 0.1825\n",
            "Epoch 2/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.1524 - val_loss: 0.1126\n",
            "Epoch 3/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0936 - val_loss: 0.0733\n",
            "Epoch 4/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0593 - val_loss: 0.0507\n",
            "Epoch 5/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0391 - val_loss: 0.0381\n",
            "Epoch 6/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0273 - val_loss: 0.0312\n",
            "Epoch 7/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0203 - val_loss: 0.0274\n",
            "Epoch 8/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0162 - val_loss: 0.0252\n",
            "Epoch 9/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0136 - val_loss: 0.0238\n",
            "Epoch 10/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0118 - val_loss: 0.0229\n",
            "Epoch 11/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0106 - val_loss: 0.0221\n",
            "Epoch 12/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0097 - val_loss: 0.0215\n",
            "Epoch 13/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0090 - val_loss: 0.0209\n",
            "Epoch 14/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0084 - val_loss: 0.0204\n",
            "Epoch 15/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0080 - val_loss: 0.0200\n",
            "Epoch 16/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0076 - val_loss: 0.0196\n",
            "Epoch 17/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0072 - val_loss: 0.0192\n",
            "Epoch 18/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0070 - val_loss: 0.0190\n",
            "Epoch 19/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0067 - val_loss: 0.0187\n",
            "Epoch 20/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0065 - val_loss: 0.0185\n",
            "Epoch 21/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0063 - val_loss: 0.0183\n",
            "Epoch 22/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0062 - val_loss: 0.0181\n",
            "Epoch 23/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0060 - val_loss: 0.0180\n",
            "Epoch 24/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0059 - val_loss: 0.0179\n",
            "Epoch 25/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0058 - val_loss: 0.0177\n",
            "Epoch 26/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0057 - val_loss: 0.0176\n",
            "Epoch 27/75\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.0056 - val_loss: 0.0175\n",
            "Epoch 28/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0055 - val_loss: 0.0174\n",
            "Epoch 29/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0054 - val_loss: 0.0173\n",
            "Epoch 30/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0054 - val_loss: 0.0172\n",
            "Epoch 31/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0053 - val_loss: 0.0172\n",
            "Epoch 32/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0052 - val_loss: 0.0171\n",
            "Epoch 33/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0052 - val_loss: 0.0171\n",
            "Epoch 34/75\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.0051 - val_loss: 0.0171\n",
            "Epoch 35/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0051 - val_loss: 0.0170\n",
            "Epoch 36/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0050 - val_loss: 0.0169\n",
            "Epoch 37/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0050 - val_loss: 0.0169\n",
            "Epoch 38/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0049 - val_loss: 0.0169\n",
            "Epoch 39/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0049 - val_loss: 0.0169\n",
            "Epoch 40/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0048 - val_loss: 0.0168\n",
            "Epoch 41/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0048 - val_loss: 0.0168\n",
            "Epoch 42/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0048 - val_loss: 0.0168\n",
            "Epoch 43/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0047 - val_loss: 0.0167\n",
            "Epoch 44/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0047 - val_loss: 0.0167\n",
            "Epoch 45/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0047 - val_loss: 0.0167\n",
            "Epoch 46/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0046 - val_loss: 0.0167\n",
            "Epoch 47/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0046 - val_loss: 0.0167\n",
            "Epoch 48/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0046 - val_loss: 0.0167\n",
            "Epoch 49/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0045 - val_loss: 0.0167\n",
            "Epoch 50/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0045 - val_loss: 0.0166\n",
            "Epoch 51/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0045 - val_loss: 0.0167\n",
            "Epoch 52/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0044 - val_loss: 0.0166\n",
            "Epoch 53/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0044 - val_loss: 0.0166\n",
            "Epoch 54/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0044 - val_loss: 0.0166\n",
            "Epoch 55/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0044 - val_loss: 0.0165\n",
            "Epoch 56/75\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.0043 - val_loss: 0.0165\n",
            "Epoch 57/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0043 - val_loss: 0.0165\n",
            "Epoch 58/75\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 0.0043 - val_loss: 0.0165\n",
            "Epoch 59/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0043 - val_loss: 0.0165\n",
            "Epoch 60/75\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.0042 - val_loss: 0.0165\n",
            "Epoch 61/75\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.0042 - val_loss: 0.0165\n",
            "6/6 [==============================] - 0s 4ms/step\n",
            "Best_hyper_parameters(LSTM): \n",
            " {'model': [128], 'optimizer': 'Adagrad', 'learning_rate': 0.01, 'batch_size': 16, 'best_avg_rmse': 2547.653445980603}\n",
            "all_avg_rmse(LSTM): \n",
            " [[[6042.88243702 5839.3664779  5555.66146962]\n",
            "  [4525.99615899 4015.63666804 3681.76839787]\n",
            "  [3448.92414704 3280.38554    3158.43764791]]\n",
            "\n",
            " [[3006.16899192 2903.55231598 2802.13395333]\n",
            "  [2697.78271831 2613.22913035 2547.65344598]\n",
            "  [2627.28201391 2727.36075894 2821.28198139]]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model': [128],\n",
              " 'optimizer': 'Adagrad',\n",
              " 'learning_rate': 0.01,\n",
              " 'batch_size': 16,\n",
              " 'best_avg_rmse': 2547.653445980603}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MUREOR3oV1g"
      },
      "source": [
        "## **Model 2: Tuning Hyperparameters of GRU Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcKmJiGDoZ8z"
      },
      "outputs": [],
      "source": [
        "def GRU_Hyper_Parameter_Tuning(layers, data, time_step, split, optimizers_names, learning_rates, batch_sizes, epochs, num_replicates = 2):\n",
        "  #======= creating training and test data===#\n",
        "  train_data, val_data = data_split(data, split)\n",
        "\n",
        "  num_features = train_data.shape[1]\n",
        "\n",
        "  min_train, max_train  = train_data[\"new_deaths\"].min(), train_data[\"new_deaths\"].max()\n",
        "  min_val, max_val   =    val_data[\"new_deaths\"].min(), val_data[\"new_deaths\"].max()\n",
        "\n",
        "  train_data_scaled  =  min_max_transform(train_data)\n",
        "  val_data_scaled    = min_max_transform(val_data)\n",
        "\n",
        "  X_train, y_train =   DatasetCreation(train_data_scaled, time_step)\n",
        "  X_val, y_val     =   DatasetCreation(val_data_scaled, time_step)\n",
        "\n",
        "  #========dealing with time series=========#\n",
        "\n",
        "  best_avg_rmse = 99999999999\n",
        "\n",
        "  collect_rmse = []\n",
        "\n",
        "  all_avg_rmse = np.zeros((len(optimizers_names), len(learning_rates), len(batch_sizes)))\n",
        "\n",
        "  best_hyper_parameters = {\"model\": layers, \"optimizer\": None, \"learning_rate\": None, \"batch_size\": None,\"best_avg_rmse\": None}\n",
        "\n",
        "  #hist_csv_file = 'model_history'+ str(time.time())+'.csv'\n",
        "\n",
        "  for opt in range(len(optimizers_names)):\n",
        "\n",
        "    for lr in range(len(learning_rates)):\n",
        "\n",
        "      for batch_size in range(len(batch_sizes)):\n",
        "\n",
        "        for i in range(num_replicates):\n",
        "\n",
        "          print(\"Running for \" + optimizers_names[opt] + \" optimizer \" + str(learning_rates[lr]) +  \" learning_rate \" +  str(batch_sizes[batch_size]) + \" batch_size and \" + str(i) +  \" replicate \" +  \"\\n\")\n",
        "\n",
        "          model = Build_GRU_Model(layers, time_step, num_features, optimizers_names[opt], learning_rate = learning_rates[lr], verbose = 0)\n",
        "\n",
        "          callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience= 5)\n",
        "\n",
        "          history = model.fit(X_train, y_train, batch_size = batch_sizes[batch_size], epochs= epochs, validation_data = (X_val, y_val), callbacks=[callback], verbose = 1)\n",
        "\n",
        "          #hist_df = pd.DataFrame(history.history)\n",
        "          #hist_df.to_csv(\"history\")\n",
        "          #with open(\"hist_csv_file.csv\", mode='w') as f:\n",
        "          #  hist_df.to_csv(f)\n",
        "\n",
        "          #==============Making predictions in original scale ==========\n",
        "          #train_pred  =  min_max_inverse_transform(model.predict(X_train).ravel(), min_train, max_train)\n",
        "          val_pred    =  min_max_inverse_transform(model.predict(X_val).ravel(), min_val, max_val)\n",
        "\n",
        "          #train_scores =  calculate_scores(min_max_inverse_transform(y_train, min_train, max_train),train_pred)\n",
        "          #scores =   calculate_scores(min_max_inverse_transform(y_val, min_val, max_val),val_pred)\n",
        "\n",
        "          collect_rmse.append(math.sqrt(mean_squared_error(min_max_inverse_transform(y_val, min_val, max_val),val_pred)))\n",
        "\n",
        "        avg_rmse = np.mean(np.array(collect_rmse))\n",
        "        all_avg_rmse[opt][lr][batch_size] = avg_rmse\n",
        "\n",
        "        if avg_rmse < best_avg_rmse:\n",
        "          best_avg_rmse = avg_rmse\n",
        "          best_hyper_parameters = {\"model\": layers,  \"optimizer\": optimizers_names[opt], \"learning_rate\": learning_rates[lr], \"batch_size\": batch_sizes[batch_size], \"best_avg_rmse\": best_avg_rmse}\n",
        "\n",
        "\n",
        "  output_dictionary = {\n",
        "      \"best_hyper_parameters\":  best_hyper_parameters,\n",
        "      \"all_avg_rmse\": all_avg_rmse\n",
        "       }\n",
        "\n",
        "  #writing output dictionary in the file\n",
        "\n",
        "  file_name = output_dir_path+ \"gru-\" + str(layers[0])+ \"N-hyperparameter_tuning__results\" + \".txt\"\n",
        "  write_dic_to_file(output_dictionary, file_name)\n",
        "\n",
        "  print(\"Best_hyper_parameters(GRU): \\n\", output_dictionary['best_hyper_parameters'])\n",
        "  print(\"all_avg_rmse(GRU): \\n\", output_dictionary['all_avg_rmse'])\n",
        "\n",
        "  return output_dictionary['best_hyper_parameters']\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLSZCSTAM70l"
      },
      "source": [
        "## **Case I: 8N-GRU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dmmULFtgBnS"
      },
      "outputs": [],
      "source": [
        "layers = [8]\n",
        "time_step = 5\n",
        "optimizers_names = ['Adam', 'Adagrad']\n",
        "learning_rates =  [0.1, 0.01, 0.001]\n",
        "batch_sizes =  [4, 8, 16]\n",
        "epochs = 3\n",
        "num_replicates = 2\n",
        "test_split = 0.2\n",
        "val_split = 0.2\n",
        "\n",
        "train_data, test_data = data_split(data, test_split)\n",
        "\n",
        "gru_N8_best_hyper_parameters = GRU_Hyper_Parameter_Tuning(layers, train_data, time_step, val_split, optimizers_names, learning_rates, batch_sizes, epochs = epochs, num_replicates = num_replicates)\n",
        "gru_N8_best_hyper_parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6Vczfa1YobQe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpLHSYnyNL45"
      },
      "source": [
        "## **Case II: 16N-GRU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cz6PD70_NOda"
      },
      "outputs": [],
      "source": [
        "layers = [16]\n",
        "time_step = 5\n",
        "optimizers_names = ['Adam', 'Adagrad']\n",
        "learning_rates =  [0.1, 0.01, 0.001]\n",
        "batch_sizes =  [4, 8, 16]\n",
        "epochs = 50\n",
        "num_replicates = 10\n",
        "test_split = 0.2\n",
        "val_split = 0.2\n",
        "\n",
        "train_data, test_data = data_split(data, test_split)\n",
        "\n",
        "gru_N16_best_hyper_parameters = GRU_Hyper_Parameter_Tuning(layers, train_data, time_step, val_split, optimizers_names, learning_rates, batch_sizes, epochs = epochs, num_replicates = num_replicates)\n",
        "gru_N16_best_hyper_parameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "792rSBFyNVZC"
      },
      "source": [
        "## **Case III: 32N-GRU**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "P6_W_n5kNXHC"
      },
      "outputs": [],
      "source": [
        "layers = [32]\n",
        "time_step = 5\n",
        "optimizers_names = ['Adam', 'Adagrad']\n",
        "learning_rates =  [0.1, 0.01, 0.001]\n",
        "batch_sizes =  [4, 8, 16]\n",
        "epochs = 50\n",
        "num_replicates = 10\n",
        "test_split = 0.2\n",
        "val_split = 0.2\n",
        "\n",
        "train_data, test_data = data_split(data, test_split)\n",
        "\n",
        "gru_N50_best_hyper_parameters = GRU_Hyper_Parameter_Tuning(layers, train_data, time_step, val_split, optimizers_names, learning_rates, batch_sizes, epochs = epochs, num_replicates = num_replicates)\n",
        "gru_N50_best_hyper_parameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MM5IxmxNlhu"
      },
      "source": [
        "## **Case IV: 64N-GRU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2bn6rSfONn73"
      },
      "outputs": [],
      "source": [
        "layers = [64]\n",
        "time_step = 5\n",
        "optimizers_names = ['Adam', 'Adagrad']\n",
        "learning_rates =  [0.1, 0.01, 0.001]\n",
        "batch_sizes =  [4, 8, 16]\n",
        "epochs = 50\n",
        "num_replicates = 10\n",
        "test_split = 0.2\n",
        "val_split = 0.2\n",
        "\n",
        "train_data, test_data = data_split(data, test_split)\n",
        "\n",
        "gru_N64_best_hyper_parameters = GRU_Hyper_Parameter_Tuning(layers, train_data, time_step, val_split, optimizers_names, learning_rates, batch_sizes, epochs = epochs, num_replicates = num_replicates)\n",
        "gru_N64_best_hyper_parameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxwZUPRwNvHY"
      },
      "source": [
        "## **Case V: 128N-GRU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qHnn8D9dNy3m"
      },
      "outputs": [],
      "source": [
        "layers = [128]\n",
        "time_step = 5\n",
        "optimizers_names = ['Adam', 'Adagrad']\n",
        "learning_rates =  [0.1, 0.01, 0.001]\n",
        "batch_sizes =  [4, 8, 16]\n",
        "epochs = 50\n",
        "num_replicates = 10\n",
        "test_split = 0.2\n",
        "val_split = 0.2\n",
        "\n",
        "train_data, test_data = data_split(data, test_split)\n",
        "\n",
        "gru_N128_best_hyper_parameters = GRU_Hyper_Parameter_Tuning(layers, train_data, time_step, val_split, optimizers_names, learning_rates, batch_sizes, epochs = epochs, num_replicates = num_replicates)\n",
        "gru_N128_best_hyper_parameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTXJu7BbZVGV"
      },
      "source": [
        "# **Implementing Deep Learning Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct6qRNAEtwO3"
      },
      "source": [
        "## **Model 1: Implementing LSTM Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbAtH3KenO9D"
      },
      "outputs": [],
      "source": [
        "\n",
        "def LSTM_Model(layers, hyper_parameters, data, time_step = 5, test_split = 0.2, epochs = 5,  num_replicates = 2):\n",
        "    #====== data transformation==========#\n",
        "    print(\"Progress: Performing data preparation steps.......\\n\")\n",
        "\n",
        "    #======= creating training and test data===#\n",
        "\n",
        "    train_data, test_data = data_split(data, test_split)\n",
        "\n",
        "    num_features = train_data.shape[1]\n",
        "\n",
        "    min_train, max_train  = train_data[\"new_deaths\"].min(), train_data[\"new_deaths\"].max()\n",
        "    min_test, max_test   =  test_data[\"new_deaths\"].min(), test_data[\"new_deaths\"].max()\n",
        "\n",
        "    train_data_scaled  =  min_max_transform(train_data)\n",
        "    test_data_scaled   = min_max_transform(test_data)\n",
        "\n",
        "\n",
        "    X_train, y_train  =   DatasetCreation(train_data_scaled, time_step)\n",
        "    X_test, y_test    =   DatasetCreation(test_data_scaled, time_step)\n",
        "\n",
        "    y_train_original  =  min_max_inverse_transform(y_train, min_train, max_train) #in original scale\n",
        "    y_test_original  =  min_max_inverse_transform(y_test, min_test, max_test) #in original scale\n",
        "\n",
        "\n",
        "    #============ arrays for collecting test scores ================#\n",
        "    rmse_array = np.zeros(num_replicates)\n",
        "    mape_array = np.zeros(num_replicates)\n",
        "    R_array    = np.zeros(num_replicates)\n",
        "    elapsed_time_array = np.zeros(num_replicates)\n",
        "\n",
        "    models_history = []\n",
        "    train_predictions = []\n",
        "    test_predictions = []\n",
        "\n",
        "    for i in range(num_replicates):\n",
        "\n",
        "      print(\"Program is running for %d replicate ----->\\n\" %i)\n",
        "\n",
        "      model = Build_LSTM_Model(layers, time_step, num_features, optimizer = hyper_parameters[0], learning_rate = hyper_parameters[1], verbose = 0)\n",
        "      callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience= 5)\n",
        "      # This callback will stop the training when there is no improvement in\n",
        "      # the loss for three consecutive epochs\n",
        "      start = time.time()\n",
        "      history = model.fit(X_train, y_train, batch_size = hyper_parameters[2], epochs= epochs, callbacks=[callback], verbose = 1)\n",
        "      end = time.time()\n",
        "      elapsed_time = end - start\n",
        "\n",
        "      models_history.append(history)\n",
        "\n",
        "\n",
        "      #==============Making train and test prediction in original scales ==========\n",
        "      train_pred   =  min_max_inverse_transform(model.predict(X_train).ravel(), min_train, max_train) #in original scale\n",
        "      test_pred    =  min_max_inverse_transform(model.predict(X_test).ravel(), min_test, max_test)\n",
        "\n",
        "      train_predictions.append(train_pred)\n",
        "      test_predictions.append(test_pred)\n",
        "\n",
        "      #============== Calculating performance scores==========\n",
        "      #train_scores =  calculate_scores(min_max_inverse_transform(y_train, min_train, max_train),train_pred)\n",
        "      scores =   calculate_scores(min_max_inverse_transform(y_test, min_test, max_test),test_pred)\n",
        "\n",
        "      rmse_array[i] =  scores['rmse']\n",
        "      mape_array[i] =  scores['mape']\n",
        "      R_array[i] = scores['R']\n",
        "      elapsed_time_array[i] = elapsed_time\n",
        "\n",
        "    min_index = rmse_array.argmin()\n",
        "    best_rmse = rmse_array[min_index]\n",
        "    mape_with_best_rmse = mape_array[min_index]\n",
        "    R_with_best_rmse =  R_array[min_index]\n",
        "    elapsed_time_with_best_rmse = elapsed_time_array[min_index]\n",
        "\n",
        "    train_predictions_with_best_rmse = train_predictions[min_index]\n",
        "    test_predictions_with_best_rmse = test_predictions[min_index]\n",
        "\n",
        "    loss_with_best_rmse = models_history[min_index].history['loss']\n",
        "\n",
        "    #val_loss_with_best_rmse = models_history[min_index].history['val_loss']\n",
        "\n",
        "\n",
        "    # Collecting important results========\n",
        "    performance_metrics =  {\n",
        "\n",
        "                        'scores': {'rmse': rmse_array,\n",
        "                                    'mape': mape_array,\n",
        "                                    'R': R_array,\n",
        "                                    'elapsed_time': elapsed_time_array\n",
        "                                    },\n",
        "\n",
        "                        'minimums': {'rmse': np.min(rmse_array),\n",
        "                                      'mape': np.min(mape_array),\n",
        "                                      'R': np.min(R_array),\n",
        "                                      'elapsed_time': np.min(elapsed_time_array)\n",
        "                                      },\n",
        "\n",
        "                        'avg_scores':  {'rmse': np.mean(rmse_array),\n",
        "                                        'mape': np.mean(mape_array),\n",
        "                                        'R': np.mean(R_array),\n",
        "                                        'elapsed_time': np.mean(elapsed_time_array)\n",
        "                                        },\n",
        "\n",
        "                          'stds':      { 'rmse': np.std(rmse_array),\n",
        "                                          'mape': np.std(mape_array),\n",
        "                                          'R': np.std(R_array),\n",
        "                                          'elapsed_time': np.std(elapsed_time_array)\n",
        "                                        },\n",
        "\n",
        "                        'maximums': {'rmse': np.max(rmse_array),\n",
        "                                     'mape': np.max(mape_array),\n",
        "                                     'R': np.max(R_array),\n",
        "                                     'elapsed_time': np.max(elapsed_time_array)\n",
        "                                     }\n",
        "\n",
        "                  }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    model_with_best_rmse = {\n",
        "\n",
        "                            'replicate': min_index,\n",
        "                            'rmse': best_rmse,\n",
        "                            #'mae': mae_with_best_rmse,\n",
        "                            'mape': mape_with_best_rmse,\n",
        "                            #'R2': R2_with_best_rmse,\n",
        "                            'R':  R_with_best_rmse,\n",
        "                            'elapsed_time': elapsed_time_with_best_rmse,\n",
        "                            'train_predictions':train_predictions_with_best_rmse,\n",
        "                            'test_predictions': test_predictions_with_best_rmse,\n",
        "                            #'y_train':y_train_original,\n",
        "                            #'y_test': y_test_original,\n",
        "                            'loss':loss_with_best_rmse,\n",
        "                             #'val_loss': val_loss_with_best_rmse\n",
        "                            }\n",
        "\n",
        "     #======= Collecting hyperparameters=============#\n",
        "    hyper_parameters = {'layers': layers,\n",
        "                        'model_specific_hyper_parameters': hyper_parameters,#additional best_hyper_parmeters for each models\n",
        "                       'epochs': epochs,\n",
        "                       'time_step':time_step,\n",
        "                       'num_replicates': num_replicates,\n",
        "                       'test_split':test_split\n",
        "                        }\n",
        "\n",
        "\n",
        "     #======= Collecting all the outputs together =============#\n",
        "    output_dictionary = {'hyper_parameters': hyper_parameters,\n",
        "                        'performance_metrics': performance_metrics,\n",
        "                         'best_model': model_with_best_rmse,\n",
        "                       }\n",
        "\n",
        "\n",
        "    #Save all scores in a file for statistical study\n",
        "    #pd.DataFrame(performance_metrics['scores']['rmse']).to_csv(output_dir_path+'lstm-'+ str(layers)+'-rmse.csv')\n",
        "    #pd.DataFrame(performance_metrics['scores']['mape']).to_csv(output_dir_path+'lstm-'+ str(layers)+'-mape.csv')\n",
        "    #pd.DataFrame(performance_metrics['scores']['R']).to_csv(output_dir_path+'lstm-'+ str(layers)+'-R.csv')\n",
        "\n",
        "\n",
        "\n",
        "    #Save data sets\n",
        "\n",
        "    pd.DataFrame(y_train_original).to_csv(output_dir_path+'y_train.csv')\n",
        "    pd.DataFrame(y_test_original).to_csv(output_dir_path+'y_test.csv')\n",
        "\n",
        "    #Save best model results\n",
        "\n",
        "    #pd.DataFrame(model_with_best_rmse['train_predictions']).to_csv(output_dir_path+'best-lstm-'+ str(layers)+'-train_predictions.csv')\n",
        "    #pd.DataFrame(model_with_best_rmse['test_predictions']).to_csv(output_dir_path +'best-lstm-'+ str(layers)+'-test_predictions.csv')\n",
        "    #pd.DataFrame(model_with_best_rmse['loss']).to_csv(output_dir_path+ 'best-lstm-'+ str(layers)+'-loss.csv')\n",
        "\n",
        "    #model_with_best_rmse['val_loss'].to_csv(output_dir_path+'best-lstm-model-val-loss.csv')\n",
        "\n",
        "    #writing all statistics\n",
        "    #write_dic_to_file(performance_metrics,  output_dir_path + 'lstm-'+ str(layers)+'-performance_metrics.txt')\n",
        "\n",
        "    #writing output dictionary in the file\n",
        "    #file_name = output_dir_path +'lstm-'+ str(layers)+'-full-results.txt'\n",
        "    #write_dic_to_file(output_dictionary, file_name)\n",
        "\n",
        "    print(\"Progress: All works are done successfully, congratulations!!\\n\")\n",
        "    return output_dictionary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UmBWebJNaOL"
      },
      "source": [
        "### **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qPxp_KqHPoL2",
        "outputId": "dc75b96b-4aad-4757-bcbe-efc401ce21d2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nhyper_parameters = ['Adam', 0.01, 4]\\n\\nlayers = [10]\\n\\nlstm_output = LSTM_Model(layers, hyper_parameters, data, time_step = 5, test_split = 0.2, epochs = 3,  num_replicates = 2) \\n\""
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "hyper_parameters = ['Adam', 0.01, 4]\n",
        "\n",
        "layers = [10]\n",
        "\n",
        "lstm_output = LSTM_Model(layers, hyper_parameters, data, time_step = 5, test_split = 0.2, epochs = 3,  num_replicates = 2)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U53Lg2bWBM9z"
      },
      "source": [
        "## **Model 2: Implementing GRU Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7URSTacBPnD"
      },
      "outputs": [],
      "source": [
        "def GRU_Model(layers, hyper_parameters, data, time_step = 5, test_split = 0.2, epochs = 5,  num_replicates = 2):\n",
        "    #====== data transformation==========#\n",
        "    print(\"Progress: Performing data preparation steps.......\\n\")\n",
        "\n",
        "    #======= creating training and test data===#\n",
        "\n",
        "    train_data, test_data = data_split(data, test_split)\n",
        "\n",
        "    num_features = train_data.shape[1]\n",
        "\n",
        "    min_train, max_train  = train_data[\"new_deaths\"].min(), train_data[\"new_deaths\"].max()\n",
        "    min_test, max_test   =  test_data[\"new_deaths\"].min(), test_data[\"new_deaths\"].max()\n",
        "\n",
        "    train_data_scaled  =  min_max_transform(train_data)\n",
        "    test_data_scaled   = min_max_transform(test_data)\n",
        "\n",
        "\n",
        "    X_train, y_train  =   DatasetCreation(train_data_scaled, time_step)\n",
        "    X_test, y_test    =   DatasetCreation(test_data_scaled, time_step)\n",
        "\n",
        "    y_train_original  =  min_max_inverse_transform(y_train, min_train, max_train) #in original scale\n",
        "    y_test_original  =  min_max_inverse_transform(y_test, min_test, max_test) #in original scale\n",
        "\n",
        "\n",
        "    #============ arrays for collecting test scores ================#\n",
        "    rmse_array = np.zeros(num_replicates)\n",
        "    mape_array = np.zeros(num_replicates)\n",
        "    R_array    = np.zeros(num_replicates)\n",
        "    elapsed_time_array = np.zeros(num_replicates)\n",
        "\n",
        "    models_history = []\n",
        "    train_predictions = []\n",
        "    test_predictions = []\n",
        "\n",
        "    for i in range(num_replicates):\n",
        "\n",
        "      print(\"Program is running for %d replicate ----->\\n\" %i)\n",
        "\n",
        "      model = Build_GRU_Model(layers, time_step, num_features, optimizer = hyper_parameters[0], learning_rate = hyper_parameters[1], verbose = 0)\n",
        "      callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience= 5)\n",
        "      # This callback will stop the training when there is no improvement in\n",
        "      # the loss for three consecutive epochs\n",
        "      start = time.time()\n",
        "      history = model.fit(X_train, y_train, batch_size = hyper_parameters[2], epochs= epochs, callbacks=[callback], verbose = 1)\n",
        "      end = time.time()\n",
        "      elapsed_time = end - start\n",
        "\n",
        "      models_history.append(history)\n",
        "\n",
        "\n",
        "      #==============Making train and test prediction in original scales ==========\n",
        "      train_pred   =  min_max_inverse_transform(model.predict(X_train).ravel(), min_train, max_train) #in original scale\n",
        "      test_pred    =  min_max_inverse_transform(model.predict(X_test).ravel(), min_test, max_test)\n",
        "\n",
        "      train_predictions.append(train_pred)\n",
        "      test_predictions.append(test_pred)\n",
        "\n",
        "      #============== Calculating performance scores==========\n",
        "      #train_scores =  calculate_scores(min_max_inverse_transform(y_train, min_train, max_train),train_pred)\n",
        "      scores =   calculate_scores(min_max_inverse_transform(y_test, min_test, max_test),test_pred)\n",
        "\n",
        "      rmse_array[i] =  scores['rmse']\n",
        "      mape_array[i] =  scores['mape']\n",
        "      R_array[i] = scores['R']\n",
        "      elapsed_time_array[i] = elapsed_time\n",
        "\n",
        "    min_index = rmse_array.argmin()\n",
        "    best_rmse = rmse_array[min_index]\n",
        "    mape_with_best_rmse = mape_array[min_index]\n",
        "    R_with_best_rmse =  R_array[min_index]\n",
        "    elapsed_time_with_best_rmse = elapsed_time_array[min_index]\n",
        "\n",
        "    train_predictions_with_best_rmse = train_predictions[min_index]\n",
        "    test_predictions_with_best_rmse = test_predictions[min_index]\n",
        "\n",
        "    loss_with_best_rmse = models_history[min_index].history['loss']\n",
        "\n",
        "    #val_loss_with_best_rmse = models_history[min_index].history['val_loss']\n",
        "\n",
        "    # Collecting important results========\n",
        "    performance_metrics =  {\n",
        "\n",
        "                        'scores': {'rmse': rmse_array,\n",
        "                                    'mape': mape_array,\n",
        "                                    'R': R_array,\n",
        "                                    'elapsed_time': elapsed_time_array\n",
        "                                    },\n",
        "\n",
        "                        'minimums': {'rmse': np.min(rmse_array),\n",
        "                                      'mape': np.min(mape_array),\n",
        "                                      'R': np.min(R_array),\n",
        "                                      'elapsed_time': np.min(elapsed_time_array)\n",
        "                                      },\n",
        "\n",
        "                        'avg_scores':  {'rmse': np.mean(rmse_array),\n",
        "                                        'mape': np.mean(mape_array),\n",
        "                                        'R': np.mean(R_array),\n",
        "                                        'elapsed_time': np.mean(elapsed_time_array)\n",
        "                                        },\n",
        "\n",
        "                          'stds':      { 'rmse': np.std(rmse_array),\n",
        "                                          'mape': np.std(mape_array),\n",
        "                                          'R': np.std(R_array),\n",
        "                                          'elapsed_time': np.std(elapsed_time_array)\n",
        "                                        },\n",
        "\n",
        "                        'maximums': {'rmse': np.max(rmse_array),\n",
        "                                     'mape': np.max(mape_array),\n",
        "                                     'R': np.max(R_array),\n",
        "                                     'elapsed_time': np.max(elapsed_time_array)\n",
        "                                     }\n",
        "\n",
        "                  }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    model_with_best_rmse = {\n",
        "\n",
        "                            'replicate': min_index,\n",
        "                            'rmse': best_rmse,\n",
        "                            #'mae': mae_with_best_rmse,\n",
        "                            'mape': mape_with_best_rmse,\n",
        "                            #'R2': R2_with_best_rmse,\n",
        "                            'R':  R_with_best_rmse,\n",
        "                            'elapsed_time': elapsed_time_with_best_rmse,\n",
        "                            'train_predictions':train_predictions_with_best_rmse,\n",
        "                            'test_predictions': test_predictions_with_best_rmse,\n",
        "                            #'y_train':y_train_original,\n",
        "                            #'y_test': y_test_original,\n",
        "                            'loss':loss_with_best_rmse,\n",
        "                             #'val_loss': val_loss_with_best_rmse\n",
        "                            }\n",
        "\n",
        "     #======= Collecting hyperparameters=============#\n",
        "    hyper_parameters = {'layers': layers,\n",
        "                        'model_specific_hyper_parameters': hyper_parameters,#additional best_hyper_parmeters for each models\n",
        "                       'epochs': epochs,\n",
        "                       'time_step':time_step,\n",
        "                       'num_replicates': num_replicates,\n",
        "                       'test_split':test_split\n",
        "                        }\n",
        "\n",
        "\n",
        "\n",
        "    #datasets  =   {'data': data,\n",
        "     #             'X_train': X_train,\n",
        "     #             'X_test': X_test,\n",
        "      #            'y_train': y_train_original,\n",
        "     #             'y_test': y_test_original\n",
        "      #            }\n",
        "     #======= Collecting all the outputs together =============#\n",
        "    output_dictionary = {'hyper_parameters': hyper_parameters,\n",
        "                        'performance_metrics': performance_metrics,\n",
        "                         'best_model': model_with_best_rmse,\n",
        "                       }\n",
        "\n",
        "\n",
        "    #Save all scores in a file for statistical study\n",
        "    #pd.DataFrame(performance_metrics['scores']['rmse']).to_csv(output_dir_path+'gru-'+ str(layers)+'-rmse.csv')\n",
        "    #pd.DataFrame(performance_metrics['scores']['mape']).to_csv(output_dir_path+'gru-'+ str(layers)+'-mape.csv')\n",
        "    #pd.DataFrame(performance_metrics['scores']['R']).to_csv(output_dir_path+'gru-'+ str(layers)+'-R.csv')\n",
        "\n",
        "\n",
        "\n",
        "    #Save data sets\n",
        "\n",
        "    pd.DataFrame(y_train_original).to_csv(output_dir_path+'y_train.csv')\n",
        "    pd.DataFrame(y_test_original).to_csv(output_dir_path+'y_test.csv')\n",
        "\n",
        "    #Save best model results\n",
        "\n",
        "    #pd.DataFrame(model_with_best_rmse['train_predictions']).to_csv(output_dir_path+'best-gru-'+ str(layers)+'-train_predictions.csv')\n",
        "    #pd.DataFrame(model_with_best_rmse['test_predictions']).to_csv(output_dir_path +'best-gru-'+ str(layers)+'-test_predictions.csv')\n",
        "    #pd.DataFrame(model_with_best_rmse['loss']).to_csv(output_dir_path+ 'best-gru-'+ str(layers)+'-loss.csv')\n",
        "\n",
        "    #model_with_best_rmse['val_loss'].to_csv(output_dir_path+'best-lstm-model-val-loss.csv')\n",
        "\n",
        "    #writing all statistics\n",
        "    #write_dic_to_file(performance_metrics,  output_dir_path + 'gru-'+ str(layers)+'-performance_metrics.txt')\n",
        "\n",
        "    #writing output dictionary in the file\n",
        "    #file_name = output_dir_path +'gru-'+ str(layers)+'-full-results.txt'\n",
        "    #write_dic_to_file(output_dictionary, file_name)\n",
        "    print(\"Progress: All works are done successfully, congratulations!!\\n\")\n",
        "    return output_dictionary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV0LYECZYBjj"
      },
      "source": [
        "### **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0fXACOVHYGiZ",
        "outputId": "a80cd417-a112-4750-ef9a-43ce48ad23ff"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nhyper_parameters = ['Adam', 0.01, 4]\\n\\nlayers = [10]\\n\\ngru_output = GRU_Model(layers, hyper_parameters, data, time_step = 5, test_split = 0.2, epochs = 3,  num_replicates = 2) \\n\\n\\n\""
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "hyper_parameters = ['Adam', 0.01, 4]\n",
        "\n",
        "layers = [10]\n",
        "\n",
        "gru_output = GRU_Model(layers, hyper_parameters, data, time_step = 5, test_split = 0.2, epochs = 3,  num_replicates = 2)\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lxf_bvruYFIK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mqVZGeTDCY5"
      },
      "source": [
        "# **Executing Multiple Deep Learning Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pqni7frZ02d"
      },
      "source": [
        "## **Executing Multiple LSTM Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GugZupYxeSc"
      },
      "outputs": [],
      "source": [
        "#hidden_layers = [[10, 5], [20, 10], [50, 20], [100, 50], [150, 100], [100, 50, 20]]\n",
        "\n",
        "def Multiple_LSTM_Models(hidden_layers, hyper_parameters, data, time_step = 5, test_split = 0.2, epochs = 5,  num_replicates = 2):\n",
        "\n",
        "  num_models = len(hidden_layers)\n",
        "\n",
        "  #== to collect all scores===#\n",
        "  rmse = []\n",
        "  mape = []\n",
        "  R = []\n",
        "  elapsed_time = []\n",
        "\n",
        "  #===to collect all avg scores===#\n",
        "  avg_rmse = []\n",
        "  avg_mape = []\n",
        "  avg_R = []\n",
        "  avg_elapsed_time = []\n",
        "\n",
        "  #=== to iteratively update the best rmse and the corresponding model\n",
        "  best_avg_rmse = 99999999999\n",
        "  best_rmse = 99999999999\n",
        "  best_model_hidden_layers = None\n",
        "  best_model_output = None\n",
        "\n",
        "  for i in range(num_models):\n",
        "    print(\"Running model with hidden neurons: \", hidden_layers[i])\n",
        "\n",
        "    print(\"\\n\")\n",
        "\n",
        "    print(\"Best Hyper_parameters used: \", hyper_parameters[i])\n",
        "\n",
        "    print(\"\\n\")\n",
        "\n",
        "    output = LSTM_Model(hidden_layers[i], hyper_parameters[i], data, time_step, test_split, epochs, num_replicates)\n",
        "\n",
        "    rmse.append(output['performance_metrics']['scores']['rmse'])\n",
        "    mape.append(output['performance_metrics']['scores']['mape'])\n",
        "    R.append(output['performance_metrics']['scores']['R'])\n",
        "    elapsed_time.append(output['performance_metrics']['scores']['elapsed_time'])\n",
        "\n",
        "    avg_rmse.append(output['performance_metrics']['avg_scores']['rmse'])\n",
        "    avg_mape.append(output['performance_metrics']['avg_scores']['mape'])\n",
        "    avg_R.append(output['performance_metrics']['avg_scores']['R'])\n",
        "    avg_elapsed_time.append(output['performance_metrics']['avg_scores']['elapsed_time'])\n",
        "\n",
        "    if avg_rmse[i] < best_avg_rmse:\n",
        "      best_avg_rmse = avg_rmse[i]\n",
        "      best_rmse = output['best_model']['rmse']\n",
        "      best_model_hidden_layers = hidden_layers[i]\n",
        "      best_model_output = output\n",
        "\n",
        "\n",
        "  rmse = np.array(rmse)\n",
        "  mape = np.array(mape)\n",
        "  R =  np.array(R)\n",
        "\n",
        "  # ===== Collecting all  scores================================#\n",
        "\n",
        "  performance_metrics = {\n",
        "\n",
        "       'scores':  {'layers': hidden_layers,\n",
        "                   'rmse': rmse,\n",
        "                   'mape': mape,\n",
        "                   'R':R,\n",
        "                   'elapsed_time': elapsed_time },\n",
        "\n",
        "       'avg_scores':  pd.DataFrame({'layers': hidden_layers,\n",
        "                                    'rmse': np.array(avg_rmse),\n",
        "                                    'mape': np.array(avg_mape), 'R':np.array(avg_R),\n",
        "                                    'elapsed_time':np.array(avg_elapsed_time)}),\n",
        "\n",
        "       'stds':     pd.DataFrame({'layers': hidden_layers,\n",
        "                                 'rmse': np.std(rmse, axis = 1),\n",
        "                                 'mape': np.std(mape, axis = 1),\n",
        "                                 'R':  np.std(R, axis = 1 ),\n",
        "                                 'elapsed_time': np.std(elapsed_time, axis = 1 )}),\n",
        "       'minimums': pd.DataFrame({'layers': hidden_layers,\n",
        "                                'rmse': np.min(rmse, axis =1 ),\n",
        "                                'mape': np.min(mape, axis= 1),\n",
        "                                'R': np.min(R, axis =1),\n",
        "                                'elapsed_time': np.min(elapsed_time, axis =1)}),\n",
        "\n",
        "       'maximums': pd.DataFrame({'layers': hidden_layers,\n",
        "                                'rmse': np.max(rmse, axis =1),\n",
        "                                'mape': np.max(mape, axis =1),\n",
        "                                'R': np.max(R, axis =1),\n",
        "                                'elapsed_time': np.max(elapsed_time,axis =1)})\n",
        "    }\n",
        "\n",
        "\n",
        "  output_dictionary = {\n",
        "                     'hyper_parameters': hyper_parameters,\n",
        "\n",
        "                      'best_avg_rmse': best_avg_rmse,\n",
        "                      'best_rmse': best_rmse,\n",
        "                      'best_model_hidden_layers': best_model_hidden_layers,\n",
        "                      'best_model_output': best_model_output\n",
        "                      }\n",
        "\n",
        "  #Save all statistics:\n",
        "  performance_metrics['avg_scores'].to_csv(output_dir_path+'multiple_lstm_models_average_scores.csv')\n",
        "  performance_metrics['stds'].to_csv(output_dir_path+'multiple_lstm_models_stds.csv')\n",
        "  performance_metrics['minimums'].to_csv(output_dir_path+'multiple_lstm_models_minimums.csv')\n",
        "  performance_metrics['maximums'].to_csv(output_dir_path+'multiple_lstm_models_maximums.csv')\n",
        "\n",
        "\n",
        "  #Save all scores in the file for future analysis\n",
        "  pd.DataFrame(performance_metrics['scores']['rmse']).to_csv(output_dir_path+'multiple_lstm_models_all_rmse.csv')\n",
        "  pd.DataFrame(performance_metrics['scores']['mape']).to_csv(output_dir_path+'multiple_lstm_models_all_mape.csv')\n",
        "  pd.DataFrame(performance_metrics['scores']['R']).to_csv(output_dir_path+'multiple_lstm_models_all_R.csv')\n",
        "\n",
        "  #Save best model results\n",
        "  pd.DataFrame(best_model_output['best_model']['loss']).to_csv(output_dir_path+'best_lstm_model_loss.csv')\n",
        "  pd.DataFrame(best_model_output['best_model']['train_predictions']).to_csv(output_dir_path+'best_lstm_model_train_predictions.csv')\n",
        "  pd.DataFrame(best_model_output['best_model']['test_predictions']).to_csv(output_dir_path+'best_lstm_model_test_predictions.csv')\n",
        "  pd.DataFrame(best_model_output['performance_metrics']['scores']['rmse']).to_csv(output_dir_path+'best_lstm_model_all_rmse.csv')\n",
        "\n",
        "  #writing all result in the file\n",
        "  write_dic_to_file(output_dictionary, output_dir_path + \"multiple_lstm_models_full_results.txt\")\n",
        "\n",
        "  #Display some key results in the screen\n",
        "  print(\"\\nBest model and its avg rmse and minimum rmse):\\n\", best_model_hidden_layers, best_avg_rmse, best_rmse)\n",
        "  print(\"Hyper_parameters:\\n\", hyper_parameters)\n",
        "  print('\\nAverage scores:\\n',  performance_metrics['avg_scores'])\n",
        "  print('\\nStandard_deviations:\\n',  performance_metrics['stds'])\n",
        "  print('\\nMinimums:\\n',  performance_metrics['minimums'])\n",
        "  print('\\nMaximums:\\n',  performance_metrics['maximums'])\n",
        "  print(\"Progress: All works are done successfully, congratulations!!\\n\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5mYB8eUPsRf"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hrnEdcrSPuE_",
        "outputId": "072063e4-67cd-4c68-bc9c-0c8f3a8ba8b2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nhidden_layers = [[10], [30], [50], [100], [150], [200]]\\nbest_hyper_parameters   = [['Adagrad', 0.1, 4],#10-5N model\\n                                     ['Adagrad', 0.01, 16],#20-10N model\\n                                    ['Adagrad', 0.01, 16], #50-20N model\\n                                   ['Adagrad', 0.01, 16], #100-50N model\\n                                   ['Adagrad', 0.01, 16], #150-100N model\\n                                   ['Adagrad', 0.001, 8] #100-50-20N model\\n                        ]\\nMultiple_LSTM_Models(hidden_layers, best_hyper_parameters, data, time_step = 5, test_split = 0.2, epochs = 3,  num_replicates = 3) \\n\""
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#hidden_layers = [[10, 5], [20, 10], [50, 20], [100, 50], [150, 100], [100, 50, 20]]\n",
        "\n",
        "\"\"\"\n",
        "hidden_layers = [[10], [30], [50], [100], [150], [200]]\n",
        "best_hyper_parameters   = [['Adagrad', 0.1, 4],#10-5N model\n",
        "                                     ['Adagrad', 0.01, 16],#20-10N model\n",
        "                                    ['Adagrad', 0.01, 16], #50-20N model\n",
        "                                   ['Adagrad', 0.01, 16], #100-50N model\n",
        "                                   ['Adagrad', 0.01, 16], #150-100N model\n",
        "                                   ['Adagrad', 0.001, 8] #100-50-20N model\n",
        "                        ]\n",
        "Multiple_LSTM_Models(hidden_layers, best_hyper_parameters, data, time_step = 5, test_split = 0.2, epochs = 3,  num_replicates = 3)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwrzCRZIQLQR"
      },
      "source": [
        "## **Executing Multiple GRU Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jBGirvnQPXV"
      },
      "outputs": [],
      "source": [
        "#hidden_layers = [[10, 5], [20, 10], [50, 20], [100, 50], [150, 100], [100, 50, 20]]\n",
        "\n",
        "def Multiple_GRU_Models(hidden_layers, hyper_parameters, data, time_step = 5, test_split = 0.2, epochs = 5,  num_replicates = 2):\n",
        "\n",
        "  num_models = len(hidden_layers)\n",
        "\n",
        "  #== to collect all scores===#\n",
        "  rmse = []\n",
        "  mape = []\n",
        "  R = []\n",
        "  elapsed_time = []\n",
        "\n",
        "  #===to collect all avg scores===#\n",
        "  avg_rmse = []\n",
        "  avg_mape = []\n",
        "  avg_R = []\n",
        "  avg_elapsed_time = []\n",
        "\n",
        "  #=== to iteratively update the best rmse and the corresponding model\n",
        "  best_avg_rmse = 99999999999\n",
        "  best_rmse = 99999999999\n",
        "  best_model_hidden_layers = None\n",
        "  best_model_output = None\n",
        "\n",
        "  for i in range(num_models):\n",
        "    print(\"Running model with hidden neurons: \", hidden_layers[i])\n",
        "\n",
        "    print(\"\\n\")\n",
        "\n",
        "    print(\"Best Hyper_parameters used: \", hyper_parameters[i])\n",
        "\n",
        "    print(\"\\n\")\n",
        "\n",
        "    output = GRU_Model(hidden_layers[i], hyper_parameters[i], data, time_step, test_split, epochs, num_replicates)\n",
        "\n",
        "    rmse.append(output['performance_metrics']['scores']['rmse'])\n",
        "    mape.append(output['performance_metrics']['scores']['mape'])\n",
        "    R.append(output['performance_metrics']['scores']['R'])\n",
        "    elapsed_time.append(output['performance_metrics']['scores']['elapsed_time'])\n",
        "\n",
        "    avg_rmse.append(output['performance_metrics']['avg_scores']['rmse'])\n",
        "    avg_mape.append(output['performance_metrics']['avg_scores']['mape'])\n",
        "    avg_R.append(output['performance_metrics']['avg_scores']['R'])\n",
        "    avg_elapsed_time.append(output['performance_metrics']['avg_scores']['elapsed_time'])\n",
        "\n",
        "    if avg_rmse[i] < best_avg_rmse:\n",
        "      best_avg_rmse = avg_rmse[i]\n",
        "      best_rmse = output['best_model']['rmse']\n",
        "      best_model_hidden_layers = hidden_layers[i]\n",
        "      best_model_output = output\n",
        "\n",
        "\n",
        "  rmse = np.array(rmse)\n",
        "  mape = np.array(mape)\n",
        "  R =  np.array(R)\n",
        "\n",
        "  # ===== Collecting all  scores================================#\n",
        "\n",
        "  performance_metrics = {\n",
        "\n",
        "       'scores':  {'layers': hidden_layers,\n",
        "                   'rmse': rmse,\n",
        "                   'mape': mape,\n",
        "                   'R':R,\n",
        "                   'elapsed_time': elapsed_time },\n",
        "\n",
        "       'avg_scores':  pd.DataFrame({'layers': hidden_layers,\n",
        "                                    'rmse': np.array(avg_rmse),\n",
        "                                    'mape': np.array(avg_mape), 'R':np.array(avg_R),\n",
        "                                    'elapsed_time':np.array(avg_elapsed_time)}),\n",
        "\n",
        "       'stds':     pd.DataFrame({'layers': hidden_layers,\n",
        "                                 'rmse': np.std(rmse, axis = 1),\n",
        "                                 'mape': np.std(mape, axis = 1),\n",
        "                                 'R':  np.std(R, axis = 1 ),\n",
        "                                 'elapsed_time': np.std(elapsed_time, axis = 1 )}),\n",
        "       'minimums': pd.DataFrame({'layers': hidden_layers,\n",
        "                                'rmse': np.min(rmse, axis =1 ),\n",
        "                                'mape': np.min(mape, axis= 1),\n",
        "                                'R': np.min(R, axis =1),\n",
        "                                'elapsed_time': np.min(elapsed_time, axis =1)}),\n",
        "\n",
        "       'maximums': pd.DataFrame({'layers': hidden_layers,\n",
        "                                'rmse': np.max(rmse, axis =1),\n",
        "                                'mape': np.max(mape, axis =1),\n",
        "                                'R': np.max(R, axis =1),\n",
        "                                'elapsed_time': np.max(elapsed_time,axis =1)})\n",
        "    }\n",
        "\n",
        "\n",
        "  output_dictionary = {\n",
        "                     'hyper_parameters': hyper_parameters,\n",
        "\n",
        "                      'best_avg_rmse': best_avg_rmse,\n",
        "                      'best_rmse': best_rmse,\n",
        "                      'best_model_hidden_layers': best_model_hidden_layers,\n",
        "                      'best_model_output': best_model_output\n",
        "                      }\n",
        "\n",
        "  #Save all statistics:\n",
        "  performance_metrics['avg_scores'].to_csv(output_dir_path+'multiple_gru_models_average_scores.csv')\n",
        "  performance_metrics['stds'].to_csv(output_dir_path+'multiple_gru_models_stds.csv')\n",
        "  performance_metrics['minimums'].to_csv(output_dir_path+'multiple_gru_models_minimums.csv')\n",
        "  performance_metrics['maximums'].to_csv(output_dir_path+'multiple_gru_models_maximums.csv')\n",
        "\n",
        "\n",
        "  #Save all scores in the file for future analysis\n",
        "  pd.DataFrame(performance_metrics['scores']['rmse']).to_csv(output_dir_path+'multiple_gru_models_all_rmse.csv')\n",
        "  pd.DataFrame(performance_metrics['scores']['mape']).to_csv(output_dir_path+'multiple_gru_models_all_mape.csv')\n",
        "  pd.DataFrame(performance_metrics['scores']['R']).to_csv(output_dir_path+'multiple_gru_models_all_R.csv')\n",
        "\n",
        "  #Save best model results\n",
        "  pd.DataFrame(best_model_output['best_model']['loss']).to_csv(output_dir_path+'best_gru_model_loss.csv')\n",
        "  pd.DataFrame(best_model_output['best_model']['train_predictions']).to_csv(output_dir_path+'best_gru_model_train_predictions.csv')\n",
        "  pd.DataFrame(best_model_output['best_model']['test_predictions']).to_csv(output_dir_path+'best_gru_model_test_predictions.csv')\n",
        "  pd.DataFrame(best_model_output['performance_metrics']['scores']['rmse']).to_csv(output_dir_path+'best_gru_model_all_rmse.csv')\n",
        "\n",
        "  #writing all result in the file\n",
        "  write_dic_to_file(output_dictionary, output_dir_path + \"multiple_gru_models_full_results.txt\")\n",
        "\n",
        "  #Display some key results in the screen\n",
        "  print(\"\\nBest model and its avg rmse and minimum rmse):\\n\", best_model_hidden_layers, best_avg_rmse, best_rmse)\n",
        "  print(\"Hyper_parameters:\\n\", hyper_parameters)\n",
        "  print('\\nAverage scores:\\n',  performance_metrics['avg_scores'])\n",
        "  print('\\nStandard_deviations:\\n',  performance_metrics['stds'])\n",
        "  print('\\nMinimums:\\n',  performance_metrics['minimums'])\n",
        "  print('\\nMaximums:\\n',  performance_metrics['maximums'])\n",
        "\n",
        "  print(\"Progress: All works are done successfully, congratulations!!\\n\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXgoJC4cobWX"
      },
      "source": [
        "## **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qlAcXn7wodpi",
        "outputId": "15c98f84-c4ec-4442-d2e3-b6690624976a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nhidden_layers = [[10], [30], [50], [100], [150], [200]]\\nbest_hyper_parameters   = [['Adagrad', 0.1, 4],#10-5N model\\n                                     ['Adagrad', 0.01, 16],#20-10N model\\n                                    ['Adagrad', 0.01, 16], #50-20N model\\n                                   ['Adagrad', 0.01, 16], #100-50N model\\n                                   ['Adagrad', 0.01, 16], #150-100N model\\n                                   ['Adagrad', 0.001, 8] #100-50-20N model\\n                                      ]\\n\\nMultiple_GRU_Models(hidden_layers, best_hyper_parameters, data, time_step = 5, test_split = 0.2, epochs = 3,  num_replicates = 3) \\n\""
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\"\"\"\n",
        "hidden_layers = [[10], [30], [50], [100], [150], [200]]\n",
        "best_hyper_parameters   = [['Adagrad', 0.1, 4],#10-5N model\n",
        "                                     ['Adagrad', 0.01, 16],#20-10N model\n",
        "                                    ['Adagrad', 0.01, 16], #50-20N model\n",
        "                                   ['Adagrad', 0.01, 16], #100-50N model\n",
        "                                   ['Adagrad', 0.01, 16], #150-100N model\n",
        "                                   ['Adagrad', 0.001, 8] #100-50-20N model\n",
        "                                      ]\n",
        "\n",
        "Multiple_GRU_Models(hidden_layers, best_hyper_parameters, data, time_step = 5, test_split = 0.2, epochs = 3,  num_replicates = 3)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pED0RG9vvhL"
      },
      "source": [
        "# **Executing all Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7W4OjVktTjS"
      },
      "outputs": [],
      "source": [
        "\n",
        "def RUN_LSTM_GRU_Models(lstm_input, gru_input,  data, time_step = 5, test_split = 0.2, epochs = 5, num_replicates = 5):\n",
        "\n",
        "  print(\"Performing LSTM Runs ==============================\\n\")\n",
        "  Multiple_LSTM_Models(lstm_input['layers'], lstm_input['best_hyper_parameters'], data, time_step, test_split, epochs,  num_replicates)\n",
        "\n",
        "  print(\"Performing GRU Runs ==============================\\n\")\n",
        "  Multiple_GRU_Models(gru_input['layers'], gru_input['best_hyper_parameters'], data, time_step, test_split, epochs,  num_replicates)\n",
        "\n",
        "  print(\" All parts are sucessefully completed!!\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fx_oPhnev4e0"
      },
      "source": [
        "## **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBKyp9tuvqZ7",
        "outputId": "16d42c25-8930-464c-8ed8-c9f451a8cb53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 5.4324e-04\n",
            "Epoch 15/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 5.0155e-04\n",
            "Epoch 16/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 4.6321e-04\n",
            "Epoch 17/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 4.3431e-04\n",
            "Epoch 18/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 4.0564e-04\n",
            "Epoch 19/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.8918e-04\n",
            "Epoch 20/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.7307e-04\n",
            "Epoch 21/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.5895e-04\n",
            "Epoch 22/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.4279e-04\n",
            "Epoch 23/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.3696e-04\n",
            "Epoch 24/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.2996e-04\n",
            "Epoch 25/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.1912e-04\n",
            "Epoch 26/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.1694e-04\n",
            "Epoch 27/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.1227e-04\n",
            "Epoch 28/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.0612e-04\n",
            "Epoch 29/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.0677e-04\n",
            "Epoch 30/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.0324e-04\n",
            "Epoch 31/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.9878e-04\n",
            "Epoch 32/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.9765e-04\n",
            "Epoch 33/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 2.9558e-04\n",
            "Epoch 34/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.9613e-04\n",
            "Epoch 35/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.9060e-04\n",
            "Epoch 36/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.9109e-04\n",
            "Epoch 37/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.9387e-04\n",
            "Epoch 38/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.9020e-04\n",
            "Epoch 39/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.9038e-04\n",
            "Epoch 40/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.8845e-04\n",
            "Epoch 41/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.8405e-04\n",
            "Epoch 42/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.8736e-04\n",
            "Epoch 43/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.8457e-04\n",
            "Epoch 44/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.8713e-04\n",
            "Epoch 45/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.8489e-04\n",
            "Epoch 46/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 2.8471e-04\n",
            "27/27 [==============================] - 1s 3ms/step\n",
            "7/7 [==============================] - 0s 4ms/step\n",
            "Program is running for 13 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "107/107 [==============================] - 3s 7ms/step - loss: 0.0202\n",
            "Epoch 2/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 0.0026\n",
            "Epoch 3/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 0.0014\n",
            "Epoch 4/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 0.0010\n",
            "Epoch 5/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 8.2406e-04\n",
            "Epoch 6/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 6.8307e-04\n",
            "Epoch 7/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 5.8128e-04\n",
            "Epoch 8/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 5.1820e-04\n",
            "Epoch 9/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 4.5435e-04\n",
            "Epoch 10/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 4.2968e-04\n",
            "Epoch 11/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 4.0385e-04\n",
            "Epoch 12/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 3.7849e-04\n",
            "Epoch 13/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.6648e-04\n",
            "Epoch 14/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 3.6101e-04\n",
            "Epoch 15/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 3.5085e-04\n",
            "Epoch 16/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 3.4022e-04\n",
            "Epoch 17/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.3593e-04\n",
            "Epoch 18/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 3.2876e-04\n",
            "Epoch 19/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 3.2358e-04\n",
            "Epoch 20/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 3.2306e-04\n",
            "Epoch 21/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 3.2031e-04\n",
            "Epoch 22/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 3.1802e-04\n",
            "Epoch 23/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.1771e-04\n",
            "Epoch 24/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 3.1756e-04\n",
            "Epoch 25/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.0957e-04\n",
            "Epoch 26/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.0623e-04\n",
            "Epoch 27/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.0954e-04\n",
            "Epoch 28/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.0953e-04\n",
            "Epoch 29/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.0770e-04\n",
            "Epoch 30/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.0036e-04\n",
            "Epoch 31/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.0100e-04\n",
            "Epoch 32/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.9656e-04\n",
            "Epoch 33/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.9836e-04\n",
            "Epoch 34/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 2.9697e-04\n",
            "Epoch 35/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.0038e-04\n",
            "Epoch 36/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 2.9513e-04\n",
            "Epoch 37/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 2.9519e-04\n",
            "Epoch 38/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.9367e-04\n",
            "Epoch 39/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.9495e-04\n",
            "Epoch 40/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 2.9044e-04\n",
            "Epoch 41/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.9088e-04\n",
            "Epoch 42/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.8881e-04\n",
            "Epoch 43/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.9185e-04\n",
            "Epoch 44/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 2.9021e-04\n",
            "Epoch 45/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.8909e-04\n",
            "Epoch 46/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.8482e-04\n",
            "Epoch 47/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.8763e-04\n",
            "Epoch 48/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.8417e-04\n",
            "Epoch 49/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.8517e-04\n",
            "Epoch 50/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.8136e-04\n",
            "27/27 [==============================] - 1s 3ms/step\n",
            "7/7 [==============================] - 0s 4ms/step\n",
            "Program is running for 14 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "107/107 [==============================] - 3s 5ms/step - loss: 0.0149\n",
            "Epoch 2/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0036\n",
            "Epoch 3/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0029\n",
            "Epoch 4/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 0.0023\n",
            "Epoch 5/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 0.0019\n",
            "Epoch 6/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 0.0016\n",
            "Epoch 7/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 0.0013\n",
            "Epoch 8/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 0.0011\n",
            "Epoch 9/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 9.5591e-04\n",
            "Epoch 10/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 8.3039e-04\n",
            "Epoch 11/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 7.2877e-04\n",
            "Epoch 12/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 6.4999e-04\n",
            "Epoch 13/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 5.8366e-04\n",
            "Epoch 14/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 5.4332e-04\n",
            "Epoch 15/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 4.9343e-04\n",
            "Epoch 16/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 4.6148e-04\n",
            "Epoch 17/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 4.3620e-04\n",
            "Epoch 18/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 4.1871e-04\n",
            "Epoch 19/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 4.0433e-04\n",
            "Epoch 20/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.8583e-04\n",
            "Epoch 21/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.7485e-04\n",
            "Epoch 22/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 3.6733e-04\n",
            "Epoch 23/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.6039e-04\n",
            "Epoch 24/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.5456e-04\n",
            "Epoch 25/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 3.4405e-04\n",
            "Epoch 26/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.4077e-04\n",
            "Epoch 27/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.3938e-04\n",
            "Epoch 28/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.3348e-04\n",
            "Epoch 29/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.3028e-04\n",
            "Epoch 30/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.2837e-04\n",
            "Epoch 31/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.2234e-04\n",
            "Epoch 32/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.2254e-04\n",
            "Epoch 33/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.1696e-04\n",
            "Epoch 34/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.1786e-04\n",
            "Epoch 35/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.1717e-04\n",
            "Epoch 36/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 3.1394e-04\n",
            "Epoch 37/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.1506e-04\n",
            "Epoch 38/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.1109e-04\n",
            "Epoch 39/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.1148e-04\n",
            "Epoch 40/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.0713e-04\n",
            "Epoch 41/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.0543e-04\n",
            "Epoch 42/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.0586e-04\n",
            "Epoch 43/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.0541e-04\n",
            "Epoch 44/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.0411e-04\n",
            "Epoch 45/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.0539e-04\n",
            "Epoch 46/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.0145e-04\n",
            "Epoch 47/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.0015e-04\n",
            "Epoch 48/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 2.9761e-04\n",
            "Epoch 49/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.0025e-04\n",
            "Epoch 50/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 2.9630e-04\n",
            "27/27 [==============================] - 1s 4ms/step\n",
            "7/7 [==============================] - 0s 4ms/step\n",
            "Program is running for 15 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "107/107 [==============================] - 3s 6ms/step - loss: 0.0490\n",
            "Epoch 2/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0066\n",
            "Epoch 3/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 0.0048\n",
            "Epoch 4/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 0.0039\n",
            "Epoch 5/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 0.0033\n",
            "Epoch 6/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 0.0028\n",
            "Epoch 7/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 0.0023\n",
            "Epoch 8/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0020\n",
            "Epoch 9/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0017\n",
            "Epoch 10/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0015\n",
            "Epoch 11/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0013\n",
            "Epoch 12/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0011\n",
            "Epoch 13/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 9.7883e-04\n",
            "Epoch 14/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 8.7044e-04\n",
            "Epoch 15/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 7.7713e-04\n",
            "Epoch 16/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 7.0790e-04\n",
            "Epoch 17/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 6.4619e-04\n",
            "Epoch 18/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 5.9907e-04\n",
            "Epoch 19/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 5.5379e-04\n",
            "Epoch 20/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 5.2152e-04\n",
            "Epoch 21/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 4.9330e-04\n",
            "Epoch 22/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 4.7059e-04\n",
            "Epoch 23/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 4.4887e-04\n",
            "Epoch 24/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 4.3587e-04\n",
            "Epoch 25/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 4.2214e-04\n",
            "Epoch 26/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 4.0950e-04\n",
            "Epoch 27/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 4.0162e-04\n",
            "Epoch 28/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.9283e-04\n",
            "Epoch 29/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.8386e-04\n",
            "Epoch 30/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.7665e-04\n",
            "Epoch 31/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.6628e-04\n",
            "Epoch 32/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 3.6527e-04\n",
            "Epoch 33/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 3.6098e-04\n",
            "Epoch 34/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.5433e-04\n",
            "Epoch 35/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.4698e-04\n",
            "Epoch 36/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 3.3793e-04\n",
            "Epoch 37/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.3232e-04\n",
            "Epoch 38/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 3.2253e-04\n",
            "Epoch 39/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 3.1662e-04\n",
            "Epoch 40/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 3.0899e-04\n",
            "Epoch 41/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.0488e-04\n",
            "Epoch 42/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 3.0529e-04\n",
            "Epoch 43/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.0364e-04\n",
            "Epoch 44/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.0301e-04\n",
            "Epoch 45/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.0110e-04\n",
            "Epoch 46/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.9989e-04\n",
            "Epoch 47/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 2.9877e-04\n",
            "Epoch 48/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 2.9945e-04\n",
            "Epoch 49/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.9656e-04\n",
            "Epoch 50/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 2.9404e-04\n",
            "27/27 [==============================] - 0s 3ms/step\n",
            "7/7 [==============================] - 0s 3ms/step\n",
            "Program is running for 16 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "107/107 [==============================] - 5s 6ms/step - loss: 0.0354\n",
            "Epoch 2/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 0.0065\n",
            "Epoch 3/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 0.0048\n",
            "Epoch 4/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 0.0038\n",
            "Epoch 5/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 0.0031\n",
            "Epoch 6/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 0.0026\n",
            "Epoch 7/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 0.0022\n",
            "Epoch 8/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 0.0019\n",
            "Epoch 9/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 0.0016\n",
            "Epoch 10/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 0.0013\n",
            "Epoch 11/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 0.0011\n",
            "Epoch 12/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 9.3696e-04\n",
            "Epoch 13/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 8.0192e-04\n",
            "Epoch 14/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 7.0511e-04\n",
            "Epoch 15/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 6.5216e-04\n",
            "Epoch 16/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 5.8804e-04\n",
            "Epoch 17/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 5.5410e-04\n",
            "Epoch 18/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 5.1691e-04\n",
            "Epoch 19/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 4.9005e-04\n",
            "Epoch 20/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 4.6730e-04\n",
            "Epoch 21/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 4.5052e-04\n",
            "Epoch 22/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 4.3300e-04\n",
            "Epoch 23/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 4.1829e-04\n",
            "Epoch 24/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 4.0546e-04\n",
            "Epoch 25/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 3.9559e-04\n",
            "Epoch 26/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.8687e-04\n",
            "Epoch 27/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.8260e-04\n",
            "Epoch 28/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.7404e-04\n",
            "Epoch 29/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.6838e-04\n",
            "Epoch 30/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.6178e-04\n",
            "Epoch 31/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.5829e-04\n",
            "Epoch 32/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.5356e-04\n",
            "Epoch 33/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.4789e-04\n",
            "Epoch 34/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.4051e-04\n",
            "Epoch 35/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.4011e-04\n",
            "Epoch 36/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.3586e-04\n",
            "Epoch 37/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.3288e-04\n",
            "Epoch 38/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.2939e-04\n",
            "Epoch 39/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.2785e-04\n",
            "Epoch 40/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.2691e-04\n",
            "Epoch 41/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.2049e-04\n",
            "Epoch 42/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.2135e-04\n",
            "Epoch 43/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.1592e-04\n",
            "Epoch 44/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.1305e-04\n",
            "Epoch 45/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.0638e-04\n",
            "Epoch 46/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.0829e-04\n",
            "Epoch 47/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.0595e-04\n",
            "Epoch 48/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.0533e-04\n",
            "Epoch 49/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.9876e-04\n",
            "Epoch 50/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.9925e-04\n",
            "27/27 [==============================] - 1s 3ms/step\n",
            "7/7 [==============================] - 0s 4ms/step\n",
            "Program is running for 17 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "107/107 [==============================] - 3s 5ms/step - loss: 0.0159\n",
            "Epoch 2/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 0.0039\n",
            "Epoch 3/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0027\n",
            "Epoch 4/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0021\n",
            "Epoch 5/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0017\n",
            "Epoch 6/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0014\n",
            "Epoch 7/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 0.0012\n",
            "Epoch 8/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 9.9192e-04\n",
            "Epoch 9/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 8.6002e-04\n",
            "Epoch 10/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 7.5471e-04\n",
            "Epoch 11/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 6.7955e-04\n",
            "Epoch 12/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 6.0434e-04\n",
            "Epoch 13/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 5.5355e-04\n",
            "Epoch 14/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 5.0510e-04\n",
            "Epoch 15/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 4.6822e-04\n",
            "Epoch 16/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 4.3667e-04\n",
            "Epoch 17/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 4.1484e-04\n",
            "Epoch 18/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.9430e-04\n",
            "Epoch 19/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.7702e-04\n",
            "Epoch 20/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.6057e-04\n",
            "Epoch 21/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.4716e-04\n",
            "Epoch 22/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.3746e-04\n",
            "Epoch 23/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.2748e-04\n",
            "Epoch 24/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.1590e-04\n",
            "Epoch 25/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.0905e-04\n",
            "Epoch 26/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.0680e-04\n",
            "Epoch 27/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.0040e-04\n",
            "Epoch 28/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.9667e-04\n",
            "Epoch 29/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.9366e-04\n",
            "Epoch 30/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.9404e-04\n",
            "Epoch 31/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.8891e-04\n",
            "Epoch 32/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.8539e-04\n",
            "Epoch 33/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.7949e-04\n",
            "Epoch 34/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.8029e-04\n",
            "Epoch 35/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.7630e-04\n",
            "Epoch 36/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.7586e-04\n",
            "Epoch 37/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.7383e-04\n",
            "Epoch 38/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.6909e-04\n",
            "Epoch 39/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.7293e-04\n",
            "Epoch 40/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.7099e-04\n",
            "Epoch 41/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.6599e-04\n",
            "Epoch 42/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.6713e-04\n",
            "Epoch 43/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 2.6587e-04\n",
            "Epoch 44/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 2.6338e-04\n",
            "Epoch 45/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.6018e-04\n",
            "Epoch 46/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.5869e-04\n",
            "Epoch 47/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 2.6010e-04\n",
            "Epoch 48/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.6103e-04\n",
            "Epoch 49/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.5912e-04\n",
            "Epoch 50/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 2.5741e-04\n",
            "27/27 [==============================] - 1s 4ms/step\n",
            "7/7 [==============================] - 0s 5ms/step\n",
            "Program is running for 18 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "107/107 [==============================] - 3s 6ms/step - loss: 0.0291\n",
            "Epoch 2/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 0.0060\n",
            "Epoch 3/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 0.0044\n",
            "Epoch 4/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 0.0035\n",
            "Epoch 5/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 0.0029\n",
            "Epoch 6/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 0.0025\n",
            "Epoch 7/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 0.0021\n",
            "Epoch 8/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 0.0018\n",
            "Epoch 9/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 0.0015\n",
            "Epoch 10/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 0.0013\n",
            "Epoch 11/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0011\n",
            "Epoch 12/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 9.6344e-04\n",
            "Epoch 13/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 8.2773e-04\n",
            "Epoch 14/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 7.3177e-04\n",
            "Epoch 15/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 6.5991e-04\n",
            "Epoch 16/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 5.8791e-04\n",
            "Epoch 17/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 5.4052e-04\n",
            "Epoch 18/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 4.9487e-04\n",
            "Epoch 19/50\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 4.6330e-04\n",
            "Epoch 20/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 4.2464e-04\n",
            "Epoch 21/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 4.1135e-04\n",
            "Epoch 22/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.9502e-04\n",
            "Epoch 23/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.7914e-04\n",
            "Epoch 24/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.7082e-04\n",
            "Epoch 25/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.5912e-04\n",
            "Epoch 26/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.5860e-04\n",
            "Epoch 27/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.4836e-04\n",
            "Epoch 28/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.4425e-04\n",
            "Epoch 29/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.4025e-04\n",
            "Epoch 30/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.3193e-04\n",
            "Epoch 31/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.2876e-04\n",
            "Epoch 32/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.3074e-04\n",
            "Epoch 33/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.3193e-04\n",
            "Epoch 34/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.2782e-04\n",
            "Epoch 35/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.2009e-04\n",
            "Epoch 36/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.2288e-04\n",
            "Epoch 37/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.2132e-04\n",
            "Epoch 38/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.1807e-04\n",
            "Epoch 39/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.2041e-04\n",
            "Epoch 40/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.1716e-04\n",
            "Epoch 41/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.1284e-04\n",
            "Epoch 42/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.1478e-04\n",
            "Epoch 43/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.1003e-04\n",
            "Epoch 44/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.1050e-04\n",
            "Epoch 45/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.1174e-04\n",
            "Epoch 46/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.1420e-04\n",
            "Epoch 47/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.1054e-04\n",
            "Epoch 48/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.0901e-04\n",
            "Epoch 49/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.0454e-04\n",
            "Epoch 50/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.0600e-04\n",
            "27/27 [==============================] - 1s 3ms/step\n",
            "7/7 [==============================] - 0s 3ms/step\n",
            "Program is running for 19 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "107/107 [==============================] - 3s 7ms/step - loss: 0.0093\n",
            "Epoch 2/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 0.0033\n",
            "Epoch 3/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 0.0023\n",
            "Epoch 4/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 0.0018\n",
            "Epoch 5/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 0.0015\n",
            "Epoch 6/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 0.0013\n",
            "Epoch 7/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 0.0011\n",
            "Epoch 8/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 9.9834e-04\n",
            "Epoch 9/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 8.9223e-04\n",
            "Epoch 10/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 8.0371e-04\n",
            "Epoch 11/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 7.3384e-04\n",
            "Epoch 12/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 6.7856e-04\n",
            "Epoch 13/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 6.2696e-04\n",
            "Epoch 14/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 5.7967e-04\n",
            "Epoch 15/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 5.3866e-04\n",
            "Epoch 16/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 5.0811e-04\n",
            "Epoch 17/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 4.9437e-04\n",
            "Epoch 18/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 4.7523e-04\n",
            "Epoch 19/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 4.4928e-04\n",
            "Epoch 20/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 4.3603e-04\n",
            "Epoch 21/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 4.2798e-04\n",
            "Epoch 22/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 4.1863e-04\n",
            "Epoch 23/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 4.0765e-04\n",
            "Epoch 24/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.9625e-04\n",
            "Epoch 25/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.8427e-04\n",
            "Epoch 26/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.8682e-04\n",
            "Epoch 27/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.7483e-04\n",
            "Epoch 28/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.7534e-04\n",
            "Epoch 29/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.6446e-04\n",
            "Epoch 30/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.6839e-04\n",
            "Epoch 31/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.5665e-04\n",
            "Epoch 32/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.4952e-04\n",
            "Epoch 33/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.4628e-04\n",
            "Epoch 34/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.3834e-04\n",
            "Epoch 35/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.3902e-04\n",
            "Epoch 36/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.2745e-04\n",
            "Epoch 37/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.3119e-04\n",
            "Epoch 38/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.2274e-04\n",
            "Epoch 39/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.2572e-04\n",
            "Epoch 40/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.1840e-04\n",
            "Epoch 41/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.1261e-04\n",
            "Epoch 42/50\n",
            "107/107 [==============================] - 1s 7ms/step - loss: 3.0985e-04\n",
            "Epoch 43/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.0186e-04\n",
            "Epoch 44/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.0517e-04\n",
            "Epoch 45/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.0319e-04\n",
            "Epoch 46/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.0008e-04\n",
            "Epoch 47/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 3.0105e-04\n",
            "Epoch 48/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.9589e-04\n",
            "Epoch 49/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.9458e-04\n",
            "Epoch 50/50\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 2.9510e-04\n",
            "27/27 [==============================] - 1s 3ms/step\n",
            "7/7 [==============================] - 0s 3ms/step\n",
            "Progress: All works are done successfully, congratulations!!\n",
            "\n",
            "Running model with hidden neurons:  [64]\n",
            "\n",
            "\n",
            "Best Hyper_parameters used:  ['Adagrad', 0.01, 16]\n",
            "\n",
            "\n",
            "Progress: Performing data preparation steps.......\n",
            "\n",
            "Program is running for 0 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 2s 5ms/step - loss: 0.0232\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0046\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0030\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0023\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0020\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0018\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0015\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 9.4151e-04\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.6869e-04\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 7.9686e-04\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 7.5786e-04\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 6.9254e-04\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.5654e-04\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 6.2892e-04\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.9860e-04\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.5908e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.4231e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.2015e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.9048e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.8438e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.7048e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.4810e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.3418e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.2723e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.1437e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.0825e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.0867e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.9804e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.9159e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.8031e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.7781e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.7299e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 3.6878e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.6719e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5830e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5518e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5505e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5313e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4631e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4884e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.5196e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.4369e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4211e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3363e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.3464e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3390e-04\n",
            "27/27 [==============================] - 1s 3ms/step\n",
            "7/7 [==============================] - 0s 3ms/step\n",
            "Program is running for 1 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 3s 5ms/step - loss: 0.0271\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0046\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0039\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0034\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0030\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0027\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0024\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0021\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0020\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0018\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0016\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0015\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0013\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0012\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 9.5732e-04\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 9.0787e-04\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.5531e-04\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.2542e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 7.9081e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 7.4701e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 7.2613e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.9268e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.6238e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 6.3866e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 6.1805e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.9362e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.8015e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.5768e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.3976e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.2432e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.1189e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.9347e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.8087e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.5550e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.4685e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.3595e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.2815e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.1861e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.0884e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.0323e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.0160e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.9349e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.9045e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.8563e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.8258e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.7736e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.6941e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.6505e-04\n",
            "27/27 [==============================] - 1s 3ms/step\n",
            "7/7 [==============================] - 0s 3ms/step\n",
            "Program is running for 2 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 3s 5ms/step - loss: 0.0358\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0069\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0048\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0038\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0032\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0027\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0024\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0021\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0019\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0017\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0016\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0013\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0012\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 9.2965e-04\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 8.7225e-04\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 8.0806e-04\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.5107e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 7.1931e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 6.7128e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 6.3555e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 6.0274e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.7247e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.4472e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.2522e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.0620e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.8612e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.6710e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.5282e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.4041e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.3183e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.1438e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.0446e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.9792e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.9075e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.8654e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.8152e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.7170e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.6979e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.6639e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.6337e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5366e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.5508e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5005e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5243e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4592e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4208e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.3856e-04\n",
            "27/27 [==============================] - 0s 3ms/step\n",
            "7/7 [==============================] - 0s 5ms/step\n",
            "Program is running for 3 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 2s 5ms/step - loss: 0.0172\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0052\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0039\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0032\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0028\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0024\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0021\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0019\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0017\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0015\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0013\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 9.5233e-04\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 8.5285e-04\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.7172e-04\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.1273e-04\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 6.5383e-04\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 5.9911e-04\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.5597e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.1560e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.8897e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.5799e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.3237e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.1688e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.9548e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.7652e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.6486e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.5881e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4328e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3800e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3098e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.1933e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.1518e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1122e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.0485e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0477e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.9585e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 2.9403e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 2.8895e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.8954e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.8552e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.8164e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.8286e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.7686e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.8325e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 2.7383e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.7541e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.7494e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.7049e-04\n",
            "27/27 [==============================] - 0s 3ms/step\n",
            "7/7 [==============================] - 0s 3ms/step\n",
            "Program is running for 4 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 2s 6ms/step - loss: 0.0232\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0046\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0035\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0029\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0025\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0021\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0019\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0016\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.0013\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 9.2133e-04\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.1736e-04\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.5734e-04\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.8901e-04\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.4393e-04\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.9653e-04\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.6455e-04\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.3832e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.0913e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.8331e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.6822e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.5044e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.2580e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.1694e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.0636e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.0123e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.8467e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.7762e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.7642e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.6694e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.6104e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5359e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.4644e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.4748e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4143e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3873e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3165e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.3096e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.2976e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.2648e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2355e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.1800e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.1812e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1130e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.1159e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0870e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0452e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0422e-04\n",
            "27/27 [==============================] - 0s 3ms/step\n",
            "7/7 [==============================] - 0s 3ms/step\n",
            "Program is running for 5 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 2s 5ms/step - loss: 0.0218\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0049\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0036\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0029\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0025\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0022\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0020\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0018\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0016\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0015\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.0013\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 9.7331e-04\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 9.2307e-04\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 8.8165e-04\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.3882e-04\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.9370e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.5765e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.3534e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.9513e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 6.7870e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.5772e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.3331e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.1136e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.9442e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.8414e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.7122e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.5176e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.4362e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.3611e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.2160e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.1503e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.0021e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.9664e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.8899e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.7871e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.7243e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.6895e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.6248e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.5521e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.4914e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.4407e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.3823e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 4.3129e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.2780e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.2641e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.1862e-04\n",
            "27/27 [==============================] - 1s 3ms/step\n",
            "7/7 [==============================] - 0s 3ms/step\n",
            "Program is running for 6 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 3s 5ms/step - loss: 0.0262\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0061\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0047\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0040\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0035\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0031\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0028\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0026\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0023\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0021\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.0020\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0018\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0016\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0015\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0014\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0012\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 9.8989e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 9.3156e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 8.8014e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 8.4753e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 8.0713e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.6502e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 7.2700e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 6.9809e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 6.7306e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 6.4595e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 6.2228e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.9483e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.8068e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.5099e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.4012e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.1938e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.0589e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.8753e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.7297e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.5879e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.5012e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 4.3156e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.2033e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 4.1322e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 4.0259e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.9359e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.8916e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.8489e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.7330e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.6539e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.6202e-04\n",
            "27/27 [==============================] - 0s 3ms/step\n",
            "7/7 [==============================] - 0s 5ms/step\n",
            "Program is running for 7 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 2s 6ms/step - loss: 0.0203\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0072\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0049\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0039\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0033\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0028\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0025\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0022\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0019\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0017\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0015\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0013\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0012\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 9.8380e-04\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.8595e-04\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 8.0823e-04\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 7.5381e-04\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.0121e-04\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 6.4941e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 6.0762e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.7563e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.4054e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.2003e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.8886e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.7613e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.5240e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.3598e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.2938e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.1359e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.0785e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.9648e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.8689e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.8054e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.7412e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.6620e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.6213e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.6492e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5662e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5469e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.4366e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4478e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4113e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.3976e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3080e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3380e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3270e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2624e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2684e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.2556e-04\n",
            "27/27 [==============================] - 0s 3ms/step\n",
            "7/7 [==============================] - 0s 5ms/step\n",
            "Program is running for 8 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 2s 6ms/step - loss: 0.0662\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0112\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0067\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0050\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0041\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0036\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0031\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0028\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0025\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0023\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0021\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0019\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0018\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0017\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0015\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.0014\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 9.4468e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.8918e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 8.5160e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.0178e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.6772e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.3734e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.0847e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 6.7578e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 6.4437e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 6.2239e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.9840e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.7787e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 5.6240e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.4770e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.2891e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.1654e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.0304e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.9395e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.8240e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.7201e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.6058e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.5324e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.4587e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.4003e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.3402e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 4.2774e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.2214e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.1858e-04\n",
            "27/27 [==============================] - 0s 3ms/step\n",
            "7/7 [==============================] - 0s 4ms/step\n",
            "Program is running for 9 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 3s 8ms/step - loss: 0.0490\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0092\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0070\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0059\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0051\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0046\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0042\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0038\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0035\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0032\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.0030\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.0028\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0026\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.0024\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0022\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0021\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0020\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0018\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0017\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0016\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0015\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0015\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0012\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 9.6876e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 9.2628e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.9584e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.5216e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.1655e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.9068e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.6423e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.3092e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.0892e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.8948e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 6.6728e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.5143e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.2899e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.1385e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.9525e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.8145e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.6805e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.6020e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.4742e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.3926e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.2474e-04\n",
            "27/27 [==============================] - 1s 3ms/step\n",
            "7/7 [==============================] - 0s 4ms/step\n",
            "Program is running for 10 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 3s 8ms/step - loss: 0.0458\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0079\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0048\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.0038\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0033\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.0029\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0026\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.0024\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0022\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0020\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.0018\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0016\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0015\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0014\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0012\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0012\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 9.8207e-04\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 9.0783e-04\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.3651e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.9416e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 7.3867e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 6.9076e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.5144e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 6.2044e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 5.8598e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.5233e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.3053e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.0354e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.8856e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 4.6656e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.5576e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.4245e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.2875e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.1855e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.0581e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.0227e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.9119e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.8824e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.7874e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.7511e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.7020e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.6265e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5894e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5708e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5122e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4887e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4578e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4464e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.3806e-04\n",
            "27/27 [==============================] - 1s 3ms/step\n",
            "7/7 [==============================] - 0s 7ms/step\n",
            "Program is running for 11 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 3s 6ms/step - loss: 0.0218\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0057\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0046\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0037\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0031\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0026\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0022\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0019\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0016\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0012\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 9.6590e-04\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 8.4711e-04\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 7.7503e-04\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 7.0297e-04\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.4805e-04\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.9544e-04\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.5059e-04\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.2177e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.9268e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.5975e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.4276e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.2104e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.0177e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.9566e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.8215e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.7254e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.6196e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4821e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.4272e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3932e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3005e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2935e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2118e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.1866e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.1467e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0778e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1046e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0854e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0102e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0516e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.9727e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.9630e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.9540e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 2.9024e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.9685e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.9065e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.8907e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.9121e-04\n",
            "27/27 [==============================] - 1s 3ms/step\n",
            "7/7 [==============================] - 0s 3ms/step\n",
            "Program is running for 12 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 3s 7ms/step - loss: 0.0150\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0035\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0024\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0019\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.0016\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0013\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.8608e-04\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.0148e-04\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 7.2673e-04\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 6.5110e-04\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 6.2266e-04\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.8433e-04\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.6234e-04\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.3570e-04\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.0974e-04\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.9135e-04\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.6915e-04\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.5950e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.4528e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.3537e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 4.2212e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 4.0992e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 4.0598e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.9726e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.8505e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.7900e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.6707e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.6404e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5960e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.5538e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5069e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4920e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4182e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.3463e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3278e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3007e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.2561e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2656e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2046e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.1658e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1488e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0774e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.1103e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0139e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 2.9927e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0280e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 2.9708e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 2.9158e-04\n",
            "27/27 [==============================] - 1s 3ms/step\n",
            "7/7 [==============================] - 0s 3ms/step\n",
            "Program is running for 13 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 3s 7ms/step - loss: 0.0201\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0040\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0031\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0026\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0021\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0018\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0015\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0013\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 9.6472e-04\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 8.3347e-04\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 7.5202e-04\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 6.6045e-04\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.0477e-04\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.4791e-04\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 5.0980e-04\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.6757e-04\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.3723e-04\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.2003e-04\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.9917e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.8161e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.6633e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.6703e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5200e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.4225e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4109e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2868e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2628e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2362e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1932e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.1760e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.1851e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.1347e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.0909e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.1819e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.0908e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0526e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0564e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0530e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 2.9942e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0518e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0385e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0101e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.0189e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0243e-04\n",
            "27/27 [==============================] - 1s 3ms/step\n",
            "7/7 [==============================] - 0s 3ms/step\n",
            "Program is running for 14 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 3s 7ms/step - loss: 0.0177\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0038\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0030\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0026\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0022\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0019\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0017\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0015\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0014\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0013\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 9.5272e-04\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 8.9403e-04\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 8.4276e-04\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 7.9691e-04\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 7.5409e-04\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 7.2176e-04\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 6.8338e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.5199e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.2312e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.0026e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.7589e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.5171e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.3384e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.1140e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 4.9313e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 4.7025e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.5632e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 4.4263e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.2792e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.1225e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 4.0108e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.8866e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.7692e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.7263e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.6468e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5650e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.5120e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5077e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.4408e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3956e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.3544e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.3119e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3262e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3225e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2843e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2259e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2653e-04\n",
            "27/27 [==============================] - 1s 3ms/step\n",
            "7/7 [==============================] - 0s 3ms/step\n",
            "Program is running for 15 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 3s 6ms/step - loss: 0.0305\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.0072\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0058\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0051\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0046\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0042\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0039\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0036\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0033\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0031\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0029\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0027\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0024\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.0022\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0020\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0018\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0017\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0016\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0015\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0012\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 9.5613e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 9.0709e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 8.6773e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 8.2219e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 7.8930e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 7.6712e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.2634e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 7.0619e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.7909e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.4999e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.3270e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 6.1214e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.8957e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.7511e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 5.5951e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.4654e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.3579e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.1996e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.0707e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.8678e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.6748e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.4443e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.1666e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.9284e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.7197e-04\n",
            "27/27 [==============================] - 1s 3ms/step\n",
            "7/7 [==============================] - 0s 3ms/step\n",
            "Program is running for 16 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 3s 6ms/step - loss: 0.0357\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0073\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0052\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0043\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0038\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0034\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0031\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0028\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0025\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0023\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0022\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0019\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0018\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0016\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0015\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0013\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0012\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 9.7319e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 9.1082e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 8.4981e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.9262e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.3890e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.9254e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 6.4973e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 6.1918e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.8474e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.5709e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.3595e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.0824e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.7673e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.5069e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.3112e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.0868e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.9300e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.7844e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.6303e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5685e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.4731e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4027e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3473e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3110e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2621e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2278e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1984e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1698e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.1036e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.1418e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.1084e-04\n",
            "27/27 [==============================] - 0s 3ms/step\n",
            "7/7 [==============================] - 0s 3ms/step\n",
            "Program is running for 17 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 2s 6ms/step - loss: 0.0173\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0039\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0033\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0028\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0024\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0020\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0018\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0016\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 9.0525e-04\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.1628e-04\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.4525e-04\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.8384e-04\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.2450e-04\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.8048e-04\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.5243e-04\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.1082e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.9147e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.5536e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.3462e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 4.2016e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.0277e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.8984e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.7311e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.6600e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5653e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4684e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.4231e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.3213e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2743e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.2601e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1589e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1888e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1590e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1226e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1078e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.0566e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.0489e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.0171e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0175e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 2.9893e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.0287e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 2.9977e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.9983e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 2.9631e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 2.9556e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.9471e-04\n",
            "27/27 [==============================] - 1s 3ms/step\n",
            "7/7 [==============================] - 0s 3ms/step\n",
            "Program is running for 18 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 2s 6ms/step - loss: 0.0262\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0042\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0031\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0024\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0019\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0016\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 9.0779e-04\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 7.9827e-04\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 7.2022e-04\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.8847e-04\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.2581e-04\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.9494e-04\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.6548e-04\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.2853e-04\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.0600e-04\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.8641e-04\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.6939e-04\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.5533e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.4413e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.3516e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.2285e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.1612e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.0371e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.0000e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.9055e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.8544e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.7495e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.7007e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.6575e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.6583e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.6254e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5004e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4663e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.4212e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.4028e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3942e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3607e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2745e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2911e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2658e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2416e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2349e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1766e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.1878e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.1614e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.1344e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1077e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0989e-04\n",
            "27/27 [==============================] - 0s 3ms/step\n",
            "7/7 [==============================] - 0s 3ms/step\n",
            "Program is running for 19 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 2s 6ms/step - loss: 0.0306\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0076\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0062\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0052\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0044\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0038\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0033\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0028\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0024\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0021\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0018\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0015\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0013\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 9.1359e-04\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.0235e-04\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.2028e-04\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.5885e-04\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.8389e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.3787e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.9149e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.5725e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.2195e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.9869e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.7691e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.6151e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4889e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2692e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.2470e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2016e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.0980e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0550e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0399e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 2.9876e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.9122e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.9005e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 2.8871e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 2.8719e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.7848e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.7999e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.7496e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 2.7314e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 2.7353e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.7191e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.7421e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 2.6714e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.6747e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.6659e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.6757e-04\n",
            "27/27 [==============================] - 1s 4ms/step\n",
            "7/7 [==============================] - 0s 6ms/step\n",
            "Progress: All works are done successfully, congratulations!!\n",
            "\n",
            "Running model with hidden neurons:  [128]\n",
            "\n",
            "\n",
            "Best Hyper_parameters used:  ['Adagrad', 0.01, 16]\n",
            "\n",
            "\n",
            "Progress: Performing data preparation steps.......\n",
            "\n",
            "Program is running for 0 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 3s 7ms/step - loss: 0.0340\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0062\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0040\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0033\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0029\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0026\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0023\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0021\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0019\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0017\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0016\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0015\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 9.7604e-04\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.9775e-04\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.3873e-04\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 7.8642e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 7.3245e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.8958e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.4555e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 6.1458e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.8160e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.5574e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 5.3206e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.0886e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.8668e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 4.6893e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 4.5986e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 4.3723e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.3010e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.2094e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 4.0660e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.9911e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.9205e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.8645e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.7987e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.7627e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.6882e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.6545e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.6767e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.5731e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5498e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5004e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.4592e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.4580e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.4040e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.3597e-04\n",
            "27/27 [==============================] - 1s 3ms/step\n",
            "7/7 [==============================] - 0s 5ms/step\n",
            "Program is running for 1 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 3s 7ms/step - loss: 0.0142\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0028\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0021\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0016\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 9.8941e-04\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.3836e-04\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 7.3686e-04\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 6.4043e-04\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.7326e-04\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.3392e-04\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 4.9058e-04\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.5618e-04\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 4.2563e-04\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 4.0898e-04\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.9007e-04\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.7512e-04\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.7701e-04\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.6580e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5532e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.4762e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4737e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.4283e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3101e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.2652e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2678e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1813e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.1804e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1788e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1361e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.1316e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.0823e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.1133e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1168e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0585e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1102e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1323e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1593e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0615e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.9757e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0193e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.0151e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0074e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0191e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.0351e-04\n",
            "27/27 [==============================] - 0s 3ms/step\n",
            "7/7 [==============================] - 0s 4ms/step\n",
            "Program is running for 2 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 2s 5ms/step - loss: 0.0305\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0047\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0035\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0029\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0024\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0020\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0018\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0015\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 9.5888e-04\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 8.5550e-04\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.9611e-04\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 7.2547e-04\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.7397e-04\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.2881e-04\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.8271e-04\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.5836e-04\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.2888e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.0442e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.9105e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.7152e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.5681e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.4517e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.3420e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.2624e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.1160e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.1165e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.0103e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.9469e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.8458e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.8226e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.8009e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.7093e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.6651e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.6553e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5909e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5727e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5198e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5661e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5168e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4823e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3974e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.4639e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.4047e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4146e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3956e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.3846e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.3806e-04\n",
            "27/27 [==============================] - 0s 4ms/step\n",
            "7/7 [==============================] - 0s 4ms/step\n",
            "Program is running for 3 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 2s 5ms/step - loss: 0.0344\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0077\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0052\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0043\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0037\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0033\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0029\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0026\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0023\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0021\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0019\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0017\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0015\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0014\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 9.6221e-04\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 8.9723e-04\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 8.3230e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 7.6442e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.0937e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.7189e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 6.4091e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 6.0549e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.7221e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.5404e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.2877e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.1076e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.9669e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.7732e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.6904e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.5570e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.4583e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.4005e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.3038e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.1988e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.1747e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.0887e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.0748e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.0024e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.9936e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.9264e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.9423e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.8761e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.8205e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.8060e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.7829e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.7146e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.7171e-04\n",
            "27/27 [==============================] - 0s 3ms/step\n",
            "7/7 [==============================] - 0s 5ms/step\n",
            "Program is running for 4 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 2s 6ms/step - loss: 0.0336\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0072\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0050\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0041\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0036\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0032\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0028\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0025\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0023\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0020\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0018\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0016\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0015\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 9.7002e-04\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.8181e-04\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.0180e-04\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 7.2968e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.7119e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.2764e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.7591e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.3012e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.0247e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 4.7554e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.5280e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.3155e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.0888e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.9782e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.7738e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.7166e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5877e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5180e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.4604e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4466e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.3653e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.3283e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.2813e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.2586e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2445e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.1933e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1812e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.1591e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1651e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1205e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0922e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.1108e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0754e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.1072e-04\n",
            "27/27 [==============================] - 0s 3ms/step\n",
            "7/7 [==============================] - 0s 5ms/step\n",
            "Program is running for 5 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 2s 5ms/step - loss: 0.0320\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0037\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0028\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0024\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0020\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0018\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0015\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0014\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0012\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 9.9813e-04\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 9.0657e-04\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.2927e-04\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 7.6006e-04\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.1120e-04\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.5783e-04\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 6.1792e-04\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.7102e-04\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.4658e-04\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.1956e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.9188e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.7141e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.6205e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.4249e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.2789e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.1439e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.0616e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.9609e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.8623e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.7561e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.6571e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.6871e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.5401e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5223e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.4623e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.3985e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3522e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3367e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2817e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2265e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.2447e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1677e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2028e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.1464e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1456e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0931e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.0772e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1011e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0365e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.1009e-04\n",
            "27/27 [==============================] - 0s 3ms/step\n",
            "7/7 [==============================] - 0s 4ms/step\n",
            "Program is running for 6 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 7s 7ms/step - loss: 0.0241\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.0062\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.0048\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0039\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0032\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0028\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0025\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0022\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0019\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0017\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0015\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 9.1386e-04\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.3037e-04\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.6403e-04\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.0518e-04\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.6338e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 6.3046e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.8612e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 5.5659e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.2200e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.0003e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.7482e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.6643e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 4.5452e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.3105e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.2172e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 4.1931e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 4.0667e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.9552e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.9018e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.8467e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.7861e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.7339e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.6651e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.6581e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.6305e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.6308e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.5878e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.5044e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.5218e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.5153e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.4050e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.4903e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.4069e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.4234e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.3809e-04\n",
            "27/27 [==============================] - 0s 3ms/step\n",
            "7/7 [==============================] - 0s 4ms/step\n",
            "Program is running for 7 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 2s 6ms/step - loss: 0.0505\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0086\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0054\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0044\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0038\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0034\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0030\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0028\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0025\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0023\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0021\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0019\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0018\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0016\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0015\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 9.5261e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.8349e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 8.3218e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 7.7586e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 7.3336e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.9016e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.5538e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 6.2354e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.9141e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.6235e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.4133e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.2351e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.9742e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.8191e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.6907e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.5443e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.4115e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.2967e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.1916e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.0450e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.9823e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.9040e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.8609e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.7773e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.7638e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.6718e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.6432e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5709e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5110e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5179e-04\n",
            "27/27 [==============================] - 1s 3ms/step\n",
            "7/7 [==============================] - 0s 5ms/step\n",
            "Program is running for 8 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 3s 6ms/step - loss: 0.0473\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0073\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0048\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0039\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0033\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0029\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0025\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0022\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0020\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0017\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0015\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 9.6956e-04\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.8290e-04\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.0001e-04\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.3395e-04\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.7754e-04\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.2453e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.8582e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.4741e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.2137e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.0117e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.7991e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.5765e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.3699e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 4.3304e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.1728e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.1118e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 3.9760e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.9551e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.8229e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.7759e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.7826e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.7451e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.7007e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.6801e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.6104e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5814e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.5865e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5442e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.5251e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.4824e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.4846e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.4655e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.4808e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.4752e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4230e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.4303e-04\n",
            "27/27 [==============================] - 0s 3ms/step\n",
            "7/7 [==============================] - 0s 5ms/step\n",
            "Program is running for 9 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 2s 6ms/step - loss: 0.0146\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0041\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0032\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0028\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0024\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0022\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0019\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0017\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0015\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 9.8900e-04\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.8369e-04\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.9950e-04\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 7.4212e-04\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.9037e-04\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.3521e-04\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.9952e-04\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.6212e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.4895e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.1659e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 4.9508e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 4.7490e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.5862e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.4431e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.3180e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.2159e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.1253e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.0542e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.9791e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.9967e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.9241e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 3.8421e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.7814e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.7725e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.7754e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.6869e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.6497e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.6365e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5284e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.6037e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5349e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5531e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.5000e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.4779e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.4491e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4397e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4361e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3729e-04\n",
            "27/27 [==============================] - 1s 3ms/step\n",
            "7/7 [==============================] - 0s 3ms/step\n",
            "Program is running for 10 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 3s 6ms/step - loss: 0.0328\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0081\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0053\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0040\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0033\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0028\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0025\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0022\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0019\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0017\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0016\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 9.2120e-04\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.4096e-04\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.7695e-04\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 6.9943e-04\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 6.5261e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.0693e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.7032e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.3087e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.0666e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.7901e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.5685e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.3900e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.2277e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.1194e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.9820e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.9313e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.7913e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.7024e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.6306e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.5879e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.5073e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.4894e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4656e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.3991e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.3839e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3265e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3298e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2990e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2475e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2400e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1886e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1990e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.1604e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.1503e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.1413e-04\n",
            "27/27 [==============================] - 1s 3ms/step\n",
            "7/7 [==============================] - 0s 3ms/step\n",
            "Program is running for 11 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 3s 6ms/step - loss: 0.0770\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0100\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0061\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0047\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0039\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0032\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0027\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0023\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0020\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0018\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0016\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 9.8221e-04\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.9770e-04\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 8.0256e-04\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.3483e-04\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 6.6157e-04\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.1106e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.6879e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.2502e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.9722e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.5856e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.3542e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.1214e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.9491e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.8532e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.7140e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.5824e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.4403e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.3978e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3719e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.2400e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.1827e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.1562e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.1049e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.0687e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.0363e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.9807e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.9502e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 2.9625e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 2.9603e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 2.9348e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 2.9126e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.8844e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.9005e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 2.8558e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.8965e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 2.8252e-04\n",
            "27/27 [==============================] - 0s 3ms/step\n",
            "7/7 [==============================] - 0s 4ms/step\n",
            "Program is running for 12 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 2s 5ms/step - loss: 0.0226\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0039\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0031\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0026\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0022\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0018\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0016\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 9.1393e-04\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.4068e-04\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.5771e-04\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.9259e-04\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.3127e-04\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.9235e-04\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.4596e-04\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.1638e-04\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.8449e-04\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.6464e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.4358e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.2449e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.0547e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.9527e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.8266e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.7075e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.6267e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5679e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4391e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4043e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.3436e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.2978e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.2905e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2318e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.1537e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1242e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1096e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0541e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.0279e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.0171e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.9582e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.9878e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.9723e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.8998e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.9081e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.9016e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 2.8689e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 2.8433e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.8564e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 2.8372e-04\n",
            "27/27 [==============================] - 1s 3ms/step\n",
            "7/7 [==============================] - 0s 4ms/step\n",
            "Program is running for 13 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 3s 5ms/step - loss: 0.0235\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0067\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0041\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0031\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0026\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0021\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0018\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0015\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 9.7692e-04\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.4195e-04\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.5005e-04\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 6.5699e-04\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.9579e-04\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.3156e-04\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 4.9325e-04\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 4.5833e-04\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.3185e-04\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.0459e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.8622e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.6803e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.5425e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4563e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3007e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.2409e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.1891e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1945e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.1089e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.0716e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.0329e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 2.9614e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.9776e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.9487e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.9341e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 2.8972e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 2.9324e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.8966e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 2.8227e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.8503e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.8715e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.8195e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 2.8112e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 2.7966e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.7909e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.7386e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 2.7818e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 2.7477e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 2.7260e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 2.7746e-04\n",
            "27/27 [==============================] - 1s 3ms/step\n",
            "7/7 [==============================] - 0s 3ms/step\n",
            "Program is running for 14 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 3s 5ms/step - loss: 0.0263\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0039\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0028\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0023\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0020\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0017\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0015\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 9.8663e-04\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.6828e-04\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 7.8692e-04\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.1107e-04\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.4018e-04\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.9245e-04\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.4841e-04\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.2433e-04\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.9631e-04\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 4.6758e-04\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 4.4515e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.3546e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.2347e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.0918e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 3.9472e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.8487e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.8148e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.7534e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.6190e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.6403e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.5851e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.5421e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.5221e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4780e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5422e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4088e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.5112e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.4015e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3197e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3586e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3136e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3418e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2594e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2816e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2558e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.2018e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2319e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.2154e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2007e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2122e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.1827e-04\n",
            "27/27 [==============================] - 1s 4ms/step\n",
            "7/7 [==============================] - 0s 6ms/step\n",
            "Program is running for 15 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 3s 6ms/step - loss: 0.0164\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0058\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0046\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0039\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0033\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0029\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0025\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0022\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0019\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0016\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 9.6890e-04\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 8.6593e-04\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.7278e-04\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 6.9643e-04\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.2872e-04\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.8581e-04\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.3522e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 4.9376e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.7028e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.3495e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.1826e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.0021e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.9162e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.7709e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.6387e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.5475e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.4862e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.4198e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3778e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3337e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.2815e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.2553e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2437e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2459e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.2227e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2021e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2122e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.1861e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1661e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1338e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.1001e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1468e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1481e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0952e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0745e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0716e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1376e-04\n",
            "27/27 [==============================] - 1s 4ms/step\n",
            "7/7 [==============================] - 0s 4ms/step\n",
            "Program is running for 16 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 3s 5ms/step - loss: 0.0285\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0048\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0037\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0032\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0028\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0025\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0022\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0020\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0017\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0015\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 9.9218e-04\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 8.9838e-04\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 8.2962e-04\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 7.5942e-04\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.0954e-04\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 6.5658e-04\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 6.1494e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.8386e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.5403e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.2411e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.0139e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.8009e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 4.6546e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.5729e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.3725e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.2596e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.1969e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.1306e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.0292e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.9471e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.9613e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.8838e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.8242e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.7555e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.7831e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.7100e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.7078e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.6404e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.6293e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.6053e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 3.5618e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.5858e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.5432e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.5120e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.4933e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.4505e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.4844e-04\n",
            "27/27 [==============================] - 0s 3ms/step\n",
            "7/7 [==============================] - 0s 4ms/step\n",
            "Program is running for 17 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 2s 5ms/step - loss: 0.0329\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0071\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0042\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0034\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0029\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0026\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0024\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0022\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0021\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0019\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0018\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0017\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0015\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 9.6043e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 9.0334e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.5049e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 7.9720e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.5592e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.1342e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 6.7455e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 6.4198e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.0788e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.8425e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.5496e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.2835e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.0661e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.8674e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.6865e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.5281e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.3652e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.2347e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.1127e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.0222e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.8621e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.7978e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.7372e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.6700e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 3.5858e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.5469e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.4829e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4506e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3895e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.3673e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.3178e-04\n",
            "27/27 [==============================] - 1s 3ms/step\n",
            "7/7 [==============================] - 0s 3ms/step\n",
            "Program is running for 18 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 3s 5ms/step - loss: 0.0371\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0066\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0046\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0037\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0031\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0026\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0023\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0020\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0018\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0016\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 9.6294e-04\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.6639e-04\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.0368e-04\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.3378e-04\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.8435e-04\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 6.3616e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.9473e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.5021e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 5.1467e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.8512e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.6342e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.4225e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.2044e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.0904e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.8991e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.8272e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.7199e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.6002e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.5687e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4525e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3723e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3479e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.2677e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2060e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1691e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1499e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.1042e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.0573e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 2.9972e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 3.0063e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 2.9543e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 2.8900e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 2.8946e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 2.9450e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.8994e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 2.8524e-04\n",
            "27/27 [==============================] - 0s 3ms/step\n",
            "7/7 [==============================] - 0s 3ms/step\n",
            "Program is running for 19 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 2s 6ms/step - loss: 0.0262\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0050\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0038\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0032\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0028\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0025\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0022\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0020\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0017\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0016\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 9.8875e-04\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 8.9648e-04\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 8.3889e-04\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.7287e-04\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 7.2147e-04\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 6.7061e-04\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 6.2765e-04\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.8735e-04\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.5507e-04\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 5.2383e-04\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.9887e-04\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.7865e-04\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.5709e-04\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.4295e-04\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.2886e-04\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 4.1497e-04\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 4.0284e-04\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.9007e-04\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.8436e-04\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.7813e-04\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.7066e-04\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.6173e-04\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.5654e-04\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.5023e-04\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.4771e-04\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.4562e-04\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.3789e-04\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.3910e-04\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.3619e-04\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3527e-04\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3208e-04\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.3047e-04\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.3022e-04\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2839e-04\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 3.2465e-04\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 3.2278e-04\n",
            "27/27 [==============================] - 0s 3ms/step\n",
            "7/7 [==============================] - 0s 4ms/step\n",
            "Progress: All works are done successfully, congratulations!!\n",
            "\n",
            "\n",
            "Best model and its avg rmse and minimum rmse):\n",
            " [128] 638.8905934405759 455.0789738949798\n",
            "Hyper_parameters:\n",
            " [['Adagrad', 0.01, 4], ['Adagrad', 0.01, 4], ['Adagrad', 0.01, 8], ['Adagrad', 0.01, 16], ['Adagrad', 0.01, 16]]\n",
            "\n",
            "Average scores:\n",
            "   layers         rmse       mape         R  elapsed_time\n",
            "0    [8]  1131.932187  13.749981  0.952513     76.346267\n",
            "1   [16]   881.455148  10.523754  0.959866     72.588168\n",
            "2   [32]   884.261022  10.879109  0.963680     37.599426\n",
            "3   [64]   724.499093   8.343379  0.972137     20.552652\n",
            "4  [128]   638.890593   7.458630  0.976586     20.850930\n",
            "\n",
            "Standard_deviations:\n",
            "   layers        rmse      mape         R  elapsed_time\n",
            "0    [8]  677.838963  9.335529  0.034876      9.204876\n",
            "1   [16]  395.251132  5.250608  0.022654      9.757793\n",
            "2   [32]  414.240164  5.820911  0.015548      4.334420\n",
            "3   [64]  229.822433  3.272393  0.008848      1.625973\n",
            "4  [128]  149.800907  2.171393  0.005588      2.075781\n",
            "\n",
            "Minimums:\n",
            "   layers        rmse      mape         R  elapsed_time\n",
            "0    [8]  398.081467  4.489636  0.838143     64.113760\n",
            "1   [16]  367.800496  3.861146  0.895410     51.875272\n",
            "2   [32]  423.535264  4.322096  0.920369     30.822969\n",
            "3   [64]  404.725373  4.016408  0.947932     17.700965\n",
            "4  [128]  455.078974  4.673378  0.967440     17.448217\n",
            "\n",
            "Maximums:\n",
            "   layers         rmse       mape         R  elapsed_time\n",
            "0    [8]  3265.418831  43.803915  0.981502     85.582210\n",
            "1   [16]  1793.575166  21.821690  0.985301     84.367493\n",
            "2   [32]  1868.617508  25.011016  0.981975     43.289879\n",
            "3   [64]  1082.511680  13.848990  0.985847     22.603601\n",
            "4  [128]  1008.235056  12.911687  0.985811     26.373942\n",
            "Progress: All works are done successfully, congratulations!!\n",
            "\n",
            " All parts are sucessefully completed!!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "lstm_input = {\n",
        "             'layers': [[8], [16], [32], [64], [128]],\n",
        "              'best_hyper_parameters':\n",
        "\n",
        "                             [\n",
        "                                  ['Adam', 0.01, 4], #8N-LSTM\n",
        "                                    ['Adam', 0.1, 16], #16N-LSTM\n",
        "                                    ['Adagrad', 0.01, 8],  #32N-LSTM\n",
        "                                    ['Adagrad', 0.01, 16], #64N-LSTM\n",
        "                                    ['Adagrad', 0.01, 16] #128N-LSTM\n",
        "                                    #['Adagrad', 0.001, 16],  # 200N-LSTM\n",
        "                                    #['Adagrad', 0.001, 16], #250N-LSTM\n",
        "\n",
        "\n",
        "                                      ]\n",
        "              }\n",
        "\n",
        "\n",
        "\n",
        "gru_input = {\n",
        "     'layers':  [[8], [16], [32], [64], [128]],\n",
        "\n",
        "      'best_hyper_parameters':   [\n",
        "                                    ['Adagrad', 0.01, 4], #8N-GRU\n",
        "                                    ['Adagrad', 0.01, 4], #16N-GRU\n",
        "                                    ['Adagrad', 0.01, 8],  #32N-GRU\n",
        "                                    ['Adagrad', 0.01, 16], #64N-GRU\n",
        "                                    ['Adagrad', 0.01, 16] #128N-GRU\n",
        "                                    #['Adagrad', 0.001, 16],  #200N-GRU\n",
        "                                    #['Adagrad', 0.001, 16], #250N-GRU\n",
        "\n",
        "                                      ]\n",
        "\n",
        "\n",
        "                   }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "RUN_LSTM_GRU_Models(lstm_input, gru_input,  data, time_step = 5, test_split = 0.2, epochs = 50, num_replicates = 20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWi5RQX92Qrc"
      },
      "source": [
        "# **Statistical Analysis and Visualization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkMu2X182Vlz"
      },
      "source": [
        "### **Visualization Supporting Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AjCR_qf2lMw"
      },
      "outputs": [],
      "source": [
        "\n",
        "def loss_plot(lstm_loss, gru_loss):\n",
        "\n",
        "\n",
        "  fig = plt.figure(figsize = (6,4))\n",
        "\n",
        "  colors = ['indigo', 'mediumblue']\n",
        "\n",
        "  plt.plot(lstm_loss, '--o',  linewidth = 1.5, color = colors[0])\n",
        "  plt.plot(gru_loss, '--o',  linewidth = 1.5, color = colors[1])\n",
        "\n",
        "\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss(MSE)\")\n",
        "  plt.legend(['LSTM', 'GRU'],  loc = 'upper right')\n",
        "  #plt.rcParams['axes.facecolor']='w'\n",
        "  plt.grid(color='#F5F5DC')\n",
        "  fig.savefig(output_dir_path +\"lstm_gru_loss_plot.pdf\",dpi=600)\n",
        "\n",
        "\n",
        "def avg_test_scores_plot(lstm_avg_scores, gru_avg_scores, cnn_avg_scores):\n",
        "\n",
        "  lstm_neurons = [8,16,32,64,128]\n",
        "  gru_neurons = [8,16,32,64,128]\n",
        "\n",
        "\n",
        "  fig = plt.figure(figsize = (19, 18))\n",
        "  plt.subplot(321)\n",
        "  plt.plot(lstm_avg_scores['layers'], lstm_avg_scores['rmse'], '--o', linewidth = 2, color = 'indigo')\n",
        "  plt.xticks([0,1,2,3,4], lstm_neurons)\n",
        "  plt.title(\"(a)\")\n",
        "  plt.xlabel(\"LSTM Neurons\")\n",
        "  plt.ylabel(\"Avg. RMSE\")\n",
        "  sns.set_style(\"whitegrid\")\n",
        "\n",
        "\n",
        "  plt.subplot(322)\n",
        "  plt.plot(lstm_avg_scores['layers'], lstm_avg_scores['mape'], '--o', linewidth = 2, color = 'darkgreen')\n",
        "  plt.xticks([0,1,2,3,4], lstm_neurons)\n",
        "  plt.title(\"(b)\")\n",
        "  plt.xlabel(\"LSTM Neurons\")\n",
        "  plt.ylabel(\"Avg. MAPE\")\n",
        "  sns.set_style(\"whitegrid\")\n",
        "\n",
        "\n",
        "  plt.subplot(323)\n",
        "  plt.plot(lstm_avg_scores['layers'], lstm_avg_scores['R'], '--o', linewidth = 2, color = 'darkred')\n",
        "  plt.xticks([0,1,2,3,4], lstm_neurons)\n",
        "  plt.title(\"(c)\")\n",
        "  plt.xlabel(\"LSTM Neurons\")\n",
        "  plt.ylabel(\"Avg. R\")\n",
        "  sns.set_style(\"whitegrid\")\n",
        "\n",
        "\n",
        "  plt.subplot(324)\n",
        "  plt.plot(gru_avg_scores['layers'], gru_avg_scores['rmse'], '--o', linewidth = 2, color = 'indigo')\n",
        "  plt.xticks([0,1,2,3,4], gru_neurons)\n",
        "  plt.title(\"(d)\")\n",
        "  plt.xlabel(\"GRU Neurons\")\n",
        "  plt.ylabel(\"Avg. RMSE\")\n",
        "  sns.set_style(\"whitegrid\")\n",
        "\n",
        "\n",
        "  plt.subplot(325)\n",
        "  plt.plot(gru_avg_scores['layers'], gru_avg_scores['mape'], '--o', linewidth = 2, color = 'darkgreen')\n",
        "  plt.xticks([0,1,2,3,4], gru_neurons)\n",
        "  plt.title(\"(e)\")\n",
        "  plt.xlabel(\"GRU Neurons\")\n",
        "  plt.ylabel(\"Avg. MAPE\")\n",
        "  sns.set_style(\"whitegrid\")\n",
        "\n",
        "\n",
        "  plt.subplot(326)\n",
        "  plt.plot(gru_avg_scores['layers'], gru_avg_scores['R'], '--o', linewidth = 2, color = 'darkred')\n",
        "  plt.xticks([0,1,2,3,4], gru_neurons)\n",
        "  plt.title(\"(f)\")\n",
        "  plt.xlabel(\"GRU Neurons\")\n",
        "  plt.ylabel(\"Avg. R\")\n",
        "  sns.set_style(\"whitegrid\")\n",
        "\n",
        "\n",
        "  fig.savefig(output_dir_path+\"multiple_avg_scores_plots.pdf\",dpi=600)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def best_model_true_vs_prediction_plot(y_train, y_test, lstm_train_pred, lstm_test_pred, gru_train_pred, gru_test_pred):\n",
        "\n",
        "  ##====== Visualizing true vs predicted plots ========#\n",
        "  fig1 = plt.figure(figsize= (19,4))\n",
        "  plt.subplot(121)\n",
        "\n",
        "  plt.scatter(y_train, lstm_train_pred, marker = \"+\" , color ='mediumblue')\n",
        "  identity_line = np.linspace(max(min(y_train), min(lstm_train_pred)), min(max(y_train), max(lstm_train_pred)))\n",
        "  plt.plot(identity_line, identity_line, color=\"red\", linestyle=\"dashed\", linewidth=2.5)\n",
        "\n",
        "  plt.xlabel(\"True\")\n",
        "  plt.ylabel(\"Predicted\")\n",
        "  plt.title(\"(a)\")\n",
        "\n",
        "\n",
        "  plt.subplot(122)\n",
        "  #sns.relplot(x = y_train_original, y = train_pred_original)\n",
        "  plt.scatter(y_train, gru_train_pred, marker = \"+\" , color ='mediumblue' )\n",
        "  identity_line = np.linspace(max(min(y_train), min(gru_train_pred)), min(max(y_train), max(gru_train_pred)))\n",
        "  plt.plot(identity_line, identity_line, color=\"red\", linestyle=\"dashed\", linewidth=2.5)\n",
        "\n",
        "  plt.xlabel(\"True\")\n",
        "  plt.ylabel(\"Predicted\")\n",
        "  plt.title(\"(b)\")\n",
        "\n",
        "\n",
        "  fig1.savefig(output_dir_path+\"true_vs_prediction_plot_train_data.pdf\", dpi=600)\n",
        "\n",
        "  ##====== Visualizing true vs predicted plots ========#\n",
        "  fig2 = plt.figure(figsize= (19,4))\n",
        "  plt.subplot(121)\n",
        "  #sns.relplot(x = y_train_original, y = train_pred_original)\n",
        "  plt.scatter(y_test, lstm_test_pred, marker = \"+\" , color ='mediumblue')\n",
        "  identity_line = np.linspace(max(min(y_test), min(lstm_test_pred)), min(max(y_test), max(lstm_test_pred)))\n",
        "  plt.plot(identity_line, identity_line, color= \"red\", linestyle=\"dashed\", linewidth=2.5)\n",
        "  plt.xlabel(\"True\")\n",
        "  plt.ylabel(\"Predicted\")\n",
        "  plt.title(\"(a)\")\n",
        "\n",
        "\n",
        "  plt.subplot(122)\n",
        "  #sns.relplot(x = y_train_original, y = train_pred_original)\n",
        "  plt.scatter(y_test, gru_test_pred, marker = \"+\" , color ='mediumblue' )\n",
        "  identity_line = np.linspace(max(min(y_test), min(gru_test_pred)), min(max(y_test), max(gru_test_pred)))\n",
        "  plt.plot(identity_line, identity_line, color=\"red\", linestyle=\"dashed\", linewidth=2.5)\n",
        "  plt.xlabel(\"True\")\n",
        "  plt.ylabel(\"Predicted\")\n",
        "  plt.title(\"(b)\")\n",
        "\n",
        "\n",
        "  fig2.savefig(output_dir_path+ \"true_vs_prediction_plot_test_data.pdf\", dpi=600)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def best_model_prediction_plot(time_step, data, y_train, y_test, lstm_train_pred, lstm_test_pred, gru_train_pred, gru_test_pred):\n",
        "\n",
        "  train_predict_plot_data = np.empty_like(data.values[:,0])# extracting closing price\n",
        "  train_predict_plot_data[:] = np.nan\n",
        "\n",
        "  test_predict_plot_data = np.empty_like(data.values[:,0])\n",
        "  test_predict_plot_data[:] = np.nan\n",
        "\n",
        "\n",
        "  fig1 = plt.figure(figsize = (19,5))\n",
        "\n",
        "  plt.subplot(121)\n",
        "\n",
        "  train_predict_plot_data[time_step:len(lstm_train_pred)+ time_step] =  lstm_train_pred\n",
        "  test_predict_plot_data[len(lstm_train_pred)+(time_step*2)+1:len(data.values)-1] = lstm_test_pred\n",
        "\n",
        "  plt.plot(data.values[:,0],'k', linewidth = 1.5)\n",
        "  plt.plot(train_predict_plot_data,'mediumblue',linewidth = 1.5)\n",
        "  plt.plot(test_predict_plot_data,'darkgreen',linewidth = 1.5)\n",
        "  plt.xlabel('')\n",
        "  plt.ylabel('Deaths')\n",
        "  plt.title(\"(a)\")\n",
        "  plt.legend(['True value', 'Predicted value in train set', 'Predicted value in test set'], loc = 'upper left')\n",
        "\n",
        "\n",
        "  plt.subplot(122)\n",
        "\n",
        "  train_predict_plot_data[time_step:len(gru_train_pred)+ time_step] =  gru_train_pred\n",
        "  test_predict_plot_data[len(gru_train_pred)+(time_step*2)+1:len(data.values)-1] = gru_test_pred\n",
        "\n",
        "  plt.plot(data.values[:,0],'k', linewidth = 1.5)\n",
        "  plt.plot(train_predict_plot_data,'mediumblue',linewidth = 1.5)\n",
        "  plt.plot(test_predict_plot_data,'darkgreen',linewidth = 1.5)\n",
        "  plt.xlabel('')\n",
        "  plt.ylabel('Deaths')\n",
        "  plt.title(\"(b)\")\n",
        "  plt.legend(['True value', 'Predicted value in train set', 'Predicted value in test set'], loc = 'upper left')\n",
        "\n",
        "\n",
        "\n",
        "  fig1.savefig(output_dir_path+\"best_model_predictions_plots_full_data.pdf\",dpi=600)\n",
        "\n",
        "\n",
        "\n",
        "  fig2 = plt.figure(figsize = (19,5))\n",
        "\n",
        "  plt.subplot(121)\n",
        "  plt.plot(data.values[len(lstm_train_pred)+(time_step*2)+1:-1, 0],'k',linewidth = 1.5)\n",
        "  plt.plot(lstm_test_pred,'darkgreen',linewidth = 1.5)\n",
        "  plt.xlabel('')\n",
        "  plt.ylabel('Deaths')\n",
        "  plt.title(\"(a)\")\n",
        "  plt.legend(['True value', 'Predicted value'], loc='upper left')\n",
        "\n",
        "  plt.subplot(122)\n",
        "  plt.plot(data.values[len(gru_train_pred)+(time_step*2)+1:-1, 0],'k',linewidth = 1.5)\n",
        "  plt.plot(gru_test_pred,'darkgreen',linewidth = 1.5)\n",
        "  plt.xlabel('')\n",
        "  plt.ylabel('Deaths')\n",
        "  plt.title(\"(b)\")\n",
        "  plt.legend(['True value', 'Predicted value'], loc='upper left')\n",
        "\n",
        "\n",
        "  fig2.savefig(output_dir_path+\"best_model_predictions_plots_test_data.pdf\",dpi=600)\n",
        "\n",
        "def best_model_comparative_rmse_boxplots(lstm_rmse, gru_rmse):\n",
        "  data = pd.DataFrame()\n",
        "  data['LSTM'] = lstm_rmse\n",
        "  data['GRU'] = gru_rmse\n",
        "\n",
        "  fig = plt.figure(figsize = (8,4))\n",
        "  p = plt.boxplot(data.T, patch_artist= True)\n",
        "  colors = ['mediumblue', 'darkred', 'darkgreen']\n",
        "  for i, box in enumerate(p['boxes']):\n",
        "    box.set(color= 'blue', linewidth = 1.2)\n",
        "    # change fill color\n",
        "    box.set(facecolor = colors[i])\n",
        "  plt.xticks([1,2], ['LSTM','GRU'])\n",
        "  plt.ylabel('RMSE')\n",
        "  fig.savefig(output_dir_path+\"comparative_rmse_boxplots.pdf\",dpi=600)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def best_model_comparative_rmse_boxplots(lstm_rmse, gru_rmse):\n",
        "  data = pd.DataFrame()\n",
        "  data['LSTM'] = lstm_rmse\n",
        "  data['GRU'] = gru_rmse\n",
        "\n",
        "  fig = plt.figure(figsize = (8,4))\n",
        "  p = plt.boxplot(data.T, patch_artist= True)\n",
        "  colors = ['mediumblue', 'darkred']\n",
        "  for i, box in enumerate(p['boxes']):\n",
        "    box.set(color= 'blue', linewidth = 1.2)\n",
        "    # change fill color\n",
        "    box.set(facecolor = colors[i])\n",
        "  plt.xticks([1,2], ['LSTM','GRU'])\n",
        "  plt.ylabel('RMSE')\n",
        "  fig.savefig(output_dir_path+\"best_model_comparative_rmse_boxplots.pdf\",dpi=600)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def all_scores_boxplot(lstm_rmse, lstm_mape, lstm_R,  gru_rmse, gru_mape, gru_R):\n",
        "\n",
        "  lstm_neurons = [8,16,32,64,128]\n",
        "  gru_neurons = [8,16,32,64,128]\n",
        "\n",
        "  fig = plt.figure(figsize = (19,19))\n",
        "  plt.subplot(321)\n",
        "  p1 = plt.boxplot(lstm_rmse, patch_artist=True)\n",
        "  for i, box in enumerate(p1['boxes']):\n",
        "    # change outline color\n",
        "    box.set(color= 'blue', linewidth = 1.2)\n",
        "    # change fill color\n",
        "    box.set(facecolor = 'mediumblue')\n",
        "  plt.xticks([1,2,3,4,5], lstm_neurons)\n",
        "  plt.title(\"(a)\")\n",
        "  plt.xlabel('Number of LSTM neurons')\n",
        "  plt.ylabel('RMSE')\n",
        "\n",
        "  plt.subplot(322)\n",
        "  p2 = plt.boxplot(lstm_mape, patch_artist=True)\n",
        "  for i, box in enumerate(p2['boxes']):\n",
        "    # change outline color\n",
        "    box.set(color= 'blue', linewidth = 1.2)\n",
        "    # change fill color\n",
        "    box.set(facecolor = 'indigo')\n",
        "  plt.xticks([1,2,3,4,5], lstm_neurons)\n",
        "  plt.title(\"(b)\")\n",
        "  plt.xlabel('Number of LSTM neurons')\n",
        "  plt.ylabel('MAPE')\n",
        "\n",
        "  plt.subplot(323)\n",
        "  p3 = plt.boxplot(lstm_R, patch_artist=True)\n",
        "  for i, box in enumerate(p3['boxes']):\n",
        "    # change outline color\n",
        "    box.set(color= 'blue', linewidth = 1.2)\n",
        "    # change fill color\n",
        "    box.set(facecolor = 'darkgreen')\n",
        "  plt.xticks([1,2,3,4,5], lstm_neurons)\n",
        "  plt.title(\"(c)\")\n",
        "  plt.xlabel('Number of LSTM neurons')\n",
        "  plt.ylabel('R')\n",
        "\n",
        "  plt.subplot(324)\n",
        "  p4 = plt.boxplot(gru_rmse, patch_artist=True)\n",
        "  for i, box in enumerate(p4['boxes']):\n",
        "    # change outline color\n",
        "    box.set(color= 'blue', linewidth = 1.2)\n",
        "    # change fill color\n",
        "    box.set(facecolor = 'mediumblue')\n",
        "  plt.xticks([1,2,3,4,5], gru_neurons)\n",
        "  plt.title(\"(d)\")\n",
        "  plt.xlabel('Number of GRU neurons')\n",
        "  plt.ylabel('RMSE')\n",
        "\n",
        "  plt.subplot(325)\n",
        "  p5 = plt.boxplot(gru_mape, patch_artist=True)\n",
        "  for i, box in enumerate(p5['boxes']):\n",
        "    # change outline color\n",
        "    box.set(color= 'blue', linewidth = 1.2)\n",
        "    # change fill color\n",
        "    box.set(facecolor = 'indigo')\n",
        "  plt.xticks([1,2,3,4,5], gru_neurons)\n",
        "  plt.title(\"(e)\")\n",
        "  plt.xlabel('Number of GRU neurons')\n",
        "  plt.ylabel('MAPE')\n",
        "\n",
        "  plt.subplot(326)\n",
        "  p6 = plt.boxplot(gru_R, patch_artist=True)\n",
        "  for i, box in enumerate(p6['boxes']):\n",
        "    # change outline color\n",
        "    box.set(color= 'blue', linewidth = 1.2)\n",
        "    # change fill color\n",
        "    box.set(facecolor = 'darkgreen')\n",
        "  plt.xticks([1,2,3,4,5], gru_neurons)\n",
        "  plt.title(\"(f)\")\n",
        "  plt.xlabel('Number of GRU neurons')\n",
        "  plt.ylabel('R')\n",
        "\n",
        "\n",
        "  fig.savefig(output_dir_path+\"all_scores_boxplots.pdf\",dpi=600)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def read_df_from_file(file_name):\n",
        "   return pd.read_csv(file_name)\n",
        "\n",
        "def comparative_qq_plots(data1, data2):\n",
        "  fig = plt.figure(figsize = (16,5))\n",
        "  ax1= fig.add_subplot(121)\n",
        "  scipy.stats.probplot(data1, dist=scipy.stats.norm, sparams=(0,1), plot=ax1)\n",
        "  ax1.get_lines()[0].set_marker('o')\n",
        "  ax1.get_lines()[0].set_markerfacecolor('mediumblue')\n",
        "  ax1.get_lines()[0].set_markersize(8.0)\n",
        "  ax1.get_lines()[1].set_linewidth(3.0)\n",
        "  #plt.xlabel('jpt')\n",
        "  #plt.ylabel('Close price')\n",
        "  plt.title(\"(a)\")\n",
        "  #plt.legend(['True value', 'Predicted value in train set', 'Predicted value in test set'], loc = 'upper left')\n",
        "\n",
        "  ax2= fig.add_subplot(122)\n",
        "  scipy.stats.probplot(data2, dist=scipy.stats.norm, sparams=(0,1), plot=ax2)\n",
        "  ax2.get_lines()[0].set_marker('o')\n",
        "  ax2.get_lines()[0].set_markerfacecolor('mediumblue')\n",
        "  ax2.get_lines()[0].set_markersize(8.0)\n",
        "  ax2.get_lines()[1].set_linewidth(3.0)\n",
        "  #plt.xlabel('jpt')\n",
        "  #plt.ylabel('Close price')\n",
        "  plt.title(\"(b)\")\n",
        "  #plt.legend(['True value', 'Predicted value in train set', 'Predicted value in test set'], loc = 'upper left')\n",
        "\n",
        "  fig.savefig(output_dir_path+ \"best_model_rmse_qq_plots.pdf\",dpi=600)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "import scipy\n",
        "\n",
        "def perform_normality_test(lstm_rmse, gru_rmse):\n",
        "  print(\"Performaing Normality Tests\\n\")\n",
        "  print(\"lstm_rmse: \")\n",
        "  print(scipy.stats.normaltest(lstm_rmse))\n",
        "  print(\"gru_rmse:\")\n",
        "  print(scipy.stats.normaltest(gru_rmse))\n",
        "\n",
        "\n",
        "def perform_pairwise_ttests(lstm_rmse, gru_rmse):\n",
        "\n",
        "  print(\"\\n Two-sample ttest between lstm_rmse and  gru_rmse\")\n",
        "  print(scipy.stats.ttest_ind(lstm_rmse, gru_rmse, equal_var = False))  # It does not require variences to be equal.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def statistical_analysis_and_visualization():\n",
        "\n",
        "  #========= Read Results Files =============================#\n",
        "  #Read loss files\n",
        "  lstm_loss = read_df_from_file(output_dir_path+'best_lstm_model_loss.csv')\n",
        "  gru_loss = read_df_from_file(output_dir_path+'best_gru_model_loss.csv')\n",
        "\n",
        "\n",
        "\n",
        "  #Read avg scores files\n",
        "  lstm_avg_scores = read_df_from_file(output_dir_path+'multiple_lstm_models_average_scores.csv')\n",
        "  gru_avg_scores = read_df_from_file(output_dir_path+'multiple_gru_models_average_scores.csv')\n",
        "\n",
        "\n",
        "\n",
        "  #Read true vs predicted values\n",
        "  y_train = read_df_from_file(output_dir_path+ 'y_train.csv')\n",
        "  y_test =  read_df_from_file(output_dir_path+ 'y_test.csv')\n",
        "  lstm_train_pred = read_df_from_file(output_dir_path+ 'best_lstm_model_train_predictions.csv')\n",
        "  lstm_test_pred = read_df_from_file(output_dir_path+ 'best_lstm_model_test_predictions.csv')\n",
        "  gru_train_pred = read_df_from_file(output_dir_path+ 'best_gru_model_train_predictions.csv')\n",
        "  gru_test_pred = read_df_from_file(output_dir_path+ 'best_gru_model_test_predictions.csv')\n",
        "\n",
        "\n",
        "  #Read best rmse files\n",
        "  best_lstm_rmse = read_df_from_file(output_dir_path+ 'best_lstm_model_all_rmse.csv')\n",
        "  best_gru_rmse = read_df_from_file(output_dir_path+ 'best_gru_model_all_rmse.csv')\n",
        "\n",
        "\n",
        "  # Read all scores files\n",
        "  lstm_all_rmse = read_df_from_file(output_dir_path+'multiple_lstm_models_all_rmse.csv')\n",
        "  lstm_all_mape = read_df_from_file(output_dir_path+'multiple_lstm_models_all_mape.csv')\n",
        "  lstm_all_R = read_df_from_file(output_dir_path+'multiple_lstm_models_all_R.csv')\n",
        "\n",
        "  gru_all_rmse = read_df_from_file(output_dir_path+'multiple_gru_models_all_rmse.csv')\n",
        "  gru_all_mape = read_df_from_file(output_dir_path+'multiple_gru_models_all_mape.csv')\n",
        "  gru_all_R = read_df_from_file(output_dir_path+'multiple_gru_models_all_R.csv')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #========= Create Visualization =============================#\n",
        "  #Create loss plots\n",
        "  loss_plot(lstm_loss.iloc[:, 1], gru_loss.iloc[:, 1])\n",
        "  #average score plot\n",
        "  avg_test_scores_plot(lstm_avg_scores, gru_avg_scores)\n",
        "  #tru vs  predicted\n",
        "  best_model_true_vs_prediction_plot(y_train.iloc[:,1],\n",
        "                                   y_test.iloc[:,1],\n",
        "                                   lstm_train_pred.iloc[:,1],\n",
        "                                   lstm_test_pred.iloc[:,1],\n",
        "                                   gru_train_pred.iloc[:,1],\n",
        "                                   gru_test_pred.iloc[:,1])\n",
        "\n",
        "  best_model_prediction_plot( 5, data,\n",
        "                                   y_train.iloc[:,1],\n",
        "                                   y_test.iloc[:,1],\n",
        "                                   lstm_train_pred.iloc[:,1],\n",
        "                                   lstm_test_pred.iloc[:,1],\n",
        "                                   gru_train_pred.iloc[:,1],\n",
        "                                   gru_test_pred.iloc[:,1],)\n",
        "\n",
        "  best_model_comparative_rmse_boxplots(best_lstm_rmse.iloc[:,1], best_gru_rmse.iloc[:,1])\n",
        "\n",
        "  all_scores_boxplot(lstm_all_rmse, lstm_all_mape,  lstm_all_R,\n",
        "                       gru_all_rmse, gru_all_mape, gru_all_R)\n",
        "\n",
        "  comparative_qq_plots(best_lstm_rmse.iloc[:,1], best_gru_rmse.iloc[:,1])\n",
        "  perform_normality_test(best_lstm_rmse.iloc[:,1], best_gru_rmse.iloc[:,1])\n",
        "  perform_pairwise_ttests(best_lstm_rmse.iloc[:,1], best_gru_rmse.iloc[:,1])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lKYLQekl5VEq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KHfTVdIw4oZ"
      },
      "outputs": [],
      "source": [
        "#statistical_analysis_and_visualization()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YRYb9nmPy7oU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVJ4hZ7AUK74"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aq-xTaqhXQW1"
      },
      "source": [
        "##  **Plot 1: Loss plot of the best LSTM and GRU Models**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puyoc-GcXTwq"
      },
      "outputs": [],
      "source": [
        "\n",
        "def read_df_from_file(file_name):\n",
        "   return pd.read_csv(file_name)\n",
        "\n",
        "lstm_loss = read_df_from_file(output_dir_path+'best_lstm_model_loss.csv')\n",
        "gru_loss = read_df_from_file(output_dir_path+'best_gru_model_loss.csv')\n",
        "\n",
        "\n",
        "def loss_plot(lstm_loss, gru_loss):\n",
        "  fig = plt.figure(figsize = (6,4))\n",
        "\n",
        "  colors = ['indigo', 'mediumblue']\n",
        "\n",
        "  plt.plot(lstm_loss, '--o',  linewidth = 1.5, color = colors[0])\n",
        "  plt.plot(gru_loss, '--o',  linewidth = 1.5, color = colors[1])\n",
        "\n",
        "\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss(MSE)\")\n",
        "  plt.legend(['LSTM', 'GRU'],  loc = 'upper right')\n",
        "  plt.grid(color='#F5F5DC')\n",
        "  fig.savefig(output_dir_path +\"lstm_gru_loss_plot.png\",dpi=600)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "D7HWkiPjX71Z",
        "outputId": "5498126b-16a5-4a87-a306-2f1b79669b6f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "loss_plot(lstm_loss.iloc[:, 1], gru_loss.iloc[:, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkgIl28gYqk4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn-sH8FsY0jM"
      },
      "source": [
        "##  **Plot 2: Average Scores Plot**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7Dze40IY2Ko"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Read avg scores files\n",
        "lstm_avg_scores = read_df_from_file(output_dir_path+'multiple_lstm_models_average_scores.csv')\n",
        "gru_avg_scores = read_df_from_file(output_dir_path+'multiple_gru_models_average_scores.csv')\n",
        "\n",
        "\n",
        "def avg_test_scores_plot(lstm_avg_scores, gru_avg_scores):\n",
        "\n",
        "  lstm_neurons = [8, 16, 32, 64, 128]\n",
        "  gru_neurons =  [8, 16, 32, 64, 128]\n",
        "\n",
        "\n",
        "  fig = plt.figure(figsize = (20, 5))\n",
        "  plt.subplot(131)\n",
        "  plt.plot(lstm_avg_scores['layers'], lstm_avg_scores['rmse'], '--o', linewidth = 2.5, color = 'indigo')\n",
        "  plt.plot(gru_avg_scores['layers'], gru_avg_scores['rmse'], '--o', linewidth = 2.5, color = 'darkgreen')\n",
        "\n",
        "  plt.xticks([0,1,2,3,4], lstm_neurons) ############ I removed an extra 6 at the end\n",
        "  plt.title(\"(a)\")\n",
        "  plt.xlabel(\"Neurons\")\n",
        "  plt.ylabel(\"Avg. RMSE\")\n",
        "  plt.legend(['LSTM', 'GRU'],  loc = 'best')\n",
        "  sns.set_style(\"whitegrid\")\n",
        "\n",
        "  plt.subplot(132)\n",
        "  plt.plot(lstm_avg_scores['layers'], lstm_avg_scores['mape'], '--o', linewidth = 2.5, color = 'indigo')\n",
        "  plt.plot(gru_avg_scores['layers'], gru_avg_scores['mape'], '--o', linewidth = 2.5, color = 'darkgreen')\n",
        "  plt.xticks([0,1,2,3,4], lstm_neurons)\n",
        "  plt.title(\"(b)\")\n",
        "  plt.xlabel(\"Neurons\")\n",
        "  plt.ylabel(\"Avg. MAPE\")\n",
        "  plt.legend(['LSTM', 'GRU'],  loc = 'best')\n",
        "  sns.set_style(\"whitegrid\")\n",
        "\n",
        "\n",
        "  plt.subplot(133)\n",
        "  plt.plot(lstm_avg_scores['layers'], lstm_avg_scores['R'], '--o', linewidth = 2.5, color = 'indigo')\n",
        "  plt.plot(gru_avg_scores['layers'], gru_avg_scores['R'], '--o', linewidth = 2.5, color = 'darkgreen')\n",
        "  plt.xticks([0,1,2,3,4], lstm_neurons)\n",
        "  plt.title(\"(c)\")\n",
        "  plt.xlabel(\"Neurons\")\n",
        "  plt.ylabel(\"Avg. R\")\n",
        "  plt.legend(['LSTM', 'GRU'],  loc = 'best')\n",
        "  sns.set_style(\"whitegrid\")\n",
        "\n",
        "  fig.savefig(output_dir_path+\"multiple_avg_scores_plots.png\",dpi=600)\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "tr1U-r5cZey6",
        "outputId": "e35fcfa0-60ed-43cf-cf7f-b797ce29b3c2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x500 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "avg_test_scores_plot(lstm_avg_scores, gru_avg_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wo9sPIlmZkFN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6J9ZFOZSb8K8"
      },
      "source": [
        "##  **Plot 3: True Vs Predicted Plots in Train and Test Data(By best LSTM, GRU)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06YEsCNzb-Ik"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Read true vs predicted values\n",
        "y_train = read_df_from_file(output_dir_path+ 'y_train.csv')\n",
        "y_test =  read_df_from_file(output_dir_path+ 'y_test.csv')\n",
        "lstm_train_pred = read_df_from_file(output_dir_path+ 'best_lstm_model_train_predictions.csv')\n",
        "lstm_test_pred = read_df_from_file(output_dir_path+ 'best_lstm_model_test_predictions.csv')\n",
        "gru_train_pred = read_df_from_file(output_dir_path+ 'best_gru_model_train_predictions.csv')\n",
        "gru_test_pred = read_df_from_file(output_dir_path+ 'best_gru_model_test_predictions.csv')\n",
        "\n",
        "\n",
        "\n",
        "def best_model_true_vs_prediction_plot(y_train, y_test, lstm_train_pred, lstm_test_pred, gru_train_pred, gru_test_pred):\n",
        "\n",
        "  ##====== Visualizing true vs predicted plots ========#\n",
        "  fig1 = plt.figure(figsize= (19,4))\n",
        "  plt.subplot(121)\n",
        "\n",
        "  plt.scatter(y_train, lstm_train_pred, marker = \"+\" , color ='mediumblue')\n",
        "  identity_line = np.linspace(max(min(y_train), min(lstm_train_pred)), min(max(y_train), max(lstm_train_pred)))\n",
        "  plt.plot(identity_line, identity_line, color=\"red\", linestyle=\"dashed\", linewidth=2.5)\n",
        "\n",
        "  plt.xlabel(\"True\")\n",
        "  plt.ylabel(\"Predicted\")\n",
        "  plt.title(\"(a)\")\n",
        "\n",
        "\n",
        "  plt.subplot(122)\n",
        "  #sns.relplot(x = y_train_original, y = train_pred_original)\n",
        "  plt.scatter(y_train, gru_train_pred, marker = \"+\" , color ='mediumblue' )\n",
        "  identity_line = np.linspace(max(min(y_train), min(gru_train_pred)), min(max(y_train), max(gru_train_pred)))\n",
        "  plt.plot(identity_line, identity_line, color=\"red\", linestyle=\"dashed\", linewidth=2.5)\n",
        "\n",
        "  plt.xlabel(\"True\")\n",
        "  plt.ylabel(\"Predicted\")\n",
        "  plt.title(\"(b)\")\n",
        "\n",
        "  fig1.savefig(output_dir_path+\"true_vs_prediction_plot_train_data.png\", dpi=600)\n",
        "\n",
        "  ##====== Visualizing true vs predicted plots ========#\n",
        "  fig2 = plt.figure(figsize= (19,4))\n",
        "  plt.subplot(121)\n",
        "  #sns.relplot(x = y_train_original, y = train_pred_original)\n",
        "  plt.scatter(y_test, lstm_test_pred, marker = \"+\" , color ='mediumblue')\n",
        "  identity_line = np.linspace(max(min(y_test), min(lstm_test_pred)), min(max(y_test), max(lstm_test_pred)))\n",
        "  plt.plot(identity_line, identity_line, color= \"red\", linestyle=\"dashed\", linewidth=2.5)\n",
        "  plt.xlabel(\"True\")\n",
        "  plt.ylabel(\"Predicted\")\n",
        "  plt.title(\"(a)\")\n",
        "\n",
        "\n",
        "  plt.subplot(122)\n",
        "  #sns.relplot(x = y_train_original, y = train_pred_original)\n",
        "  plt.scatter(y_test, gru_test_pred, marker = \"+\" , color ='mediumblue' )\n",
        "  identity_line = np.linspace(max(min(y_test), min(gru_test_pred)), min(max(y_test), max(gru_test_pred)))\n",
        "  plt.plot(identity_line, identity_line, color=\"red\", linestyle=\"dashed\", linewidth=2.5)\n",
        "  plt.xlabel(\"True\")\n",
        "  plt.ylabel(\"Predicted\")\n",
        "  plt.title(\"(b)\")\n",
        "\n",
        "\n",
        "  fig2.savefig(output_dir_path+ \"true_vs_prediction_plot_test_data.png\", dpi=600)\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        },
        "id": "BerXrMm5cUuP",
        "outputId": "8f72cf74-523e-41ad-f998-49f5f1ab337c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1900x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1900x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "#tru vs  predicted\n",
        "best_model_true_vs_prediction_plot(y_train.iloc[:,1],\n",
        "                                   y_test.iloc[:,1],\n",
        "                                   lstm_train_pred.iloc[:,1],\n",
        "                                   lstm_test_pred.iloc[:,1],\n",
        "                                   gru_train_pred.iloc[:,1],\n",
        "                                   gru_test_pred.iloc[:,1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jcsTlzOcart"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejanwI6_gOaC"
      },
      "source": [
        "## **Plot 4: Prediction Plots Given by Best LSTM and GRU Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcAVOoW5gi4J"
      },
      "outputs": [],
      "source": [
        "def best_model_prediction_plot(time_step, data, y_train, y_test, lstm_train_pred, lstm_test_pred, gru_train_pred, gru_test_pred):\n",
        "\n",
        "  train_predict_plot_data = np.empty_like(data.values[:,0])# extracting closing price\n",
        "  train_predict_plot_data[:] = np.nan\n",
        "\n",
        "  test_predict_plot_data = np.empty_like(data.values[:,0])\n",
        "  test_predict_plot_data[:] = np.nan\n",
        "\n",
        "\n",
        "  fig1 = plt.figure(figsize = (19,5))\n",
        "\n",
        "  plt.subplot(121)\n",
        "\n",
        "  train_predict_plot_data[time_step:len(lstm_train_pred)+ time_step] =  lstm_train_pred\n",
        "  test_predict_plot_data[len(lstm_train_pred)+(time_step*2)+1:len(data.values)-1] = lstm_test_pred\n",
        "\n",
        "  plt.plot(data.values[:,0],'k', linewidth = 1.5)\n",
        "  plt.plot(train_predict_plot_data,'mediumblue',linewidth = 1.5)\n",
        "  plt.plot(test_predict_plot_data,'darkgreen',linewidth = 1.5)\n",
        "  plt.xlabel('')\n",
        "  plt.ylabel('Deaths')\n",
        "  plt.title(\"(a)\")\n",
        "  plt.legend(['True value', 'Predicted value in train set', 'Predicted value in test set'], loc='upper right')\n",
        "\n",
        "\n",
        "  plt.subplot(122)\n",
        "\n",
        "  train_predict_plot_data[time_step:len(gru_train_pred)+ time_step] =  gru_train_pred\n",
        "  test_predict_plot_data[len(gru_train_pred)+(time_step*2)+1:len(data.values)-1] = gru_test_pred\n",
        "\n",
        "  plt.plot(data.values[:,0],'k', linewidth = 1.5)\n",
        "  plt.plot(train_predict_plot_data,'mediumblue',linewidth = 1.5)\n",
        "  plt.plot(test_predict_plot_data,'darkgreen',linewidth = 1.5)\n",
        "  plt.xlabel('')\n",
        "  plt.ylabel('Deaths')\n",
        "  plt.title(\"(b)\")\n",
        "  plt.legend(['True value', 'Predicted value in train set', 'Predicted value in test set'], loc='upper right')\n",
        "\n",
        "\n",
        "\n",
        "  fig1.savefig(output_dir_path+\"best_model_predictions_plots_full_data.png\",dpi=600)\n",
        "\n",
        "\n",
        "\n",
        "  fig2 = plt.figure(figsize = (19,5))\n",
        "\n",
        "  plt.subplot(121)\n",
        "  plt.plot(data.values[len(lstm_train_pred)+(time_step*2)+1:-1, 0],'k',linewidth = 1.5)\n",
        "  plt.plot(lstm_test_pred,'darkgreen',linewidth = 1.5)\n",
        "  plt.xlabel('')\n",
        "  plt.ylabel('Deaths')\n",
        "  plt.title(\"(a)\")\n",
        "  plt.legend(['True value', 'Predicted value'], loc='upper right')\n",
        "\n",
        "  plt.subplot(122)\n",
        "  plt.plot(data.values[len(gru_train_pred)+(time_step*2)+1:-1, 0],'k',linewidth = 1.5)\n",
        "  plt.plot(gru_test_pred,'darkgreen',linewidth = 1.5)\n",
        "  plt.xlabel('')\n",
        "  plt.ylabel('Deaths')\n",
        "  plt.title(\"(b)\")\n",
        "  plt.legend(['True value', 'Predicted value'], loc='upper right')\n",
        "\n",
        "\n",
        "  fig2.savefig(output_dir_path+\"best_model_predictions_plots_test_data.png\",dpi=600)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 804
        },
        "id": "VMu9LTvZgrNP",
        "outputId": "acf88702-77e5-496a-8fc9-fc3b6a62631c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1900x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1900x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "best_model_prediction_plot( 5, data,\n",
        "                                   y_train.iloc[:,1],\n",
        "                                   y_test.iloc[:,1],\n",
        "                                   lstm_train_pred.iloc[:,1],\n",
        "                                   lstm_test_pred.iloc[:,1],\n",
        "                                   gru_train_pred.iloc[:,1],\n",
        "                                   gru_test_pred.iloc[:,1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EnnfOQFgxpi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5D0WMCIiA12"
      },
      "source": [
        "## **Plot 5: Comparative RMSE Boxplots of Best LSTM and GRU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OokBnuRhiBcI"
      },
      "outputs": [],
      "source": [
        "#Read best rmse files\n",
        "best_lstm_rmse = read_df_from_file(output_dir_path+ 'best_lstm_model_all_rmse.csv')\n",
        "best_gru_rmse = read_df_from_file(output_dir_path+ 'best_gru_model_all_rmse.csv')\n",
        "\n",
        "def best_model_comparative_rmse_boxplots(lstm_rmse, gru_rmse ):\n",
        "  data = pd.DataFrame()\n",
        "  data['LSTM'] = lstm_rmse\n",
        "  data['GRU'] = gru_rmse\n",
        "\n",
        "  fig = plt.figure(figsize = (8,4))\n",
        "  p = plt.boxplot(data.T, patch_artist= True)\n",
        "  colors = ['mediumblue', 'darkred', 'darkgreen']\n",
        "  for i, box in enumerate(p['boxes']):\n",
        "    box.set(color= 'blue', linewidth = 1.2)\n",
        "    # change fill color\n",
        "    box.set(facecolor = colors[i])\n",
        "  plt.xticks([1,2], ['LSTM','GRU'])\n",
        "  plt.ylabel('RMSE')\n",
        "  fig.savefig(output_dir_path+\"comparative_rmse_boxplots.png\",dpi=600)\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "wESyTLmdiknq",
        "outputId": "b2f05ef2-b8c1-45ca-95fa-85094c1da99a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-6d434c86d6f0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_model_comparative_rmse_boxplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_lstm_rmse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_gru_rmse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-43-4a0051d30234>\u001b[0m in \u001b[0;36mbest_model_comparative_rmse_boxplots\u001b[0;34m(lstm_rmse, gru_rmse)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'blue'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# change fill color\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfacecolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'LSTM'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'GRU'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RMSE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "best_model_comparative_rmse_boxplots(best_lstm_rmse.iloc[:,1], best_gru_rmse.iloc[:,1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4sfj9jHipxb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SojKo4vrjAfQ"
      },
      "source": [
        "## **Plot 6: Statistical Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uwo-ugE6jHOD"
      },
      "outputs": [],
      "source": [
        "\n",
        "import scipy\n",
        "\n",
        "def perform_normality_test(lstm_rmse, gru_rmse):\n",
        "  print(\"Performaing Normality Tests\\n\")\n",
        "  print(\"lstm_rmse: \")\n",
        "  print(scipy.stats.normaltest(lstm_rmse))\n",
        "  print(\"gru_rmse:\")\n",
        "  print(scipy.stats.normaltest(gru_rmse))\n",
        "\n",
        "def perform_pairwise_ttests(lstm_rmse, gru_rmse):\n",
        "\n",
        "  print(\"\\n Two-sample ttest between lstm_rmse and  gru_rmse\")\n",
        "  print(scipy.stats.ttest_ind(lstm_rmse, gru_rmse, equal_var = False))  # It does not require variences to be equal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "lSdLTW8gjFEo",
        "outputId": "deb6ff0d-1f35-42e4-f236-a898e2deb646"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x500 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABS0AAAHWCAYAAACSb6y2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAC9PUlEQVR4nOzdeVyVdfr/8dd9ENFEQRBtXEtNzdxQm8rBpTSnNBVcxslMMdPGUsk0M9zTzMyN0rLRclwqxxVxWr+W02Tqr809cyvLpRSQRUwQOPfvj1tJ5D4Kejgc4f18PHwY5/6c+1znGqcur/uzGKZpmoiIiIiIiIiIiIh4CUdRByAiIiIiIiIiIiJyKTUtRURERERERERExKuoaSkiIiIiIiIiIiJeRU1LERERERERERER8SpqWoqIiIiIiIiIiIhXUdNSREREREREREREvIqaliIiIiIiIiIiIuJV1LQUERERERERERERr6KmpYiIiIiIiIiIiHgVNS1FRNxg4cKFPPDAAzidzny/JzMzk7Zt2/LOO+8UYmQiIiIiUlJcWpMeO3aM+vXr89Zbb131fTNnzqRXr14eiFBEJP/UtBQRuU5paWksWrSIQYMG4XDk/1+rvr6+DBgwgAULFpCRkVGIEYqIiIhIcXetNSlA//79+eGHH/j0008LKToRkYJT01JE5DqtXr2arKwsHnrooQK/t3v37iQlJbFhw4ZCiExERERESorrqUlDQkJo3749b7/9diFEJiJybdS0FBG5TmvXruW+++7Dz8+vwO+tUKECYWFhrFu3rhAiExEREZGS4npqUoAHH3yQb7/9lqNHj7o5MhGRa6OmpYjIdTh69Cj79++nVatWuV5/6623+Pvf/85dd91FkyZN6N69Ox999JHtPVq1asW3335LcnKyByIWERERkeLGVU160b/+9S/uvfdemjRpQt++fTlw4ECeMRffqyXiIuIt1LQUEbkO27dvB6Bhw4a5Xl+6dCm33347w4cP55lnnsHHx4eoqCj++9//5rnHHXfcgWmaOfcSERERESkIVzUpQGxsLEuXLqVPnz4MHjyYgwcP0r9/fxISEnKNK1++PDVr1uS7777zSMwiIldTqqgDEBG5kf34448AVK9ePdfrH3/8MWXKlMn5+ZFHHqF79+4sXryYdu3a5Rpbo0YNAA4dOsS9995buAGLiIiISLHjqiYF+OWXX/jkk0+oUqUKAG3atKFXr14sXLiQ559/PtfYGjVqcOjQocIPWEQkHzTTUkTkOiQnJ1OqVCnKlSuX6/VLG5YpKSmcOXOGFi1a8P333+e5R0BAAABJSUmFG6yIiIiIFEuualKADh065DQsAZo0aULTpk35/PPP84ytUKGCalIR8RqaaSkiUgg2bdrEG2+8wb59+zh//nzO64Zh5BlrmqbLayIiIiIi16NWrVp5Xrvlllv48MMP87xumqZqUhHxGmpaiohch8DAQLKyskhLS8Pf3x+Ab775hiFDhnDnnXcyceJEQkJC8PX1Zc2aNfznP//Jc4+UlBQAKlas6NHYRURERKR4sKtJr0VqaqpqUhHxGmpaiohch9q1awNw7NgxGjRoAFj7Wfr5+fHWW29RunTpnLFr1qyxvcexY8cAqFOnTiFHKyIiIiLFkV1NetHPP/+cZ/yRI0eoVq1antft3i8iUlS0p6WIyHUIDQ0FYM+ePTmv+fj4YBgG2dnZOa8dO3aMTz/91PYee/fuxTAMmjVrVqixioiIiEjxZFeTXrRx40ZOnjyZ8/OuXbvYuXMnbdq0yTXuzJkz/PLLLzn3EhEpampaiohchxo1alCvXj22bt2a81rbtm05d+4cjz/+OO+99x7z5s3jb3/7GzVr1rS9x5YtW2jevLmW4oiIiIjINbGrSS+qWbMmDz/8MAsXLmT+/PkMGjSIwMBAHn/88VzjtmzZgmmatG/f3lNhi4hckZqWIiLXqUePHnz22Wekp6cDcM899/Diiy+SkJDAtGnTeP/99xk1ahT3339/nveeOXOGzZs3ExER4emwRURERKQYubwmvSg8PJxHH32Ud955hwULFlC3bl2WLFlC5cqVc4376KOPaNGihcsH7SIinmaYF4+tFRGRa3LmzBk6dOjAqFGj6NWrV4He+69//YtFixaxceNGypQpU0gRioiIiEhxdz01aXx8PO3bt2f27Nl06NChkCIUESkYzbQUEblO5cuXZ+DAgbz11ls4nc58vy8zM5N//etfDBkyRA1LEREREbku11qTAixZsoR69eqpYSkiXkUzLUVERERERERERMSraKaliIiIiIiIiIiIeBU1LUVERERERERERMSrqGkpIiIiIiIiIiIiXkVNSxEREREREREREfEqpYryw9PS0oiJiWHjxo0kJibSsGFDoqOjadKkCQCmafLqq6+yatUqUlNTad68OZMmTeKWW27JuUdycjJTpkxh06ZNOBwOOnbsyNixYylXrly+43A6nWRlZeFwODAMw91fU0RERKRQmaaJ0+mkVKlSOBx6Jn0jUj0qIiIiNzp316RFenr4008/zcGDB5k0aRKVK1cmLi6Of/3rX3zwwQdUqVKFf/7zn/zzn/9k+vTpVK9enZiYGA4cOMAHH3yAn58fAI8//jjx8fG88MILZGZmEh0dTePGjZk1a1a+4zh//jy7d+8urK8pIiIi4hGNGzemdOnSRR2GXAPVoyIiIlJcuKsmLbKmZXp6Os2bN+f111+nXbt2Oa93796d1q1b8/TTT9O6dWsGDBjAwIEDAThz5gytWrVi+vTpdO7cmcOHD9OpUydWr15N48aNAfjf//7H4MGD+fzzz6lSpUq+YsnKymLnzp2AlVgfHx/3ftkbWHZ2Nrt371ZeLqO82FNe7Ckvrik39pQXe8qLvYt5AWjatCmlShXpQhq5RqpHXdP/9+0pL64pN/aUF3vKiz3lxTXlxl5h1KRFVtVmZWWRnZ2dM2PyIj8/P7777juOHTtGfHw8rVq1yrlWvnx5mjZtyvbt2+ncuTPbt2+nQoUKOQ1LgFatWuFwONi1axf3339/vmLREhwREREpDlTT3Lgu/d/Ox8dHfwmyobzYU15cU27sKS/2lBd7yotryo1r7qpJi6xp6e/vT2hoKK+//jq1a9emUqVK/Oc//2HHjh3UrFmT+Ph4AIKDg3O9Lzg4mISEBAASEhIICgrKdb1UqVIEBATkvL+gtCzHnvJiT3mxp7zYU15cU27sKS/2lBcRERERkeKvSNcPzZgxg+joaNq0aYOPjw8NGzakc+fO7N27t8hi0vTe3DTt2Z7yYk95sae8uKbc2FNe7Ckv9i5diiMiIiIiUlwUadOyZs2aLF++nN9//520tDQqV67M008/TY0aNQgJCQEgMTGRypUr57wnMTGRBg0aAFCpUiVOnz6d655ZWVmkpKTkvL+gNL3XnvJiT3mxp7zYU15cU27sKS/2lBcRERERkeLv+s8fd4ObbrqJypUrk5KSwubNm2nfvj3Vq1cnJCSErVu35oxLS0tj586dhIaGAhAaGkpqaip79uzJGbNt2zacTidNmjTx+PcQERERERERERGR61ekMy2/+OILTNPk1ltv5ZdffmHGjBnUrl2b7t27YxgG/fr144033qBWrVpUr16dmJgYKleuTIcOHQCoU6cOrVu3Zvz48UyePJnMzEymTJlC586d831yuIiIiIiIiIiIiHiXIm1anjlzhtmzZ/Pbb78RGBhIx44dGTFiBL6+vgAMGjSIc+fOMWHCBFJTU2nRogWLFi3KdeL4zJkzmTJlCv3798fhcNCxY0fGjRtXVF9JRERERERERERErlORNi07depEp06dXF43DIOoqCiioqJcjgkMDGTWrFmFEZ6IiIiIiIiIiIgUAa/Y01JERERERERERETkoiKdaSkiIiJSXKWnw6pVEBsLiYkmwcEG4eHQqxeUKVPU0YmIiIhIcXej16NqWoqIiIi4WVwcREaaJCUZOBxpOJ3ncThKs3atP1FRJkuWGHTpUtRRioiIiEhxVRzqUS0PFxEREXGjuDgIDzdJTk4B9uJ0HgCOXPh9L8nJKXTrZhIXV8SBioiIiEixVFzqUTUtRURERNwkPd16og0pmOaPQMZlIzIuvJ5CZKRJerrnYxQRERGR4qs41aNqWoqIiIi4yapVkJRkYJrHrzjONI+TlGSwerWHAhMRERGREqE41aNqWoqIiIi4SWwsOBxp5H2ifbkMHI401q3zQFAiIiIiUmIUp3pUTUsRERERN0lMNHE6z+drrNOZyenTZiFHJCIiIiIlSXGqR9W0FBEREXGT4GADh6N0vsY6HL4EBRmFHJF8/fXX/OMf/yAsLIz69euzcePGXNdN0yQmJoawsDCaNGlCZGQkR44cyTUmOTmZkSNH0rx5c1q2bEl0dDRnz57NNeaHH36gT58+NG7cmLZt27Jw4cLC/moiIiIieRSnelRNSxERERE3CQ8Hp9Mf8LvKSD+cTn8iIjwQVAn3+++/U79+fSZOnGh7feHChSxbtoxJkyaxcuVKypYty8CBA8nI+GNJ1ahRozh06BCLFy9mwYIFfPPNN0yYMCHnelpaGgMHDqRq1aqsXbuW0aNHM2/ePP79738X+vcTERERuVRxqkfVtBQRERFxk169oGJFE8OodsVxhlGNihVNevb0UGAlWNu2bRkxYgT3339/nmumabJ06VKGDBlChw4daNCgATNmzODUqVM5MzIPHz7MF198wdSpU2natCktW7Zk3LhxvP/++5w8eRKAuLg4MjMzmTZtGrfddhudO3fm0UcfZfHixR79riIiIiLFqR4tVdQBiIiIiBQXZcrAkiUG3boFYBi1L5zaeOkm6H4XCsgAliwxKFOmiAIVAI4dO0Z8fDytWrXKea18+fI0bdqU7du307lzZ7Zv306FChVo3LhxzphWrVrhcDjYtWsX999/Pzt27KBly5aULv3HUqywsDAWLlxISkoKAQEBBYorOzv7+r9cMXIxH8pLbsqLa8qNPeXFnvJiT3lxzdtz4+sLb78N3btfvR59+20nvr7gjq9SGPlQ01JERETEjbp0gdhYg8jIAJKSAnE40nA6M3E4fHE6/QkMNFmyxKBLl6KOVOLj4wEIDg7O9XpwcDAJCQkAJCQkEBQUlOt6qVKlCAgIyHl/QkIC1atXzzWmUqVKOdcK2rTcvXt3gcaXFMqLPeXFNeXGnvJiT3mxp7y45s25qVEDZs4MYNKkWpw5c0eeetTfP5PJkw9To0YKO3YUdbSuqWkpIiIi4mZdu8KJEwarV8O6df6cPm0SFGQQEQE9e2qGpVxZ48aN8fHxKeowvEZ2dja7d+9WXi6jvLim3NhTXuwpL/aUF9dulNw0awZPPAFr1jiJjS2XU4+Ghzvp0cNBmTK3uvXzLubFndS0FBERESkEZcpA377WL/DeUxlLspCQEAASExOpXLlyzuuJiYk0aNAAsGZMnj59Otf7srKySElJyXl/pUqVcmZmXnTx54szLgvCx8fHq/8SVFSUF3vKi2vKjT3lxZ7yYk95ce1GyE25ctCvn/Xrj3r0xqlLdRCPiIiIiJRI1atXJyQkhK1bt+a8lpaWxs6dOwkNDQUgNDSU1NRU9uzZkzNm27ZtOJ1OmjRpAkCzZs345ptvyMzMzBmzZcsWbr311gIvDRcRERERi5qWIiIiIlJsnT17ln379rFv3z7AOnxn3759nDhxAsMw6NevH2+88Qaffvop+/fvZ/To0VSuXJkOHToAUKdOHVq3bs348ePZtWsX3377LVOmTKFz585UqVIFgC5duuDr68vYsWM5ePAgH3zwAUuXLmXAgAFF9r1FREREbnRaHi4iIiIixdaePXvoZ62JAuCll14CICIigunTpzNo0CDOnTvHhAkTSE1NpUWLFixatAg/P7+c98ycOZMpU6bQv39/HA4HHTt2ZNy4cTnXy5cvz1tvvcULL7xA9+7dqVixIk8++SS9e/f23BcVERERKWbUtBQRERGRYuuuu+5i//79Lq8bhkFUVBRRUVEuxwQGBjJr1qwrfk6DBg149913rzlOEREREclNy8NFRERERERERETEq6hpKSIiIiIiIiIiIl5FTUsRERERERERERHxKmpaioiIiIiIiIiIiFdR01JERERERERERES8ipqWIiIiIiIiIiIi4lXUtBQRERERERERERGvoqaliIiIiIiIiIiIeBU1LUVERERERERERMSrqGkpIiIiIiIiIiIiXkVNSxEREREREREREfEqalqKiIiIiIiIiIiIV1HTUkRERERERERERLyKmpYiIiIiIiIiIiLiVdS0FBEREREREREREa+ipqWIiIiIiIiIiIh4FTUtRURERERERERExKuoaSkiIiIiIiIiIiJeRU1LERERERERERER8SpqWoqIiIiIiIiIiIhXUdNSREREREREREREvIqaliIiIiIiIiIiIuJV1LQUERERERERERERr6KmpYiIiIiIiIiIiHgVNS1FRERERERERETEq6hpKSIiIiIiIiIiIl6lSJuW2dnZzJ07l/vuu48mTZrQoUMH5s+fj2maOWNM0yQmJoawsDCaNGlCZGQkR44cyXWf5ORkRo4cSfPmzWnZsiXR0dGcPXvWw99GRERERERERERE3KFIm5YLFy7kvffeY8KECXzwwQeMGjWKRYsWsWzZslxjli1bxqRJk1i5ciVly5Zl4MCBZGRk5IwZNWoUhw4dYvHixSxYsIBvvvmGCRMmFMVXEhERERERERERketUpE3L7du30759e9q1a0f16tV54IEHCAsLY9euXYA1y3Lp0qUMGTKEDh060KBBA2bMmMGpU6fYuHEjAIcPH+aLL75g6tSpNG3alJYtWzJu3Djef/99Tp48WZRfT0RERERERERERK5BqaL88NDQUFauXMlPP/3Erbfeyg8//MC3337LmDFjADh27Bjx8fG0atUq5z3ly5enadOmbN++nc6dO7N9+3YqVKhA48aNc8a0atUKh8PBrl27uP/++wsUU3Z2tnu+XDFxMR/KS27Kiz3lxZ7y4ppyY095sae82FM+RERERKQ4KtKm5eDBg0lLS+PBBx/Ex8eH7OxsRowYQdeuXQGIj48HIDg4ONf7goODSUhIACAhIYGgoKBc10uVKkVAQEDO+wti9+7d1/JVij3lxZ7yYk95sae8uKbc2FNe7CkvIiIiIiLFX5E2LT/88EM2bNjArFmzqFu3Lvv27eOll16icuXKREREFElMjRs3xsfHp0g+2xtlZ2eze/du5eUyyos95cWe8uKacmNPebGnvNi7mBcRERERkTxSU2HDBsjIgL//HW66qagjyrcibVrOmDGDwYMH07lzZwDq16/PiRMnePPNN4mIiCAkJASAxMREKleunPO+xMREGjRoAEClSpU4ffp0rvtmZWWRkpKS8/6C8PHx0V+EbCgv9pQXe8qLPeXFNeXGnvJiT3kRERERkYJKT4dVqyA2FhITTYKDDcLDoVcvKFOmqKMrBKYJ77wDzz4Lv/1mvbZ6Nbz/PhhG0caWT0V6EE96ejrGZYny8fHBNE0AqlevTkhICFu3bs25npaWxs6dOwkNDQWsfTFTU1PZs2dPzpht27bhdDpp0qSJB76FiIiIiIiIiIh4q7g4qFrVpF8/iI1N4/PPk4iNTaNfP+v1DRuKOkI327ED2rSBRx/9o2EJ8OGHcP58kYVVUEU60/Lee+9lwYIFVK1aNWd5+OLFi+nRowcAhmHQr18/3njjDWrVqkX16tWJiYmhcuXKdOjQAYA6derQunVrxo8fz+TJk8nMzGTKlCl07tyZKlWqFOXXExERERERERGRIhQXB+HhJpACHMfpzADA6QTwIzm5Gt26BRAba3DhiJUbV1ISjB8Pb7xx8Qvm9uc/g5+f5+O6RkXatBw3bhwxMTFMnjw5Zwl47969eeqpp3LGDBo0iHPnzjFhwgRSU1Np0aIFixYtwu+SJM+cOZMpU6bQv39/HA4HHTt2ZNy4cUXxlURERERERERExAukp0NkpNWwNM0fbUZkYJo/Yhi1iYwM4MQJ48ZcKu50wuLFMGYMXDi4Oo9OneCttzwb13Uq0qalv78/Y8eOZezYsS7HGIZBVFQUUVFRLscEBgYya9aswghRRERERERERERuQKtWQVKSARy/4jjTPE5SUiCrV0Pfvp6JzW2+/hqeesr63c6tt0JMDHTp4tm43KBI97QUEREREREREREpDLGx4HCkARlXGZmBw5HGunUeCMpdEhJg0CC46y77hmWZMvDCC/D99zdkwxKKeKaliIiIFH8l7qRGEREREfEKiYkmTmf+Dp5xOjM5fdoEvPxk7exsePNNGDfO2sPSTkQEzJ4Nt9zi0dDcTU1LERERKTRxcdY+QklJBg5HGk7neRyO0qxd609UlMmSJcaN+uBXRERERLxccLCBw1Ha9kyayzkcvgQFeXnD8ssvYehQ63RwO/Xqwauvwl//6tGwCouWh4uIiEihuHhSY3JyCrAXp/MAcOTC73tJTk6hWzeTuLgiDlREREREiqXwcHA6/YGrnZjth9PpT0SEB4K6Fr/9Bv37Q1iYfcOyXDl4+WXYvbvYNCxBTUsREREpBHlParx8H6GMC6+nEBlpkp7u+RhFREREpHjr1QsqVjQxjGpXHGcY1ahY0aRnTw8Fll+ZmTBnjjWDculS+zEPPwz798Po0VC6tGfjK2RqWoqIiIjbXTyp0TTzc1KjwerVHgpMxEZaWhovvvgi9957L02aNOHvf/87u3btyrlumiYxMTGEhYXRpEkTIiMjOXLkSK57JCcnM3LkSJo3b07Lli2Jjo7m7NmzHv4mIiIicqkyZWDJEgMIwDBqk3fGpd+F1wNYssTwrv3WN22CZs3gmWfgzJm81xs1ssa8+y5Uu3JT9kalpqWIiIi4XbE+qVGKnXHjxrFlyxZmzJjBhg0b+Mtf/sKAAQM4efIkAAsXLmTZsmVMmjSJlStXUrZsWQYOHEhGxh9/vkeNGsWhQ4dYvHgxCxYs4JtvvmHChAlF9ZVERETkgi5dIDbWIDAwALgDh6MecOuF3+8gMDCA9eu9aJ/1o0ehd2+47z7r5O/LVagAc+fCd99Bu3aejs6j1LQUERERt7u2kxpFPC89PZ1PPvmEZ599ljvvvJNatWoxbNgwatWqxbvvvotpmixdupQhQ4bQoUMHGjRowIwZMzh16hQbN24E4PDhw3zxxRdMnTqVpk2b0rJlS8aNG8f777+f0/gUERGRotO1K5w4YbBsGYSH+9OuXSDh4f4sW2a97hUNy4wMmD4dGjSAlSvtx0RGwoEDEBUFvr4eDa8o6PRwERERcbtid1KjFFtZWVlkZ2fj55d7uZifnx/fffcdx44dIz4+nlatWuVcK1++PE2bNmX79u107tyZ7du3U6FCBRo3bpwzplWrVjgcDnbt2sX9999foJiys7Ov70sVMxfzobzkpry4ptzYU17sKS/2imNefH2t7R8ffjjvtYJ8zULJzUcf4RgxAuPgQdvLZmgozpgYuFiPeOH/LoXxZ0VNSxEREXG78HBYu/biSY1XWiLu5Sc1SrHn7+9PaGgor7/+OrVr16ZSpUr85z//YceOHdSsWZP4+HgAgoODc70vODiYhIQEABISEggKCsp1vVSpUgQEBOS8vyB27959jd+meFNe7Ckvrik39pQXe8qLPeXFNXfkpvTx49SYPZvAzz+3vZ5VoQLHn3yShIgI8PGxPzm8GFPTUkRERNyuVy+IijJJTq524ZRwe4ZRjcBAk549NdNSis6MGTOIjo6mTZs2+Pj40LBhQzp37szevXuLJJ7GjRvj4+NTJJ/tjbKzs9m9e7fychnlxTXlxp7yYk95sae8uOaW3Jw7h/HKKxgzZmCkp+e5bBoG5qBBGC+8QPVKlah+nTF7wsW8uJOaliIiIuJ2F09q7NbNOqnROkX80hmXfhhGNbzypEYpcWrWrMny5cv5/fffSUtLo3Llyjz99NPUqFGDkJAQABITE6lcuXLOexITE2nQoAEAlSpV4vTp07numZWVRUpKSs77C8LHx0d/QbShvNhTXlxTbuwpL/aUF3vKi2vXlBvThLg4ePppOHLEfsxdd2HMm4fRsuX1hnjD00E8IiIiUihuuJMapcS76aabqFy5MikpKWzevJn27dtTvXp1QkJC2Lp1a864tLQ0du7cSWhoKAChoaGkpqayZ8+enDHbtm3D6XTSpEkTj38PERER8UIHD0LnztY+SnYNy5AQePtt2LIF1LAENNNSRERECtHFkxpXr4Z16/w5fdokKMggIgJ69tQMS/EOX3zxBaZpcuutt/LLL78wY8YMateuTffu3TEMg379+vHGG29Qq1YtqlevTkxMDJUrV6ZDhw4A1KlTh9atWzN+/HgmT55MZmYmU6ZMoXPnzlSpUqWIv52IiIgUqbNn4cUXYdYsOH8+73WHA556Cl54AQIDPR6eN1PTUkRERApVmTLQt6/1C7R3pXifM2fOMHv2bH777TcCAwPp2LEjI0aMwNfXF4BBgwZx7tw5JkyYQGpqKi1atGDRokW5ThyfOXMmU6ZMoX///jgcDjp27Mi4ceOK6iuJiIhIUTNNWLUKRo6EY8fsx7RuDfPmgVZm2FLTUkRERERKtE6dOtGpUyeX1w3DICoqiqioKJdjAgMDmTVrVmGEJyIiIjea77+HYcPgs8/sr//pTzBzJjz8MBh6qO+K9rQUERERERERERG5Xqmp1szKpk3tG5alSsGoUbB/P/Tpo4blVWimpYiIiIiIiIiIyLUyTVi+HEaPht9+sx/ToQO8+ircfrtnY7uBqWkpIiIiIiIiIiJyLXbsgKFD4csv7a/XqAFz5kD37ppZWUBaHi4iIiIiIiIiIlIQSUlWs7JFC/uGZenSMG4c7NsHPXqoYXkNNNNSREREREREREQkP5xOgmNjcSxYAAkJ9mM6dYKYGKhb17OxFTNqWoqIiIiIiIiIiFzN11/jePJJbvnmG/vrtWtbzcqHHvJsXMWUloeLiIiIiIiIiIi4Eh8PgwbBXXdh2DUsy5SBF16AvXvVsHQjzbQUERERERERERG5XHY2LFhg7U2ZnGw/pnt3mDULbrnFk5GVCGpaioiIiIiIiIiIXOrLL62DdnbssL1s1q+P8eqr0LGjZ+MqQbQ8XERERKSwnDoFcXFw+HBRRyIiIiIi+fHrr9CvH4SF2TYszXLlODZsGM7t29WwLGRqWoqIiIi4W1ISjBljLRPq1g3q1YOPPy7qqERERETElcxMmDMH6teHZcvsxzz8MM7vv+dk//5QurRn4yuB1LQUERERcZdz52DGDOvkyJdftn4GcDrh7beLNjYRERERsffZZ9CsGTzzDJw5k/d6o0bw3//Cu+9CtWqejq7E0p6WIiIiItcrKwsWL4ZJk+DECfsxjRt7NCQRERERuYqjR2HUKFi50v56hQrWqeBPPgm+vp6NTdS0FBEREblmpglr1sDYsXDggP0YHx946il4/nnPxiYiIiIi9jIyYPZsmDoVfv/dfkxkJEyfDlWqeDQ0+YOaliIiIiLX4rPPrH0rv/7a9ZhevaxiuF49z8UlIiIiIq599BEMHw4HD9pfb94c5s2De+7xbFySh/a0FBERESmI776zTops3951w7JDB+vaypVqWIqIiIh4g59+gvBwePBB+4ZlUBAsWABffaWGpZfQTEsRERGR/Dh4EMaPh3//2/WYFi2sZUQdOnguLhERERFx7dw564DEl1+G9PS81w0DBg+GF1+E4GDPxycuqWkpIiIiciW//mptwL5okXXgjp3bbrMK3R49wKGFLCIiIiJFzjQhLg6efhqOHLEfc9ddMH++9eBZvI6aliIiIiJ2kpNhxgyYO9d6Qm/nT3+yTgwfMEAnSoqIiIh4i4MHrX0rP/rI/npIiDXzsn9/PXD2YmpaioiIiFzq3Dnrifu0aZCUZD8mIMA6hGf4cLjpJs/GJyIiIiL2zp61Vr/MmgXnz+e97nDA0KEweTIEBno8PCkYNS1FREREwFr6vWSJNXPy2DH7MWXKWI3K556zNmsXERERkaJnmrBqFYwc6bqOa93aOhW8SRPPxibXTE1LERERKdlME9atg7Fj4Ycf7Mf4+MBjj8GECVC9umfjExERERHXvv8ehg2Dzz6zv/6nP8HMmfDww9ahO3LD0MJ9ERERKbk2bYK777YO0HHVsOzRA/bsgX/+Uw1LEREREW+RmmrNrGza1L5hWaoUPPss7N8PffqoYXkD0kxLERERKXm2b4fnn4ePP3Y95r77YPp0uPNOz8UlIiIiIldmmrB8OYweDb/9Zj+mQwd49VW4/XbPxiZupaaliIiIlByHD8O4cbBihesxoaFWs/L++/VEXkRERMSb7NhhHaTz5Zf212vUgDlzoHt31XHFgJaHi4iISPH322/w1FPQoIHrhmXduta1b76Bjh1V6IqIiIh4i6Qkq1nZooV9w7J0aevB9L591tY+quOKBc20FBERkeIrJQVeecV64v777/Zjbr4ZJk6EgQPB19ez8YmIiIiIa04nvP22ta1PQoL9mE6dICbGegAtxYqaliIiIlL8pKfD66/DtGmQmGg/pkIFeO45iIqCcuU8G5+IiIiIXNnXX1srZb7+2v567dpWs/Khhzwbl3iMmpYiIiJSfGRnw9Kl1szJo0ftx/j5wbBhMGYMBAd7Nj4RERERubL4eIiOhrfesg7duVyZMtb1Z5+1/lmKrSJtWt53330cP348z+t9+vRh4sSJZGRkMH36dD744APOnz9PWFgYEydOpFKlSjljT5w4waRJk/h//+//cdNNNxEeHs7IkSMpVUr9WBERkRLDNGH9ehg7Fr7/3n6MwwEDBlgNzRo1PBufiIiIiFxZdjYsWGDtTZmcbD+me3eYNQtuucWTkUkRKdLO3urVq8nOzs75+eDBgwwYMIAHHngAgGnTpvH5558zd+5cypcvz5QpUxg6dCgrLmygn52dzRNPPEGlSpVYsWIFp06d4rnnnsPX15dnnnmmSL6TiIiIeNj//mfNmty61fWYiAh48UW4/XbPxSUiIiIi+fPll9ZBOzt22F+vXx9efdU6LFFKjCI9PTwoKIiQkJCcX5s2baJmzZr8+c9/5syZM6xZs4YxY8Zwzz330KhRI6ZNm8b27dvZceEP8ebNmzl06BCvvPIKt99+O23btiUqKop33nmH8+fPF+VXExERkcK2c6e18Xrbtq4blu3awbZtsHatGpYiIiIi3ubXX6FfPwgLs29YlisHL78Mu3apYVkCec0a6vPnzxMXF8eAAQMwDIM9e/aQmZlJq1atcsbUqVOHqlWrsmPHDpo1a8aOHTuoV69eruXiYWFhTJo0iUOHDtGwYcMCx3HpzE/5Ix/KS27Kiz3lxZ7y4ppyY095sZeTl4MHMaZMwXjvPQy7fY4As1kznFOnwl//CoZhLTcqpvTnRERERG44mZkwb561bc+ZM/ZjHn4YXnkFqlXzbGziNbymablx40bOnDlDREQEAAkJCfj6+lKhQoVc44KDg4mPj88Zc2nDEsj5+eKYgtq9e/c1va+4U17sKS/2lBd7yotryo095SW3UomJ1HjrLUqtXYsjK8t2TEa1ahwfMoSkjh2tPSx37vRwlCIiIiJyRZs2WUvBXe1D3qiR1dBs29azcYnX8Zqm5Zo1a2jTpg1VqlQp0jgaN26Mj49PkcbgTbKzs9m9e7fychnlxZ7yYk95cU25sae8XCY1FWPWLIy5czHOnrUdYlapgjluHKUGDqRW6dLU8nCIReninxcRERERr3b0KIwaBStX2l+vUAFeeAGefBJ8fT0bm3glr2haHj9+nC1btvDaa6/lvFapUiUyMzNJTU3NNdsyMTGRkJCQnDG7du3Kda+EhASAnDEF5ePjo78g2lBe7Ckv9pQXe8qLa8qNvRKfl4wMeOMN6wCdC/99z6NCBXj2WYynn8bw9/dsfCIiIiJydRkZMGcOTJkCv/9uPyYyEqZPhyKeyCbepUgP4rlo7dq1BAcH065du5zXGjVqhK+vL1sv2Vj/xx9/5MSJEzRr1gyAZs2aceDAARITE3PGbNmyBX9/f+rWreup8EVERMSdsrNhyRKoVw9GjLBvWJYuDc88A4cPw7hxoIaliIiIiFdIT4dly6BHDxjd5ENOVGoMzz9v37Bs3hy2bIHFi9WwlDyKfKal0+lk7dq1hIeHU6rUH+GUL1+eHj16MH36dAICAvD392fq1KmEhobmNC3DwsKoW7cuo0eP5tlnnyU+Pp65c+fyyCOPULp06SL6RiIiInJNTBM2bIDoaNi7136Iw4HZrx+OyZOhZk0PBygiIiIiVxIXB5GRJhWSfiaGoXTjffuBFSvCtGkwaBCU5JVFckVF3rTcsmULJ06coEePHnmuRUdH43A4GD58OOfPnycsLIyJEyfmXPfx8WHBggVMmjSJ3r17U7ZsWSIiIhg+fLgnv4KIiIhcr82bYcwY+PJLl0PMbt34vk8fGvTooeJWRERExMvExcHfu/3OaKbwHHMoS0aeMU4MfvnrYG5ZPhUuO1hZ5HJF3rQMCwtj//79ttf8/PyYOHFirkbl5apVq8bChQsLKzwREREpTLt3WzMr//Mf12PatIHp03H++c+k79jhsdBEREREJH/Sz5n8u8969hLFrfxiO2YbjRjGqxz+qh0n/A3KeDhGufF4xZ6WIiIiUsIcOQL9+kHTpq4blk2bwgcfwH//C/fc48noRERERCS/Dh4k8e7OvHM2wrZheYqKDGAirVjMN1QmKclg9eoiiFNuOGpaioiIiOecOgVRUdYhO8uWWftYXu7WW2H5cvjuO3jwQTAMz8cpIiIiIld29qy1YqZRI6rt+jDP5WwcvEZv6rGOf9EVEweQgcORxrp1ng9XbjxFvjxcRERESoAzZ2DWLOtXWpr9mMqVYfx4GDzYOh1cRERERLyPacKqVTByJBw7Zjvkf4QylOfYzW15rjmdmZw+bQJ6MC1XppmWIiIiUngyMuDVV6FOHZg82b5hWb68de3wYRg6VA1L8ajs7Gzmzp3LfffdR5MmTejQoQPz58/HvGQWsGmaxMTEEBYWRpMmTYiMjOTIkSO57pOcnMzIkSNp3rw5LVu2JDo6mrNnz3r424iIiBSy77+HDh2gd2/bhuUJKtGHqbRloW3DEsDh8CUoSA1LuTo1LUVERMT9srOt5d8NGljLwePj844pXRqeftpqVk6YAP7+Hg9TZOHChbz33ntMmDCBDz74gFGjRrFo0SKWLVuWa8yyZcuYNGkSK1eupGzZsgwcOJCMjD9ORR01ahSHDh1i8eLFLFiwgG+++YYJEyYUxVcSERFxv9RUa2Zl06bw2Wd5Ljt9SvEKo6jP+7zHg7ieRemH0+lPREShRivFhJqWIiIi4j6mCe+/D6Gh1kE7l81GA6w9Kvv3hwMHYM4cCAnxeJgiF23fvp327dvTrl07qlevzgMPPEBYWBi7du0CrFmWS5cuZciQIXTo0IEGDRowY8YMTp06xcaNGwE4fPgwX3zxBVOnTqVp06a0bNmScePG8f7773Py5Mmi/HoiIiLXxzStB9H168Ps2ZCVlXdMhw5kfrOLlyrO4KxR74q3M4xqVKxo0rNnIcUrxYr2tBQRERH3+PJLGDMGNm92PaZrV3jxRWjUyHNxiVxBaGgoK1eu5KeffuLWW2/lhx9+4Ntvv2XMmDEAHDt2jPj4eFq1apXznvLly9O0aVO2b99O586d2b59OxUqVKBx48Y5Y1q1aoXD4WDXrl3cf//9BYopOzvbPV+umLiYD+UlN+XFNeXGnvJiT3mxl52dTdn9+zGGD4ctW2zHmDVq4Jw1CyIiKGUYvP22k+7dAzCM2pjmcSDjktF+GEY1IIC333bi62stzLkR6c+MvcLIh5qWIiIicn327LFOjtywwfWYsDCYPh3+8hfPxSWSD4MHDyYtLY0HH3wQHx8fsrOzGTFiBF27dgUg/sLWBsHBwbneFxwcTEJCAgAJCQkEBQXlul6qVCkCAgJy3l8Qu3fvvpavUuwpL/aUF9eUG3vKiz3l5Q8+qalUfeMNbl+zBsPpzHPd6evLyX79+C0yEmfZsrBzJwA1asDMmQFMmlSLM2fuwOFIw+nMxOHwxen0x98/k8mTD1OjRgo7dnj4SxUC/ZkpfGpaioiIyLX5+WeYOBGWLrWWDtlp1Aheegk6d7aWhYt4mQ8//JANGzYwa9Ys6taty759+3jppZeoXLkyEUW04Vbjxo3x8fEpks/2RtnZ2ezevVt5uYzy4ppyY095sae8XMLpxFi8GGPsWIwLD+YuZz74IOacOVSuW5fKNtebNYMnnoA1a5zExpbj9GmToCCD8HAnPXo4KFPm1kL9Cp6gPzP2LubFndS0FBERkYJJSLCWeL/+Opw/bz+mVi2YMgX69AEVc+LFZsyYweDBg+ncuTMA9evX58SJE7z55ptEREQQcmHP1cTERCpX/uOvZ4mJiTRo0ACASpUqcfr06Vz3zcrKIiUlJef9BeHj46O/BNlQXuwpL64pN/aUF3slPi9ffw1PPWX9bqd2bYiJwXjoIa6WpXLlrK3N+/WDPw7kKX4Pr0v8nxkP0EE8IiIikj9pafDCC1bROneufcMyJARiYmD/fnj0UTUsxeulp6djXDYL2MfHB/PC7OHq1asTEhLC1q1bc66npaWxc+dOQkNDAWtfzNTUVPbs2ZMzZtu2bTidTpo0aeKBbyEiInKN4uNh0CC46y7bhqVZpoxV/+3dCw89VAQBSkmmmZYiIiJyZefPwz//ac2cPHXKfoy/P4waBc88A+XLezY+ketw7733smDBAqpWrZqzPHzx4sX06NEDAMMw6NevH2+88Qa1atWievXqxMTEULlyZTp06ABAnTp1aN26NePHj2fy5MlkZmYyZcoUOnfuTJUqVYry64mIiNjLzoYFC2DcOEhOth2SdO+9VFi4EJ86dTwbm8gFalqKiIiIPacT3nsPxo+Hn36yH+PrC0OGwNixUNluZyMR7zZu3DhiYmKYPHlyzhLw3r1789RTT+WMGTRoEOfOnWPChAmkpqbSokULFi1ahJ+fX86YmTNnMmXKFPr374/D4aBjx46MGzeuKL6SiIjIlX35JQwdisvTcOrXJ3vOHH6sXJlmt9ziychEclHTUkRERHIzTfjwQ3j+edi1y36MYUDfvtZyIRWzcgPz9/dn7NixjB071uUYwzCIiooiKirK5ZjAwEBmzZpVGCGKiIi4x6+/wnPPwbJl9tfLlYMJE+Dpp60tforDEd9yQ1PTUkRERP6wdSuMGQP/+5/rMQ89ZB3Eo736RERERLxfZia89hpMmgRnztiPefhheOUVqFbN+jk722PhibiipqWIiIjA999DdDSsX+96TKtWMH06tG7tubhERERE5Np99hkMG2bVenYaNYJ586BtW8/GJZIPOj1cRESkJPvlF3jsMWjc2HXD8o47IC4ONm9Ww1JERETkRnD0KPTuDe3b2zcsK1SAuXPhu+/UsBSvpZmWIiIiJVFiIkybBvPnQ0aG/ZiaNa09K/v2tfY1EhERERHvlpEBs2fD1Knw++/2YyIjrdUzVap4NDSRglLTUkREpCQ5e9Z6qj5jBqSm2o8JDoZx46xTwS85HVlEREREvNhHH8Hw4XDwoP315s2tpeD33OPZuESuUYGblr/++iuGYXDzzTcDsGvXLjZs2EDdunXp3bu32wMUERERN8jMhIULrZmTJ0/ajylXDkaOtH5VqODZ+EQuoXpTRESkAH76CUaMcLnVT0a5IBbfNo2V/gOpOLMU4eHQqxeUKePZMEUKqsB7Wo4cOZJt27YBEB8fz4ABA9i9ezdz5sxh3rx5bg9QREREroPTCe+9B7ffDk89Zd+w9PWFoUPh8GGYPFkNSylyqjdFRETy4dw560Twhg1tG5amYbC49GCqnj3AU7seYdP/UomNTaNfP6ha1WTDBs+HLFIQBW5aHjx4kCZNmgDw4Ycfctttt7FixQpmzpzJunXr3B6giIiIXAPThI8/hpYtoU8fqyF5OcOARx6BH36A117TvkbiNVRvioiIXIFpWk3Khg2tB87p6XmGnK53F3eaXzEw82VO8xtO5wHgyIXf95KcnEK3biZxcR6PXiTfCty0zMrKonTp0gBs2bKF++67D4DatWsTHx/v3uhERESk4P7f/4P77oMHHoDt2+3HPPigdW35cqhd27PxiVyF6k0REREXDh6ETp0gPByOHMl7PSSEzH8u5rZTX/KdURfT/BG4/NDFjAuvpxAZadr1PEW8QoGblnXr1mXFihV88803bNmyhTZt2gBw6tQpAgMD3R2fiIiI5NcPP0D37nD33fDf/9qPuXjtgw+gaVNPRieSb6o3RURELnP2LERHQ6NG1oE7l3M4rEN4DhxgRZlITif7YJrHr3hL0zxOUpLB6tWFFLPIdSpw03LUqFH8+9//5tFHH6Vz5840aNAAgM8++yxnGY+IiIh40LFj8PjjcMcd4Grp7O23W9e2bIG2bT0bn0gBqd4UERG5wDRh5Upo0ABeegnOn887pk0bawVNTAwEBhIbCw5HGnlnWF4uA4cjzWX5KFLUCnx6+F133cW2bdtIS0sjICAg5/W//e1vlC1b1q3BiYiIyBWcPm0Vr6+9BhkuitIaNay9jvr1Ax8fz8Ynco1Ub4qIiAB798KwYbBpk/31qlVh5kz4+9+tvcovSEw0cTptmps2nM5MTp82AeOqY0U8rcBNSwDTNNm7dy+//PILDz30EP7+/vj6+lKmTBl3xyciIiKXO3vWepI+YwakpNiPCQqylhA99RTov89yA1K9KSIiJUF6OqxaBbGxVrMxONig119T6bl3MqVefxWysvK+qVQpGDECxo+H8uXzXA4ONnA4SuN0Xv3zHQ5fgoLUsBTvVOCm5fHjx3n88cf59ddfOX/+PH/5y1/w9/dn4cKFnD9/nhdeeKEw4hQREZHMTHjrLWvm5G+/2Y+56SariH32WbhkhprIjUT1poiIlARxcRAZaZKUZOBwpOF0ZtDPWEu7teMpxUn7N3XoYK2yubB1ip3wcFi71h/w48pLxP1wOv2JiLiOLyFSiAq8p+WLL75Io0aN+Oqrr/Dz88t5/f7772fbtm1uDU5EREQApxP+/W9o2BCGDLFvWJYqBU8+CYcPw9SpaljKDU31poiIFHdxcRAebpKcnALspbFzA1/QniXmYG62a1jWrAlr1sAnn1yxYQnQqxdUrGhiGNWuOM4wqlGxoknPntfxRUQKUYGblt9++y1DhgyhdOnSuV6vVq0aJ0+6eBIgIiIiBWeaVmF6553WXkWHDtmPe/hh6+Tw+fPh5ps9G6NIIVC9KSIixVl6ujXDElIIMHfwGi/wLX0JY2eesRmUJmvMONi3D7p3z7V3pStlysCSJQYQgGHUxppxeSm/C68HsGSJoZ2ExGsVuGnpdDpx2myM8Ntvv1GuXDm3BCUiIlLS3bR3L46OHeGvf4XvvrMf9MAD1rV334U6dTwboEghUr0pIiLF2apVkJxk8pgZwwEiGMpKfMj7370NtOUO9rLijinWFkAF0KULxMYaBAYGAHfgcNQDbr3w+x0EBgawfr1Bly5u+UoihaLATcu//OUvLFmyJNdrZ8+e5bXXXqNt27ZuC0xERKRE2r8fR+/e3N6/P4arkyL//GfrFMkPP4TQUM/GJ+IBqjdFRMTbpKfDsmXQowe0a2fSo4f1c3p6we+15+2v2MafWcQkQkjOc/0Q1XmIOXRlNj85bmbdumuLuWtXOHHCYNkyCA/3p127QMLD/Vm2zHpdDUvxdgU+iGfMmDEMHDiQTp06cf78eUaNGsWRI0eoWLEis2fPLowYRUREir/jx60Ddt5+GyM7235MgwYwbZq1u3o+lgaJ3KhUb4qIiDfJe2DOeRyO0qxd609UlMmSJflsAMbHQ3Q0L/33LRyYeS7/jh/TeIyZPErGhSXdTmcmp0+bwLXVfmXKQN++1q9rvYdIUSlw0/Lmm29m/fr1vP/+++zfv5/ff/+dnj170qVLF8poIwQREZGCSUqC6dPh1VddP6qvVs1qaPbvbx24cwXp6daSo9hYSEw0CQ42CA+3NmTXf6blRqF6U0REvMXFA3MgBTiO02mdxm3tYuJHcnI1unULIDbWoGtXFzfJyoI334Rx4yA52XbJ62raM5IR/MKfcr3ucPgSFKRmo5RMBW5aApQqVYpu3bq5OxYREZGS4/ffrUblyy9DcrLtELNiRYzoaHjqKShb9qq3dNssABEvoHpTRESK2qUH5pjmjzYjMjDNHzGM2kRGBnDihM2hNl9+adVyO/MesgPwA7UYxmg2crfNVT+cTn8iIq7zi4jcoArctIyNjb3i9fDw8GsMRUREpATIzITFi62ZkydO2A4xy5blt7//ncqvvIJPcHC+buuWWQAiXkL1poiIeINVqyApyQCOX3GcaR4nKSmQ1asvLsMGfv0VnnvO2vjSxhn8mcKzzOWvZOJrO8YwqhEYaNKzp2ZaSslU4Kbliy++mOvnrKwszp07h6+vL2XLllURKSIiYsc0YfVqa1nQgQP2Y3x8YNAgnGPHcuLkSSoHBubr1m6ZBSDiRVRvioiIN4iN5cLqlYyrjMzA4Uhj3Tp/+vbOhNdeg0mT4MwZ++F9+rDt/hnMfKwqkALmceDSz/DDMKoBASxZorpNSq4CNy2//vrrPK8dOXKESZMmMXDgQLcEJSIiUqxs3AhjxsC337oe07s3TJkCt90G2dlw8mS+b39dswBEvJDqTRER8QaJiSZO5/l8jXU6M6l1+FNoOgz27bMf1Lix1dBs25b7gdggiIwMICkp8EJzNBOHwxen05/AQG3tI2K3/2uB3XLLLYwcOTLPU3EREZES7dtv4f77rV+uGpYdO8I338CKFVbD8hpcnAWQ+wm9nYuzAK7pY0SKlOpNERHxtOBgA4ej9FXHVec3/s3jzN7Zwb5hWaECxMTAd99B27Y5L3ftCidOGCxbBuHh/rRrF0h4uD/Lllmvq2EpJd01HcRje6NSpTh16pS7biciInLjOnDAWga+apXrMXfeaZ0aft991/1xBZ0FcPq0CWhvJLnxqN4UERFPCg+HtWv9AT/sHg6X5jzPsJxxvE05ztnfZMAAeOklqFLF9nKZMtYKGGsVjOozkUsVuGn56aef5vrZNE3i4+N55513aN68udsCExERueGcOAEvvACLFllLvO3UqwfTpkH37mC4pzC9OAvAOnTnyhwOX4KCVBCLd1O9KSIi3qBXL4iKMklOrpZn3/AH+JJXeYXbOGr/5ubNYf58uNvuVHARyY8CNy2feuqpXD8bhkFQUBB33303zz33nNsCExERuWEkJ8PLL1vLfs65eMpetaq1IfuAAVDKbQsdgKvPAviDH06nPxERbv14EbdTvSkiIt6gTBlYssSgW7cADKM2pnmcW/iROcwinM/t3xQUZD2gfvxx65BFEblmBf5b0w8//FAYcYiIiNx4zp2zNlOfPh2SkuzHBAbC88/DsGFQtmyhhHGlWQCXMoxqBAaa9OypmZbi3VRvioiIt+jSBWJjDf7RvzSDk1fxHC9TlvS8Aw0DBg+GF1+E4GDPBypSDLl3qoeIiEhJkJUF//qXNXPyuIsTu8uWhagoGD0aKlYs1HDsZgHknnHph2FUAwJYssSgTJlCDUdERESk+DBNujrX81DACBzJR+zH3H03zJsHLVp4NDSR4i5fTcuXXnop3zd8/vnnrzkYERERr2aasHYtjB0L+/fbj/HxgYEDYeJEa0m4h1ycBRAZGUBSUiAORxpOZyYOhy9Opz+BgSZLlugUSvFeqjdFRMTrHDhgPYT+6CMcdtdDQmDGDOjXDxy2I0TkOuSrafn999/n62bGNRwocPLkSV555RW++OILzp07R61atZg2bRqNGzcGrI3XX331VVatWkVqairNmzdn0qRJ3HLLLTn3SE5OZsqUKWzatAmHw0HHjh0ZO3Ys5cqVK3A8IiIitjZtgjFj4KuvXI/p1QumTrUO2ykCXbvCiRMGq1fDunX+nD5tEhRkEBEBPXtqhqV4t8KsN0VERAokLc1a5j1rFmRm5r3u4wNPPmkdwBgY6PHwREqKfDUtly1bVigfnpKSwsMPP8xdd93FwoULqVixIj///DMBAQE5YxYuXMiyZcuYPn061atXJyYmhoEDB/LBBx/g5+cHwKhRo4iPj2fx4sVkZmYSHR3NhAkTmDVrVqHELSIiJcj27Vaz8pNPXI9p397a17JlS8/F5UKZMtC3r/UL1NyRG0dh1ZsiIiL5ZpqwahWMHAnHjtmPadPG2tO8SRPPxiZSAhXpnpYLFy7k5ptvzrUcqEaNGjn/bJomS5cuZciQIXTo0AGAGTNm0KpVKzZu3Ejnzp05fPgwX3zxBatXr86ZnTlu3DgGDx7M6NGjqVKlime/lIiIFA+HDsH48bBihesxLVpYzcoL/40SERERkRvU3r3w9NPW6ho7VavCzJnw979bh+6ISKG7pqbl7t27+fDDD/n111/JvGyq9Lx58/J9n88++4ywsDCGDx/O119/TZUqVejTpw9/+9vfADh27Bjx8fG0atUq5z3ly5enadOmbN++nc6dO7N9+3YqVKiQ07AEaNWqFQ6Hg127dnH//fcX6LtlZ2cXaHxxdzEfyktuyos95cWe8uKaV+bmt98wpk7FWLQIIyvLdoh5222YL7yA2aOHtX+Rm+P3yrx4AeXFXnHNh7vqTRERkStKTaX67Nk4/v1v+5quVCkYMcJ6mF2+vOfjEynBCty0fP/993nuuecICwtj8+bNhIWF8dNPP5GYmFjgBuHRo0d57733GDBgAP/4xz/YvXs3U6dOxdfXl4iICOLj4wEIDg7O9b7g4GASEhIASEhIICgoKPeXKlWKgICAnPcXxO7duwv8npJAebGnvNhTXuwpL655Q24caWncvHQpld99F0d6uu2Y85Uq8eugQSR062YVsLt2FWpM3pAXb6S8FH/urDdFRERsmSYsX47j2WepcvKk/ZgOHayl4A0aeDY2EQGuoWm5YMECnn/+eR555BFCQ0MZO3Ys1atXZ8KECYSEhBToXqZp0qhRI5555hkAGjZsyMGDB1mxYgUREREFDc0tGjdujI+PT5F8tjfKzs5m9+7dystllBd7yos95cU1r8hNejrG669jTJ+Ocfq07RAzIABz9Gh8hg2j+k03Ub2QQ/KKvHgh5cXexbwUJ+6sN0VERPLYsQOGDoUvv7TfAbxmTZgzByIitBRcpAgVuGl59OhR2rZtC0Dp0qX5/fffMQyDyMhI+vfvz/Dhw/N9r5CQEOrUqZPrtdq1a/Pxxx/nXAdITEykcuXKOWMSExNpcOFJR6VKlTh92V8ys7KySElJuaai1sfHR38RsqG82FNe7Ckv9pQX14okN1lZsHQpTJoER4/ajylTBoYNwxgzBuOyWf2eoD8z9pSX4s+d9aaIiEiO06etZd4LFoDTmfd66dIwejQ8/zzcdJPn4xORXBwFfUOFChU4e/YsAJUrV+bgwYMApKamcu7cuQLdq3nz5vz000+5Xjty5AjVqlUDoHr16oSEhLB169ac62lpaezcuZPQ0FAAQkNDSU1NZc+ePTljtm3bhtPppIlO8xIRkcuZJqxbZ534OHCgfcPS4YDHH4eDB2HGDCiChqVISebOelNERASnExYtgvr14fXXbRuWZqdO1mE8U6aoYSniJQo80/LOO+9ky5Yt1K9fnwceeIAXX3yRbdu2sWXLFu65554C3at///48/PDDLFiwgAcffJBdu3axcuVKXnjhBQAMw6Bfv3688cYb1KpVi+rVqxMTE0PlypVzThOvU6cOrVu3Zvz48UyePJnMzEymTJlC586ddXK4iIjk9vnnMGYMbNvmekz37vDii27Zuyg9HVatgthYSEw0CQ42CA+HXr2sSZwiYs+d9aaIiJRwX31lLQX/+mvby2bt2hweNoxbhw3TSg4RL5PvpuWBAweoV68e48ePJyMjA4AhQ4bg6+vLd999R8eOHRkyZEiBPrxJkybMmzeP2bNnM3/+fKpXr050dDRdu3bNGTNo0CDOnTvHhAkTSE1NpUWLFixatAg/P7+cMTNnzmTKlCn0798fh8NBx44dGTduXIFiERGRYmzHDmuZz0cfuRyyO+Rejj45nfvG/NktDcW4OIiMNElKMnA40nA6z+NwlGbtWn+iokyWLDHo0uX6P0ekOCmMelNEREqo+Hir/nvrLfvrZctCdDTOESNI+eEHz8YmIvmS76Zl165dady4Mb169aJTp04AOBwOBg8efF0B3Hvvvdx7770urxuGQVRUFFFRUS7HBAYGMmvWrOuKQ0REiqHDh619i957z+WQ72jK84xjY8JfcU4uT8VXr7+hGBcH4eEmkAIcx+m0mi/WSiQ/kpOr0a1bALGxBpc8pxMp8Qqr3hQRkRIkKwvefBPGjYPkZPsxPXrArFlQqxZkZ3s0PBHJv3zvabl8+XLq1q3L9OnTad26Nc899xzffPNNYcYmIiJybU6etJYBNWjgsmF5iFv5OzNoySI+oTZO8yCwl+TkFLp1M4mLu7aPTk+3ZlhCCqb5I5Bx2YiMC6+nEBlpkp5+bZ8jUhwVRb153333Ub9+/Ty/Jk+eDEBGRgaTJ0/mrrvuIjQ0lGHDhpGQkJDrHidOnGDw4ME0bdqUe+65h5dffpmsrKxCjVtERGxs3gwtW1p1oF3Dsn59+OQTWL3aaliKiFfLd9OyZcuWvPTSS2zevJlx48Zx/Phx+vbty1//+lf++c9/Eh8fX5hxioiIXF1qqjWzsk4dmD/fetJ+mZNGFZ5kJrfzHv+mPWau/xRef0Nx1SpISjIwzeNXHGeax0lKMli9uuCfIVJcFUW9uXr1ajZv3pzza/HixQA88MADAEybNo1NmzYxd+5cli1bxqlTpxg6dGjO+7Ozs3niiSfIzMxkxYoVTJ8+nXXr1vHqq6+6PVYREXHh11/h0UehdWvYuTPv9XLl4OWXYdcuuP9+z8cnItekwKeH33TTTfTo0YPly5fz8ccf88ADD/Duu+9y77338o9//KMwYhQREbmy9HSYMwdq14apU+HCqcO5VKjA9p4vUts8zBs8QBa+Lm93PQ3F2FhwONLIO8Pychk4HGmsW1fwzxAp7jxZbwYFBRESEpLza9OmTdSsWZM///nPnDlzhjVr1jBmzBjuueceGjVqxLRp09i+fTs7duwAYPPmzRw6dIhXXnmF22+/nbZt2xIVFcU777zD+fPn3RqriIhcJjMTZs+2ZlAuX24/pk8f2L8fRo+G0qU9G5+IXJcCnx5+qVq1avHEE09QtWpVZs+ezeeff+6uuERERK4uOxuWLYOJE+GXX+zH+PlZS4Sef56pg4NJd6SBM78NRX/69i1YSImJJk5n/hoVTmcmp0+bgFGwDxEpQTxZb54/f564uDgGDBiAYRjs2bOHzMxMWrVqlTOmTp06VK1alR07dtCsWTN27NhBvXr1qFSpUs6YsLAwJk2axKFDh2jYsGGB48jW/mq5XMyH8pKb8uKacmOv2OXls89wREVh7Ntne9nZsBGfdH2Vf+5vx+lHTIKCTLp1M+nZ08x16GKxy4ubKC+uKTf2CiMf19y0/Prrr1mzZg0ff/wxDoeDBx98kJ49e7ozNhEREXumaZ12Ex0N339vP8bhgMhImDQJatQAPNNQDA42cDhKXzh058ocDl+CgtSwFHHF0/Xmxo0bOXPmDBEREQAkJCTg6+tLhQoVco0LDg7OWaqekJCQq2EJ5Px8rcvZd+/efU3vK+6UF3vKi2vKjb0bPS++v/1G9ZgYgv7v/2yvZ5crx//uH0H3jdEkTy+Lw3EGpzMTh8OXdevKM2xYJpMn/0ybNim53nej56WwKC+uKTeFr0BNy5MnT7Ju3TrWrVvHzz//TGhoKOPGjePBBx/kpptuKqwYRURE/vC//8GYMbB1q+sxERHWMvHLZjh5oqEYHg5r1/oDflx5ibgfTqc/F3ojInJBUdaba9asoU2bNlSpUqVQP+dqGjdujI+PT5HG4E2ys7PZvXu38nIZ5cU15cbeDZ+XjAyMOXMwpk3D+P132yHO/v35pN00Oj/2JyAF+BHnhRU2Vv3nR1paNUaOrMPatU66dCkGeSkkyotryo29i3lxp3w3LR9//HG2bt1KxYoV6datGz169KB27dpuDUZERMSlXbvg+efhgw9cj2nbFqZPh7vvtr3siYZir14QFWWSnFztwqE+9gyjGoGBJj17aqalyEVFWW8eP36cLVu28Nprr+W8VqlSJTIzM0lNTc012zIxMZGQkJCcMbt27cp1r4uni18cU1A+Pj76S5AN5cWe8uKacmPvhszLhx/C8OFw6JD99ebNYd48zofewyNVTSDFRR1mHbpoGLV57LEATpww8L2wzfkNmRcPUF5cU24KX74P4ilVqhQxMTF8/vnnPPvss2pYioiIZ/z0E/TtC82auW5YNm1qFbObNrlsWILVUKxY0cQwql3xIw2jGhUrmlzLKtQyZWDJEgMIwDBqYzVIL+V34fUAliwxcu2pJFLSFWW9uXbtWoKDg2nXrl3Oa40aNcLX15etl8zs/vHHHzlx4gTNmjUDoFmzZhw4cIDExMScMVu2bMHf35+6det6KnwRkeLpp5+sp86dOtk3LIOCYMEC+OoruOceVq2CpCQD0zx+xdtez6GLIuI5+Z5puWDBgsKMQ0REJLeTJ+HFF61CNDPTfszF08J797b2sLyKiw3Fbt2shqJV0F4649LvQkPz+hqKXbpAbKxBZGQASUmBOBxpOXspOZ3+BAaaLFli0KXLtd1fpLgqqnrT6XSydu1awsPDKVXqj/K4fPny9OjRg+nTpxMQEIC/vz9Tp04lNDQ0p2kZFhZG3bp1GT16NM8++yzx8fHMnTuXRx55hNI6pVZE5NqcOwcvv2ytoMmwWR1jGDB4sFUrBgfnvBwby4W6K/+HLj78sFsjFxE3uq7Tw0VERNwuNRVmzbJ+nT1rP6ZKFZgwAR5/HArYFPBUQ7FrVzhxwnqCv26dP6dPmwQFGUREQM+emmEp4k22bNnCiRMn6NGjR55r0dHROBwOhg8fzvnz5wkLC2PixIk51318fFiwYAGTJk2id+/elC1bloiICIYPH+7JryAiUjyYJqxfDyNGwJEj9mPuvhvmzYMWLfJcurZDF0XEW6lpKSIi3iEjA954w3pifmE/uDzKl4fRo+Hpp8Hf/5o/ylMNxTJlrJXtfftCQU8hFxHPCQsLY//+/bbX/Pz8mDhxYq5G5eWqVavGwoULCys8EZGS4cABiIqCjz6yvx4SAjNmQL9+LlfYeOLQRRHxHDUtRUSkaGVnYyxbBpMmwc8/248pXRqGDrUO4qlUyS0fq4aiiIiIiBdIS7MeWs+aZb8lkI+PVQdOmgSBgVe8lScOXRQRz1HTUkREioZpwoYNNBw1Csfhw/ZjHA7rafrkyVCzpmfjExEREZHCY5qwahWMHAnHjtmPadMGXnsNmjTJ1y179YKoKJPk5GouTg+3GEY1AgNNevbUg2sRb5avpuWnn36a7xu2b9/+moMREZESYvNmGDMGny+/pKyrMd26WU/d77jDk5GJSBFRvSkiUoLs3QvDhsGmTfbXq1aFmTPh73+3Dt3Jp2s5dDE7+3q+iIgUpnw1LZ966qlcPxuGgWmauX6+aN++fW4KTUREip3duyE6Gv7zH9djWre2Tops1cpzcYlIkVO9KSJSAqSkWCtoXn3VvltYqpR1CM/48dZe5tfAU4cuikjhy1fT8ocffsj55y1btjBz5kxGjBhBaGgoANu3b2fu3Lk888wzhROliIjc2I4csU77Xr7cWgpkp0kTeOklePDBAj1RF5HiQfWmiEgxZpqwbJl1oOLJk/ZjOnSwloI3aHDdH+epQxdFpHAVeE/LadOmMWnSJFq2bJnzWuvWrSlbtizjx4/nww8/dGuAIiJyA4uPh6lTrVPB7TZWBzKqVsX3pZdw9O3r8iRIESlZVG+KiBQjO3ZYB+l8+aX99Zo1Yc4ciIhw64NrHboocuMrcNPyl19+oUKFCnle9/f35/jx424JSkREbnBnzsDs2dZeRGlp9mNCQnCOHcveu+6i6Z13qmEpIjlUb4qIFAOnT1vLvBcsAKcz7/XSpa2Zl88/Dzfd5Pn4RMTrFfhviI0bN2b69OkkJCTkvJaQkMArr7xCk3ye6CUiIsVURoa1R1GdOjBpkn3D0t/f2svo8GHMoUMxfX09HqaIeDfVmyIiNzCnExYtgvr14fXX7RuWnTtbh/FMmaKGpYi4dE3Lw4cOHUq7du3405/+BMCvv/7KLbfcwvz5890eoIiI3ACys+G996yn6UeO2I8pXRqGDIGxYyEk5I/3iYhcRvWmiMgN6quvrKXgX39tf712bYiJgYce8mxcInJDKnDTslatWsTFxfHll1/y448/AlCnTh1atWqV61RHEREpAUwTPvjAWtaze7f9GMOARx+1ZlfecotHwxORG5PqTRGRG0x8vFUPvvWW/fWyZSE6GkaNQqfgiEh+FbhpCWAYBmFhYdx5552ULl1axaOISEm0ZQuMGQNffOF6TJcu8OKL0Lix5+ISkWJB9aaIyA0gKwvefBPGjYPkZPsx3btbe53XquXR0ETkxlfgPS2dTifz58+ndevWhIaGcuzYMQDmzp3LqlWr3B6giIh4mb17oVs3+MtfXDcsL16Li1PDUkQKTPWmiMgNYPNmaNnSWg5u17CsXx8+/hjWrFHDUkSuSYGblq+//jrr1q3j2WefxfeSwxPq1avH6tWr3RqciIh4kZ9/hshIqwkZF2c/plEj2LDBaliGhXk0PBEpPlRvioh4sV9/tbb+ad0adu7Me71cOZg+HXbtgo4dPR+fiBQbBW5arl+/nilTptC1a1ccjj/eXr9+/Zw9h0REpBhJSIARI6BePViyxNrH8nK1alnXduywNlbXMk4RuQ6qN0VEvFBmprXMu359WL7cfszDD8P+/fDcc9YhjNcgPR2WLYMePaBdO5MePayf09OvI3YRuSEVeE/LkydPUrNmzTyvm6ZJVlaWW4ISEREvkJYGc+bAK6/AmTP2YypVsvYw+sc/wM/Ps/GJSLGlelNExMt8+ikMGwb79tlfb9QI5s2Dtm2v62Pi4iAy0iQpycDhSMPpPI/DUZq1a/2JijJZssSgS5fr+ggRuYEUeKZl3bp1+eabb/K8/tFHH3H77be7JSgRESlC58/D/PlQty5MmGDfsPT3h4kT4ccfISpKDUsRcSvVmyIiXuLoUfjb36BDB/uGZYUKEBMD27e7pWEZHm6SnJwC7MXpPAAcufD7XpKTU+jWzXS5S5GIFD8Fnmn55JNPMmbMGE6ePIlpmnzyySf89NNPxMbG8uabbxZGjCIi4glOJ6xYAePHW81IO76+MGQIjB0LlSt7Nj4RKTFUb4qIFLGMDGsp+NSp8Pvv9mMiI629K6tUue6PS0+3ZlhCCqZpV4dmYJo/Yhi1iYwM4MQJgzJlrvtjRcTLFXimZYcOHViwYAFbt26lbNmyvPrqqxw+fJgFCxbwl7/8pTBiFBGRwmSa8OGH0Lw5PPKIfcPSMKBvX2uPopgYNSxFpFCp3hQRKUIffmgt946Otm9YNm8OW7bA4sVuaVgCrFoFSUkGpnn8iuNM8zhJSQY6k02kZCjQTMusrCwWLFhAz549Wbx4cWHFJCIinrJtG4wZA59/7npM584wbRo0aeK5uESkxFK9KSJSRH76yTp8cf16++tBQVZN+Pjj4OPj1o+OjeXCHpYZVxmZgcORxrp1/vTt69YQRMQLFWimZalSpXjrrbe0AbqIyI1u3z6IiIB77nHdsLznHvjf/+A//1HDUkQ8RvWmiIiHnTsHkybB7bfbNywNA554Ag4csH53c8MSIDHRxOk8n6+xTmcmp0+bbo9BRLxPgZeH33333Xz99deFEYuIiBS2o0fhscesJT+xsfZj7rjDKli//BJat/ZoeCIioHpTRMQjTNOqBxs2hMmTrX0sL3f33fD117BgAQQHF1oowcEGDkfpfI11OHwJCjIKLRYR8R4FPoinTZs2zJo1iwMHDnDHHXdQtmzZXNfbt2/vtuBERMRNEhPhpZdg3jz7ghSgRg144QV49NFCeYIuIpJfqjdFRArZgQMQFQUffWR/PSQEZsyAfv3AUeC5TgUWHg5r1/oDfsCVloj74XT6ExFR6CGJiBcocNNy8uTJALZ7DBmGwb59+64/KhERcY+zZ2HuXKvoTE21HxMcbJ0GPmQIOoZRRLyB6k0RkcLh+P13jOhomDMHMjPzDvDxgSeftB5kBwZ6LK5evSAqyiQ5uZqL08MthlGNwECTnj0101KkJChw0/KHH34ojDhERMSdMjNh0SKr4PztN/sx5crBM8/AqFFQoYJn4xMRuQLVmyIibmaaGCtXcsfTT+M4dcp+TJs28NprRbKXeZkysGSJQbduARhG7QuniF8649IPw6gGBLBkiaHn7CIlxHXN885wtcRQRESKhtMJK1ZYG6k/+aR9w7JUKXjqKTh82GpqqmEpIl5M9aaIyHXauxfat8fRpw+l7RqWVavCu+/Cf/9bpIcvdukCsbEGgYEBwB04HPWAWy/8fgeBgQGsX2/QpUuRhSgiHlbgpmV2djbz58+ndevWhIaGcvToUQDmzp3LqlWr3B6giIjkg2nCJ59Ay5bw8MNWQ9JOnz6wf7+1t2WVKp6NUUQkn1Rvioi4QUqKtaqmaVPYtCnv9VKlYPRo+OEHq340in7JddeucOKEwbJlEB7uT7t2gYSH+7NsmfW6GpYiJUuBm5ZvvPEG69at49lnn8XX1zfn9Xr16rF69Wq3BiciIvnw1VfQvj389a+wfbv9mAcftK698w7Uru3Z+NwgPR2WLYMePaBdO5MePayf09OLOjIRKQyqN0VEroNpwtKlUL++tXdldnbeMR06wO7d8PLLUL68Wz7WXfVamTLQty+sWQObNhmsWWP9rCXhIiVPgZuW69evZ8qUKXTt2hXHJaeI1a9fnx9/dL1hroiIuNkPP0DPnnDXXfZPz8G69t//wgcfQLNmnozObeLioGpVk379IDY2jc8/TyI2No1+/azXN2wo6ghFxN1Ub4qIXKMdO6B1a+jfH06ezHM54+abyV61ylqh06CB2z5W9ZqIFIYCH8Rz8uRJatasmed10zTJyspyS1AiInIFx47B5Mnw9tvWHpZ2GjSAadMgPNwrlvpcq7g4CA83gRTgOE6ntbed9bX9SE6uRrduAcTGGnTtWoSBiohbqd4UESmg06dh/HhYsMC+PixdGueoUezt1Immd9/t1vpQ9ZqIFJYCz7SsW7cu33zzTZ7XP/roI26//Xa3BCUiIjZOn7b2HbrtNutkcLuCtHp1eOsta7lPRMQN3bBMT4fISKsANs0fyX2CJEDGhddTiIw0tVRcpBhRvSkikk9Op1UX1q8Pr79uXx8+9BDs3Yv5wguYNmusr2dZt+o1ESlMBZ5p+eSTTzJmzBhOnjyJaZp88skn/PTTT8TGxvLmm28WRowiIiXb779DTIy151BKiv2YoCCIjrZOBS8mG/6sWgVJSQZw/IrjTPM4SUmBrF5t7XckIjc+1ZsiIvnw1VcwdCh8/bX99dq1rRryoYesn232toyLs5qOSUkGDkcaTud5HI7SrF3rT1SUyZIlVz78RvWaiBSmAs+07NChAwsWLGDr1q2ULVuWV199lcOHD7NgwQL+8pe/FEaMIiIlU2YmvPkm1K1rNSTtGpY33QRjx8KPP8LIkcWmYQkQGwsORxp5n9hfLgOHI4116zwQlIh4hOpNEZEriI+Hxx+39i63a1iWLQtTpsDevX80LG1cXNadnJwC7MXpPAAcufD7XpKTU+jWzSQuznUoqtdEpDAVeKYlQMuWLVm8eLG7YxEREbCW9axeDePGwcGD9mNKlYJBg6y9i/70J8/Glw/p6daT99hYSEw0CQ42CA+HXr3y31dNTDRxOs/na6zTmcnp0yZw4y6HF5HcVG+KiFwmK8t6oD1uHCQn24/p3h1mz4Zata54q7zLui9nLes2jNpERgZw4oRhW8OpXhORwlTgmZYiIlKINm6EP/8Zevd23bD8+99h3z5r3yIvbFi66/TI4GADh6N0vsY6HL4EBakAFhERkWJq82Zo2dJaDm7XsKxf3zoRfM2aqzYs4Y9l3aaZn2XdBqtX219XvSYihSlfMy3vvPNOjHwe5vDVV19dV0AiIiXSN9/AmDHw6aeux/z1r9aJ4M2bey6uAnLn6ZHh4bB2rT/gx5WXHPnhdPoTEeGGLyAiRUb1poiIjV9/tQ5iXL7c/nq5cjBxIkRFQen8NQ/hj2XdF2s11y4u6/a33YtS9ZqIFKZ8NS2jo6Nz/jk5OZk33niDsLAwmjVrBsCOHTvYvHkzTz75ZIE+/LXXXmPevHm5Xrv11lv56KOPAMjIyGD69Ol88MEHnD9/nrCwMCZOnEilSpVyxp84cYJJkybx//7f/+Omm24iPDyckSNHUqrUNa18FxHxrAMHrCU+q1a5HvPnP8P06XDvvYUayvUu6b6WZUa+vq7v16sXREWZJCdXc3E/i2FUIzDQpGdPPbkXuZEVVr0pInJDysyE116DSZPgzBn7MX36wIwZUK1agW/vrmXdqtdEpDDlq7MXccnjkGHDhjF8+HD6XvKYpV+/fixfvpwtW7YQGRlZoABuu+22XPsV+fj45PzztGnT+Pzzz5k7dy7ly5dnypQpDB06lBUrVgCQnZ3NE088QaVKlVixYgWnTp3iueeew9fXl2eeeaZAcYiIeNTx4/DCC/DWW7YnOQLWMp9p0yAiAvI5++haXe/JkXBtp0c+/LDrcWXKwJIlBt26BWAYtS8sX7r0Cb4fhlENCGDJEvt9lkTkxlGY9aaIyA3l009h2DBrOyA7jRrBvHnQtu01f8TFZd3Wapgru9KybtVrIlKYCjwdcfPmzYwaNSrP661bt2bWrFkFDsDHx4eQkJA8r585c4Y1a9Ywc+ZM7rnnHsBqYnbq1IkdO3bQrFkzNm/ezKFDh1i8eDGVKlXi9ttvJyoqipkzZzJ06FBKF2B6/EXZrpoHJdTFfCgvuSkv9pQXe7nykpSEMWMGxmuvYaSn2443q1XDnDABs39/68Cd/FST12HDBuje3cHVlnSvXeu8YuNy3ToHDsfZfC8zWru2HH/725X/zHTqBGvXwoABASQnB15oqGbicPjidPoTEGCyeLGTTp1c935vRPr/kj3lxV5xzIe7600RkRvC0aMwcqTrFTgVKlingj/5pFUjXgd3Luvu0gViYw0iIwNISspbrwUG5u8BuIjI5Qr8b7rAwEA+/fRTHnvssVyvf/rppwQGBhY4gJ9//pmwsDD8/Pxo1qwZI0eOpGrVquzZs4fMzExatWqVM7ZOnTpUrVo1p2m5Y8cO6tWrl2u5eFhYGJMmTeLQoUM0bNiwwPHs3r27wO8pCZQXe8qLPeUlLyM9nVMjR3LzkiX4uFjik1WhAr/178+p3r0xy5SBPXsKPa6MDIN+/RoDZ666pLtfv3J89NFu/PxM23v98sttOJ1Z+fpcpzOTX345w+7d1mFDV/ozU6MGvP++waefVmTTpkBSU32oUCGTe+/9ifbtk/DzM9mxI18fe8PR/5fsKS/Fn7vrTRERr5aRYZ34PXUq/P67/ZgBA+Cll6BKFbd8pLuXdXftCidOWAf2rFvnz+nTJkFBBhER0LOnZliKyLUpcNNy2LBhjBs3jq+++oomTZoAsGvXLr744gumTJlSoHs1adKEl156iVtvvZX4+Hjmz5/PI488woYNG0hISMDX15cKFSrkek9wcDDx8fEAJCQk5GpYAjk/XxxTUI0bN861RL2ky87OZvfu3crLZZQXe8qLjawszLffxjlxIqVd/HvJLFsWc9gwjNGj+VNgIJ48D3z5coMzZxzkZ0n3mTN3cOhQMx55xL5pWbOmg+3bz+Z7mVHNmuVo3Lhxvv/M3HUXXLLlHVABqHn1D7sB6f9L9pQXexfzUpy4s94UEfFqH34Iw4fDoUP215s3h/nz4e673fqxhbGsu0wZ6NuXCwf2aO9KEbl+BW5adu/enTp16rB06VL+7//+D4DatWvz7rvv0rRp0wLdq+0le3A0aNCApk2bcu+99/Lhhx9Spogexfj4+OgvQjaUF3vKiz3lBTBNWLMGxo61Dtux4+MDjz+OMWECRtWqno3vgri4gp0cuX69P/362Y+IiLCerOd3mVH37n/sY6w/M/aUF3vKS/HnznpTRMQr/fQTjBgB69fbXw8KsvY2f/xxq2YsBFrWLSLerkBNy8zMTCZMmMCTTz5ZKPsJVahQgVtuuYVffvmFVq1akZmZSWpqaq7ZlomJiTl7YFaqVIldu3blukdCQgKA7T6ZIiIe8emnMGYMfPON6zF/+5u1J1G9ep6Ly4a7To4EnR4pIu5R2PWmiEiROncOXn4Zpk+3loVfzjBg8GB48UUIDi70cLSsW0S8maMgg319ffnkk08KKxbOnj3L0aNHCQkJoVGjRvj6+rJ169ac6z/++CMnTpygWbNmADRr1owDBw6QmJiYM2bLli34+/tTt27dQotTRMTWt99Cx47QoYPrhuX998PXX8O//13kDUv44+TI/LjSyZHwxzIjsJYZWTMuL+V34XWdHikirhV2vSkiUiRME2JjoWFDmDzZvmF5991WnbhggUcalhddXNa9Zg1s2mSwZo31s2o1ESlqBWpaAnTo0IFPP/3ULR/+8ssv89VXX3Hs2DG+++47hg4disPh4KGHHqJ8+fL06NGD6dOns23bNvbs2UN0dDShoaE5TcuwsDDq1q3L6NGj+eGHH/jiiy+YO3cujzzyyDWdHC4ick0OHoTevaFlS7iwjPFyZxs2JPvjj+GTT6xxXiI8HJzOi0u6r+TqJ0fCH8uMAgMDgDtwOOoBt174/Q4CAwNYv17LjETkytxZb+bHyZMnGTVqFHfddRdNmjShS5cuufYJNU2TmJgYwsLCaNKkCZGRkRw5ciTXPZKTkxk5ciTNmzenZcuWREdHc/bsWY99BxHxYgcOQKdO1l46l/27A4CQEHj7bfjyS2jRwuPhiYh4qwLvaVmrVi3mz5/Pd999xx133EHZsmVzXe/narMzG7/99hvPPPMMycnJBAUF0aJFC1auXElQUBAA0dHROBwOhg8fzvnz5wkLC2PixIk57/fx8WHBggVMmjSJ3r17U7ZsWSIiIhg+fHhBv5aISMGdOGEt8V64ELKz7cfcdhvZU6bwQ506NAsN9Wx8+VAYS7q1zEhErpc7682rSUlJ4eGHH+auu+5i4cKFVKxYkZ9//pmAgICcMQsXLmTZsmVMnz6d6tWrExMTw8CBA/nggw/w87Me+owaNYr4+HgWL15MZmYm0dHRTJgwQUvcRUqytDRrmfesWZCZmfe6jw8MHQqTJkFgoKejExHxegVuWq5evZry5cuzZ88e9uzZk+uaYRgFKiLnzJlzxet+fn5MnDgxV6PyctWqVWPhwoX5/kwRkeuWnGztRRQTY+1LZOdPf7IK0AEDwOGAHTs8GGD+FcbJkRfvq9MjReRaubPevJqFCxdy880389JLL+W8VqNGjZx/Nk2TpUuXMmTIEDp06ADAjBkzaNWqFRs3bqRz584cPnyYL774gtWrV9O4cWMAxo0bx+DBgxk9ejRVqlRxW7wicgMwTVi5EkaOhOPH7ce0aQOvvQZNmng2NhGRG0iBm5afffZZYcQhIuL9zp2DefPgpZcgKcl+TGCgdQjPsGFw003Wa65mYXoJnRwpIt7Gk/XmZ599RlhYGMOHD+frr7+mSpUq9OnTh7/97W8AHDt2jPj4eFq1apXznvLly9O0aVO2b99O586d2b59OxUqVMhpWAK0atUKh8PBrl27uP/++wsUU7aX/3fD0y7mQ3nJTXlxrUhzs3cvjqgojP/+1/ayWbUq5owZmL17W4fueDBG/Zmxp7zYU15cU27sFUY+Cty0vOj06dMAOUu5RUSKraws+Ne/rJmTrp6WlykDw4dbDcuKFT0ZnVtoSbeIeCNP1JtHjx7lvffeY8CAAfzjH/9g9+7dTJ06FV9fXyIiIoiPjwcg+LJDMYKDg0lISAAgISEhT4ylSpUiICAg5/0Fcel+mvIH5cWe8uKaJ3PjSEuj6j//SeV//xvD5i/upo8PJx95hF8HDsRZrhzs3Omx2C6nPzP2lBd7yotryk3hK1DTMjU1lTlz5vDBBx+QmpoKQIUKFejcuTNPP/00FSpUKJQgRUSKhGnCunUwdiz88IP9GB8feOwxmDgRqlXzbHxupiXdIuINPF1vmqZJo0aNeOaZZwBo2LAhBw8eZMWKFURc7fSxQtK4cWN8fHyK5LO9UXZ2Nrt371ZeLqO8uObR3JgmxvLlGGPGYJw8aT+kfXucMTGENGhASOFGc0X6M2NPebGnvLim3Ni7mBd3ynfTMjk5md69e3Pq1Cm6dOlC7dq1ATh8+DDr1q1j69atrFixItem5SIiN6xNm6xZk1995XpMz54wdSrUr++5uEREirGiqDdDQkKoU6dOrtdq167Nxx9/nHMdIDExkcqVK+eMSUxMpEGDBgBUqlQpZ1boRVlZWaSkpOS8vyB8fHz0lyAbyos95cW1Qs/Njh3WQTpffml/vWZNmDMHIyICH8N7Hgjrz4w95cWe8uKaclP48t20nD9/PqVLl+b//u//qFSpUq5rw4cP57HHHmP+/PlER0e7PUgREY/Zvh2efx4u/GXV1n33wfTpcOednovrKtLTYdUqiI2FxEST4GCD8HDrdHAt7RaRG0VR1JvNmzfnp59+yvXakSNHqHZh9nz16tUJCQlh69at3H777QCkpaWxc+dOHn74YQBCQ0NJTU1lz549NGrUCIBt27bhdDppokM2RIqf06dh/HhYsACczrzXS5eG0aOtmvLiHueFRDWgiBRnjvwO/PTTTxk9enSeAhKsJ9DPPvssGzdudGtwIiIec+gQPPwwNG/uumHZvDl88gls3Oi2hmV6OixbBj16QLt2Jj16WD+np+f/HnFxULWqSb9+EBubxuefJxEbm0a/ftbrGza4JVQRkUJXFPVm//792blzJwsWLODnn39mw4YNrFy5kj59+gB/nFb+xhtv8Omnn7J//35Gjx5N5cqVc04Tr1OnDq1bt2b8+PHs2rWLb7/9lilTptC5c2edHC5SnDidsHAh1KsHr79u37B86CHYuxemTCn0hqVqQBEp7vI90/LUqVPcdtttLq/Xq1fvmjYaFxEpUr/9Bi+8YBWgWVn2Y+rWhRdftJaDO/L9rOeq4uIgMtIkKcm4cGL3eRyO0qxd609UVP5O7I6Lg/BwE0gBjuN0ZgAXa2g/kpOr0a1bALGxBl27ui10EZFCURT1ZpMmTZg3bx6zZ89m/vz5VK9enejoaLpe8i/NQYMGce7cOSZMmEBqaiotWrRg0aJF+Pn55YyZOXMmU6ZMoX///jgcDjp27Mi4cePcGquIFKGvvrKWgn/9tf312rUhJsZqWubT9cySVA0oIiVBvpuWFStW5NixY9x88822148dO6b9LEXkxpGSAq+8AnPmwO+/24+5+WbrgJ2BA8HX160f745CMz3danpCCqb5o82IDEzzRwyjNpGRAZw4oVPARcS7FVW9ee+993Lvvfe6vG4YBlFRUURFRbkcExgYyKxZs9wem4gUsfh4a5n3W2/ZXy9bFqKjYdSoAq3Hvp6H16oBRaSkyPeUobCwMObOncv58+fzXDt//jwxMTG0bt3arcGJiLhdejrMmmU9DX/xRfuGZUAATJtmLRn/xz/c3rDMW2hmXDYi48LrKURGmi6Xiq9aBUlJBqZ5/IqfZ5rHSUoyWL3aDcGLiBQi1Zsi4jWysmD+fGspuKuGZY8esG8fjBtX4IZleLhJcnIKsBen8wBw5MLve0lOTqFbN5O4OPv3qwYUkZIi3zMto6Ki6NGjB3/961/p06cPtWvXxjRNfvzxR959913Onz/PjBkzCjNWEZFrl5UFS5daMyePHbMdYpYpgzFsmHVqeFDQVW95rUt6LhaakJ9CM5DVq6Fv37zXY2O58GT+8qbn5TJwONJYt87f9j4iIt5C9aaIeIXNm62l4Dt32l+vXx9eew3uv7/At3bHLEnVgCJSUuS7aXnzzTezYsUKJk+ezOzZszFNE7CWy7Rq1YoJEybwpz/9qdACFRG5JqYJ69dby3b27bMdko2Dt3mMGL8JvNS6Bl2u3q+8riU97io0ExNNnM68s5HsOJ2ZnD5tAka+xouIFAXVmyJSpH791Tr1e/ly++v+/jBhAkRFWSeEXwN3PLxWDSgiJUW+m5YANWrUYNGiRaSkpPDzzz8DULNmTQIDAwsjNhGR6/P559asyW3bXA5Zw32M5Un20wAjtTzduplX3bD8evejdFehGRxs4HCUtj248nIOhy9BQSpWRcT7qd4UEY/LzLRmTk6aBGfO2I/p0wdmzIBq1a7ro9zx8Fo1oIiUFAVqWl4UEBBAkyZN3B2LiIh77Nhhzaz88EOXQz6jJWMYxtc0uvBK/jYsv5YlPZdviemuQjM8HNau9Qf8yLsv5qX8cDr9iYi4+ueJiHgL1Zsi4hGffgrDhrlckUPjxlZDs21bt3ycOx5eqwYUkZIi3wfxiIh4vR9/hEcegdBQlw3L77idv/Ia7VlwScPyD1fbsNwdG5+Hh4PTebHQvJIrF5q9ekHFiiaGceUn/oZRjYoVTXr2vMrHiYiIiJQUR4/C3/4GHTrYNywrVICYGPjuO7c1LOGPh9f54erhtWpAESkp1LQUkRvfyZPWE/IGDeDdd22H/FquDn2MxbRkKZ/QCtf7+lxcimN/9eKSnis/1b7yfdxVaJYpA0uWGEAAhlGbvE1QvwuvB7Bkif3MUREREZESJSMDpk2z6sZVq+zHDBgABw7A8OFQ6poWJ7rkjofXqgFFpKRQ01JEblypqdZm6HXqwLx51n5El6tSBV5/nUeb7+U9sytmPv6198dSnLyubUlPbu4sNLt0gdhYg8DAAOAOHI56wK0Xfr+DwMAA1q93fSiQiIiISInx4YfWcu+xY+H33/Neb94ctm6Ft9+2ashC4K6H16oBRaQkcO9jIxERT0hPhzfegBdfhMRE+zEVKlinPz79NJQrR8BGcDgyr3sfSXftR3mx0IyMDCApKfDChuyZOBy+OJ3+BAZe+QTyS3XtCidOWEvR163z5/Rpk6Agg4gI6NlTT9dFRESkhPvxRxgxwjpN0U5QkDX78vHHwcenUEO5+PC6Wzfr4bW15dClK3j8LjQ0r/7wWjWgiBR3alqKyI0jOxuWL7dmV/7yi/0YPz8YOhSefx6Cg3NedteG5e7c+NydhWaZMtC3LxdOl9QJkSIiIiKcOwczZ8L06day8MsZBjzxBEydmqtuvJr0dGtleWystQonONggPNyaRZmf+s2dD69VA4pIcaampYh4P9OEDRusE8H37rUf43BA//4waRLUrJnncq9eEBVlkpxczcWp3xbDqEZgoEnPnvZFn7vuc5EKTRERERE3M00C/vtfHD17wpEj9mPuvtvaXqhFiwLdOi4OIiNNkpKMC83G8zgcpVm71p+oKK2UERFxJzUtRcS7ffEFjBkDW7a4HhMebi0Vb9jQ5RB3LcW5lvtkZ+f3y4qIiIjIdTlwAMfw4dT9+GP765Urw8svQ79+1kPvAoiLg/BwE0gBjuN0WjWgtW2QH8nJ1ejWLYDYWIOuXa9+Pz28FhG5Mh3EIyLeadcueOghaNPGdcPy4rV1667YsLzIXRuWa+NzERERES+TlmZtD9SoEYZdw9LHB6KiYP9+iIwscMMyPd2aYQkpF1bbXL7cPOPC6ylERpqkp1/b1xARkT9opqWIeJeffrL2rHznHWtZuJ2mTeGll+CBB6y9iArAXUtxtKRHRERExAuYJqxcCSNHwvHj9mPatIHXXoMmTa75Y1atgqQkA3DxGTnhHCcpKZDVqy/OoBQRkWulpqWIeIdTp6wl3m+8AZmZ9mNuvdXaKP3vfy/w0/FLuWspjpb0iIiIiBShvXth2DDYtMn2slm1KsbMmVbtWMAH3ZeLjeXCHpZXOogRIAOHI4116/zVtBQRuU5qWopI0TpzBmbNsn6lpdmPqVwZxo+HwYOhdGnPxiciIiIi3iUlBSZPhldftd083CxVipN9+hASE4NPYKBbPjIx0cTpPJ+vsU5nJqdPm+ihtojI9VHTUkSKRkYGLFhgzZxMSLAfU748PPssjBgB/v6ejU9EREREvItpwrJlMHo0nDxpP6ZDB5xz53I8PZ2Q8uXd9tHBwQYOR+kLh+5cmcPhS1CQGpYiItdLB/GIiGdlZ1vFZoMG8PTT9g3L0qWta4cPWzMs1bAUERERKdl27IDWraF/f/uGZc2asGYNfPKJVWdeJj3dKkF79IB27Ux69LB+zu+BOeHh4HT6A35XGemH0+lPRET+7isiIq5ppqWIFLr0dFi10uTogvfpvSuaOmd32w80DOjXz1ruU6uW63utsvYVSkw0CQ42CA+HXr3Q4TciIiIixc3p09ZD7AULsJ3mWLq0NfPy+efhpptsbxEXZ538nZRkXNiX8jwOR2nWrvUnKspkyRKDLl2uHEavXhAVZZKcXO3CKeH2DKMagYEmPXtqpqWIyPVS01JEClVcHLzRdzPRZ57nUTa7Hti1q3UQT6NGV7zX9RacIiIiInIDcDrh7betZqSrrYQeegjmzIG6dV3eJi4OwsNNIAU4nnOQjtX/9CM5uRrdugUQG2vQtavrcMqUgSVLDLp1C8AwamOax4FLD+XxwzCqAQEsWWLoYbqIiBtoebiIFJpNr+2Bbl358ExrWrtoWH5BGJunb4b166/asAwPN0lOTgH24nQeAI5c+H0vyckpdOtmEhdXKF9FRERERDzlq6/g7rth0CD7hmXt2rBhg/XrCg3L9HTrgTekXJgdefnJ3xkXXk8hMtK86lLxLl0gNtYgMDAAuAOHox5w64Xf7yAwMID16/UQXUTEXdS0FBH3+/lnsvv2p+3wJnRlg+2QXdSlM3NpSxxdX251xSLR3QWniIiIiHih+Hh4/HG46y74+us8lzN8yrKjxxTSv91rzbK8itWrDZKSjAuzIl0zzeMkJRmsXn31ELt2hRMnDJYtg/Bwf9q1CyQ83J9ly6zX1bAUEXEfNS1FJI9r3qg8Pp7qs2bhuP12fN5ZigMzz5Aj/IlHeYFQ3uUDWmNy4qpF4qpVuL3gFBEREREvkZUF8+dDvXrw1lu2Q9bwEA2c3xC6ZhxVa/uxwf65eC7r11tbCuV94H25DByONNaty1+4ZcpA377WuT+bNhmsWWP9rCXhIiLupaaliOQSFwdVq5r06wexsWl8/nkSsbFp9OtnvW5bIJ45A5Mn47jtNqq89x7G+fN5hpyiIsMZRX3WspzOOPG5cOXqRWJsLIVScIqIiIhIEdu8GVq2hKFDITk5z+UfuJX7mU9PJnPETKcg2wKdPm3idOatS+04nZmcPp33gbuIiBQdHcQjIjkKvFH5+fPw5pswZQrEx2N3RuIZbmImjzKbR0ijnO3n/lEk2p+ymJh4LQWnTmwUERER8Vq//mqd+r18ue3lNMoxmceJ4WEy8b3kirUtkGHUJjIygBMnXB96ExRk4HCUtj10/HIOhy9BQaofRUS8iWZaighQsH0jB/TP5vzi5VC/Pgwfbu0/dJlMw5dXGUId1vMCg102LOHqRWJwsFVw5ocKThEREREvlpkJs2dbdaSLhuU79KE+ccyk32UNyz/kZ1ugbt1MnE5/wO8qQfnhdPoTEZG/ryAiIp6hpqWIAPndN9LkQfNdPktuTunHHoUjR/KOMAycffuyYeYBonideP50lU++epEYHo4KThEREZEb3WefQdOmMHKktb3Q5Ro1Ynzr/9LP8SYnCLzKza6+LVDPniYVK5oYRrUr3skwqlGxoknPnlf9BiIi4kFqWooIcPV9I+9hJ58ziPd5iqbssh1jdu7M9++9h/mvf9HpyVvcViT26oUKThEREZEb1dGj0Ls3tG8P+/blvV6hAsTEwPbtfOFo47Z9KMuUgSVLDCAAw6hN3gfgfhdeD2DJEtfLzEVEpGioaSkigOt9IxtymFieYQuP0Ybt9m9u1Qq++ALn+vWk160LuLdIVMEpIiIicgPKyICXXoIGDWDlSvsxkZFw4IC15VCpUm7fFqhLF4iNNQgMDADuwOGoB9x64fc7CAwMYP16gy5dCvLFRETEE3QQj4gAf+wbeXGj8pr8yiTepD//wYGLJ9h33GEVog89BIYB2dm5Ll8sEiMjA0hKCsThSMPpzMTh8MXp9Ccw0GTJkvwVie68l4iIiIgUso8+shqRBw/aX2/eHObNg3vuyfVyeDisXXtxWyD7FUCW/G8L1LUrnDhh7X+5bp0/p0+bBAUZRERAz5564C0i4q3UtBQR4I8CMZjfiWYBT7ESPzJtxx6hFsefeIG/zH8EfHyueF93FokqOEVERES83E8/wYgRsH69/fWgIJg2DR5/3LaO7NULoqJMkpOrXTgE0p5hVCMw0KRnz/wdwFimDPTta/0CHdooInIjUNNSRADo9WAaR8vMZmj6TCpgszE6EE8g0xjNe4EjODK3DFy5X5nDnUWiCk4RERERL3TuHLz8svUrPT3vdcOAwYPhxRchONjlbS5uC9Stm7UtkHVI5KUzLv0u7HOubYFERIo7NS1FSrrz52HhQspMmUJ0+knbIWmUZRb9mc3znDFqsH6pCkQRERERAUwT4uLg6afhyBH7MXfdBfPnQ4sW+bqltgUSERFQ01Kk5HI6YcUKGD8efrRfenMeXxYwgJeM5/jNrE3FiibrVSCKiIiICFgH6ERFWftX2jBDQjBmzIB+/cBRsDNgtS2QiIioaSlS0pimVVg+/zzs3Gk/xDD4qdUjzPSfzL6MW2mlAlFERERELkpLs5Z5z5oFmXn3QM/GwTyGMuf8JF4LrkiXgvUrc2hbIBGRkk1NS5GSZNs2GDMGPv/c9ZhOnTCmTaN206a87rnIRERERMTbmSasWgUjR8KxY7ZDPqc5wxjNbhphpBp062YSG2vQtauHYxURkRuempYiN7j0dKt2jI2FxEST4GCD8HDr5MWcWZH79kF0tDXIlXvugenToU2bwg9aRERERG4se/fCsGGwaZPt5RNUYhRP8x4PYM2KzMA0f8QwahMZGcCJE1qxIyIiBXONE/Xd75///Cf169fnxRdfzHktIyODyZMnc9dddxEaGsqwYcNISEjI9b4TJ04wePBgmjZtyj333MPLL79MVlaWp8MXKRJxcVC1qkm/fhAbm8bnnycRG5tGv37W6//39lEYOBAaNXLdsGzY0Lr25ZdqWIqIiIhIbqmp8Mwz0LSpbcMyk1LMoB/1Wct7PMjly7hN8zhJSdbelCIiIgXhFTMtd+3axYoVK6hfv36u16dNm8bnn3/O3LlzKV++PFOmTGHo0KGsWLECgOzsbJ544gkqVarEihUrOHXqFM899xy+vr4888wzRfFVRDwmLg7Cw00gBTiO05kBWOfrBHGO6KQVtB64EMiwv0GNGvDCC/Doo+Dj46mwRURERORGYJqwfDk8+yycPGk7ZCP3MpSn2M+tV7hRBg5HGuvW+V/Ym1JERCR/inym5dmzZ3n22WeZOnUqAQEBOa+fOXOGNWvWMGbMGO655x4aNWrEtGnT/n97dx4f473+f/w1E0ksIUg4RKu1NLGFRBek1KlD22MpqaVaS7UOqpYUtVMJaiulaEsVVZzjtEpKS3vwa/ut2mqr5QRFa4sSscRSEpn5/XE3c0TuISGzSN7PxyOPZu77M5NrLiO9XPf9+XzYsWMHO3fuBGD9+vUcPHiQt99+m6pVq9KwYUNiYmJYvHgxqampHnpHIq539Sp06WI0LO32w2Q0JgvzB0OZx2GeZgAzKWjWsAwKgnfeMXZ77NJFDUsRERERyWznTmjQwNj126xhWb48I6stpQmf3aZhabDZ0jh71p77cYqISJ7m8TstR48eTcOGDYmKiuKDDz5wHN+zZw9paWlERUU5jlWqVImQkBB27txJREQEO3fuJDQ0lODgYMeY+vXrExsby8GDB6lWrVqO40lPT7+7N5THZORDecnM03n5978tnDtnBU4AUIA0/kE8bzKHsiSbPsdepAj211/H3r8/ZFwgyOX4PZ0Xb6W8OKfcmFNezCkv5pQPEck1Z8/CyJEwa5Yxfedmfn4waBAMHcp/OxXGuu+S6bCbWa2+lCyp3b9FRCRnPNq0/Oqrr/jvf//LUpMFTs6cOYOvry/FihXLdDwoKIikpCTHmBsbloDjccaYnNq9e/cdPS+vU17MeSovCxZUxGotgN32B+1Yw1jepzLmOzimUYCvynWhwry2XA8Kgl9/dXl8+ryYU16cU27MKS/mlBcRkVxms8G8eTB0KNy0h4BDs2YwbRpUrgxAq1awbFkA4I/T5YgA8MdmCyA6OndDFhGRvM9jTcuTJ0/y1ltvMW/ePPz9/T0VRhbh4eH4aLqsQ3p6Ort371ZebuLpvNjSobFtOeMZQW32Ox23mGd4k7e4v3IE6/7m+ik5ns6Lt1JenFNuzCkv5pQXcxl5kTszY8YMZs6cmelYhQoV+PrrrwFjY8gJEyawatUqUlNTqV+/PqNGjcp04TwxMZHY2Fg2b95M4cKFadWqFQMGDKBAAY9PahK5vS1boHdv+Okn8/MVK8K770Lz5pkOt20LMTF2zp8v9+dyReYslnIUL26nTRvdaSkiIjnjsUpq7969JCcn89xzzzmOpaen89NPP7F48WLmzp1LWloaKSkpme62TE5OplSpUoBxV+WuXbsyvW7G7uIZY3LKx8dH/xAyobyY80hetmxhRsIQwsm6e2OG1UQxlN78TBhWaygRQVa3Ll2pz4s55cU55cac8mJOeZHc9tBDDzF//nzH4xs/X9oYUvKspCTjzsq5c83PFyoEw4bBG29AwYJZThcsCAsWWGjZMhCLpSJ2+wky33Hpj8VSDghkwQKL2UuIiIjcksealnXr1mXlypWZjg0dOpSKFSvSrVs3ypYti6+vLxs3buTpp58G4PDhwyQmJhIREQFAREQEs2bNIjk5maCgIAA2bNhAQEAAlf+ctiCSZ+zbByNGwOefE+5kyCZqMIQ+fM8jfx7RdBwREZHb8fHxMb3gnbEx5OTJk6lXrx5gNDGbNm3qWGM9Y2PI+fPnExwcTNWqVYmJiWHy5Mn07t0bPz8/d78dkVu7fh1mzzbqyvPnzce0bg1TpsADD9zypVq0gPh4C126BHLuXHGs1kvYbGlYrb7YbAEUL25nwQILLVrk/tsQEZG8z2NNy4CAAEJDQzMdK1y4MMWLF3ccb926NRMmTCAwMJCAgADGjh1LZGSko2lZv359KleuzKBBgxg4cCBJSUlMmzaNDh06qECUvOP4cYiLM9YZcrLSeQIPMoxexPMk8L+pN5qOIyIicntHjhyhfv36+Pv7ExERwYABAwgJCdHGkF5Cm3CZu6O8rF+PNSYGy88/m562h4VhmzYNmjTJ+CG3fclmzeDYMfj8cwvx8UU4e9ZOyZIWWrWy0bq1nYIFc33vx9vSZ8ac8mJOeTGnvDin3JhzRT68eqGdYcOGYbVa6du3b6Y1hDL4+Pgwa9YsYmNjef755ylUqBDR0dH07dvXg1GL5JKzZ2HCBJgxA65eNR1yjPuIZTALaEA6N/6C0HQcERGR7KhZsybjx4+nQoUKJCUl8d5779GhQwdWrlypjSG9jPJiLjt5KXDmDPe9+y5Bq1ebnk8vVIiT3bpx+oUXsPv6ws6dOY6jenXj60b79uX4ZXKVPjPmlBdzyos55cU55cb1vKppuXDhwkyP/f39GTVqVKZG5c3KlSvHnDlzXB2aiPtcuWIsdj5xIly4YD6mZEkYNoyfH+jF8u7+pJ+zaDqOiIjIHWjYsKHj+ypVqlCrVi2efPJJVq9eTUEPXfXTZlOZaRMuc9nKS1oalpkzsYwejeXiRdMhthdegAkTKFuuHGVdGK876TNjTnkxp7yYU16cU27MuWJzSK9qWorka2lpxhTwuDg4edJ8TOHC0K8fDBwIgYE0BxKbw9KlsHx5gGM6TnQ0tGmjOyxFRERyqlixYjz44IMcPXqUqKgobQzpRZQXc07zsm4d9OkDCQmmz9tNDYYGzKDHC3+lRXkXB+kh+syYU17MKS/mlBfnlBvXs3o6AJF8z2aDTz815tO8+qp5w7JAAejZEw4ehLFjITDQcapgQejYET7/HL791sLnnxuP1bAUERHJucuXL3Ps2DFKlSpFjRo1HBtDZjDbGPLAgQMkJyc7xmhjSPGoY8egXTto3Ni0YXmBIvTlDSJZxKrLEbRsaWfFCg/EKSIichu601LEk9auhSFDYNs252Pat4cxY0D/8BEREcl1EydO5MknnyQkJITTp08zY8YMrFYrzZs3p2jRotoYUu4d167BO+8YF7ivXDEdMo9nGUpvThMEpIP9MBZLRbp0CSQxUbN0RETEu6hpKeIJW7cazcp165yPefppGDcOatd2X1wiIiL5zO+//07//v05f/48JUuW5OGHH+bTTz+lZMmSgDaGlNx39Sp89hnEx0Nysp2gIAutWkHbtncxU2b1aujb15iVY2IbVejFEDYTnuWc3X6Cc+eKs3SpMVtHRETEW6hpKeJCNxelNfx+YeD54Tzw01LnT3rsMWPX8CefdFucIiIi+dXUqVNveV4bQ0puWrECunSxc86xiWIqVqsfy5YFEBOT800U/Y4fxxoXBytXmp5PpgTD6MlHtMKGs3XXrmG1XmL58gA1LUVExKuoaSniIjcWpfdZDjDCPo6uLKIA6eZPCAsz7qyMjgaLxb3BioiIiIhLrVgBrVrZgQvACWy2a4CxvDn4c/58OVq2DCQ+3sKzz97mxf74A8v48VSfOBFLamrW8xYLK8p04+WTAzlLym1js9nSOHvWDqgGFRER76GNeERcwFGUnjvCeP7BfntNerDAvGFZrhzMmQN79sBzz6lhKSIiIpLHXL1qXMyGC9jth4FrN4249ufxC3TpYufqVScvZLcbU3iqVcM6ZgxWs4Zl3brw008sqDeb89Yy2YrPavWlZEnVoCIi4l3UtBTJZVevQs+XLjPIHschajKEuRTOUpjCWYozqtBEru7+Bf7xD2OHcBERERHJcz77DM6ds2C3n7jlOGN9SQtLzVYSOnAAmjY1ZuX89lvW86VLw/z58OOP8PDDtGoFNlsA4H+b6Pyx2QKIjs7eexEREXEXNS0l37h6FRYuhNat4a9/tdO6tfHY6ZXsO3H9Ojtem8OW86FMII4SXMwy5Ar+jONlKrKK0X8MYulXhXIxABERERHxNvHxYLVeIusdljfLWF/yhkOXLsHQoVCjBnz9dZZn2H18ICYG9u+HLl3AavwTr21bKFHCjsVS7pY/0WIpR4kSdtq0yck7EhERcT01LSVfWLECQkLsdO4M8fGX+P77c8THX6JzZ+O4k7XLs89uh6VLoXp16s3vTjkSswy5jg+zaE1lvmA4vbmAf9aiVERERETynORkOzabyVRuE471Je12+Pe/oUoVY5PGtLQsYy/Wro3tp59g2jQoXjzTuYIFYcECCxCIxVKRrHdc+v95PJAFCyx3vnO5iIiIi2g+quR5ubrouZl162DIENi61emQf9OEEbzGQcpnOq5Fz0VERETyvqAgC1ar35/1561Zrb7U8tkLjfrAd9+ZDwoJwTZpEgfCwoioWdPpa7VoAfHxFrp0CeTcueJ/7liehtXqi80WQPHiOd+xXERExF3UtJQ8Leui5zczFj23WCrSpUsgiYnZv8pcaN8+rEOGwNq1Tsf8hzoMpQ/bqWp6Xouei4iIiOR9rVrBsmUZ60s6nyJejFRG2d7k9W+ng81kA8cCBaB/fxgxAnvhwrBz521/9rPPQmKisU7m8uUBnD1rp2RJC9HR0KaN7rAUERHvpaal5GkZi55DdhY9L87SpdCx421e9JdfsIwYQbVPP3U65CceYQiv8v+IvMULadFzERERkfygbVuIibFz/nw50wvpFmx0ZBWTeI8ynAazOzIbN4YZM4zp4gDpJk1NJwoWNGpco87VBXMREbk3aE1LydPuatHzm508CT17QrVqWJ01LENDSV38GU8X38y3lta3/Ila9FxEREQkf7jV+pK12M8PdOMTRhkNy5uVLw+ffw7/+c//GpYiIiL5gJqWkqfd0aLnNzt/HoYNg0qVYNYsuH4965iyZWH2bNizB78X27DgEyta9FxEREREMmSsL1m8eCBQnSBLMDN5n2105HF2Zn2Cvz+MHAkJCfDcc2DRHZIiIpK/aHq4uMTVq8bU7Ph4o3EYFGShVStjaow7m3Q5XfQ80/qSf/wB770H48bBuXOmz7EXL45lyBDo0wcKF3Yc16LnIiIiIp7lLfXojZ59FhKP2djZdx7VFg2lWGqy+cDmzY0dwStVcmt8IiIi3kRNS8l1K1YYm9+cO2f5s1mXitXqx7JlAcTEuLdZl91FzzOtL3n9OixYALGxcPy46Wh7wYKcateOUlOm4BMcbDpGi56LiIiIt/DGBp4reVM9msmWLRTs1Yu6W7ean69UyWhWNm/u1rBERES8kZqWkqtWrIBWrYzduuEENpvRKDTudPTn/PlytGwZSHy8hWefdX08t1v0PIPFUo7igTbaFYiH8OGwb5/5QB8feOUVbCNGcCIpiVIlStzy52vRcxEREfE0r23guYi31aMAJCXB0KEwd675+UKFjOWI3ngjb3aRRURE7oDWtJRcc/WqURDDhT8bhDff2Xjtz+MX6NLFztWrro/pVoueG4z1JRvad3CodBR+L7R23rBs0wb27oUPP4Ry5VwcuYiIiMjdy2jgnT9/AdiLzXYA+O3P/+7l/PkLtGxpZ8UKDweaS7yuHr1+HWbOhNBQ5w3L1q2NdStHjFDDUkRE5AZqWrrY1auwcKFRi/z1r3ZatzYeu6Nh526ffQbnzlmw20/ccpzdfoJz54xp0+5w86LnVmsoUAGrNZQIUlnj055vaUSJA5vNX6BRI9iyxXiDYWHuCVpERETkLnldA88NvKoeXb8eHnnEWPv8/Pms58PCjB3Bly6FBx5wYSAiIiL3JjUtXWjFCggJsdO5M8THX+L7788RH3+Jzp2N4ytXejrC3BUfD1brJW69diTANazWSyxf7oag/pSxvuTChdCqVQAv1knmu5Du7KA2f7v+jfmTatc2Csm1a+HRR90XrIiIiEgu8KoGnpt4RT168iR06gQNGsDPP2c9HxAAkybBrl3QpIkLAhAREckb1LR0kfw2FQeMRd1tttRsjbXZ0jh71u7iiDIrWBA6Nv6dz8v0YvG2qjQ4/i/zgZUrw5Il8NNPRiFp0VqUIiIicu/xigaem3m0Hk1LgylTjDsoFy0yH/Pii8ZSRAMHgp9f7v1sERGRPEgb8bhA1qk4NzOm4lgsFenSJZDExLyxk3RQkAWr1e/PRc5vzWr1pWRJNzYDL1yAyZPhnXfgyhXzMWXKwKhR0LUr+Pq6LzYRERERF7izBt69fbHWY/XounXGNPCEBPPz4eHG2pZPPJE7P09ERCQf0J2WLpAfp+IAtGoFNlsAWTe7uZk/NlsA0dFuCOrqVaNRWakSjB1r3rAMDIRx4+DgQXj1VTUsRUREJE/IaOBlh9svKLuI2+vRY8egXTto3Ni8YRkYCNOnw/btaliKiIjkkJqWLpAfp+IAtG0LJUrYsVhuvbO2xVKOEiXstGnjwmDS02H+fGOnxgEDIDk56xh/f3jjDTh0CIYOhSJFXBiQiIiIiHt55QVlF3NbPXrtmnHRu0oV444FMy+/DPv3G3dgFtAENxERkZxS09IFvH1tR1cpWBAWLLAAgVgsFclaIPv/eTyQBQtcNCXebje6xjVrwiuvGFe/b2a1GlPAf/kF3n4bgoJcEIiIiIiIZ3nVBWU3cUs9uno11KgBw4ebz+KpXRs2boR58+Avf7mDHyAiIiKgpqVL5MepOBlatID4eAvFiwcC1bFaQ4EKf/63OsWLB/LFFxZatHDBD/+//4PHH4foaPjvf83HREfDnj3w0Udw//0uCEJERETEO3jFBWUPcFk9evgwtGwJTZsaywrdrGRJmDULtmyBunXv/o2IiIjkc5qn4AKtWsGyZRlTcW41RTzvTMW50bPPQmKisVbn8uUBnD1rp2RJC9HR0KaNCwrin382pnevXu18zF//ChMmQJ06ufzDRURERLxXRgOvS5dAzp0rjtV6CZstDavVF5stgOLF7SxY4KILyh6Uq/XoH38YdeTEica08JtZLNCjh7F+umbwiIiI5Bo1LV2gbVuIibFz/nw5J7uHGyyWchQvbqdNm7xzp2WGggWhY0fjy2W7UB4+DG++Cf/8pzEt3ExEBIwfD08/bRSUIiIiIvmM2y8oe4m7rkftdvjiC+jXD377zXxM3brw3nvGlHARERHJVWpaukDGVJyWLY2pOMYu4jdelfX/c22hvDUVx21OnTKuZM+eDWlp5mMqVjTGPP+8sYaliIiISD7mlgvKecmBA9C3L3zzjfn50qWNOy87d1atKSIi4iL6P6yLeHRtx7wqJcW4s7JSJZg507xh+Ze/GFe7ExLghRdURIqIiIhI9l26ZCw7VKOGecPSxwdiYoxdwbt0Ua0pIiLiQrrT0oXy61ScXHftGrz/Prz1FiQnm48pVgwGDTKKyIAA98YnIiIiIvc2ux0+/RQGDIATJ8zHPPGEceE8PNy9sYmIiORTalq6mKbi3IX0dFi0yLi78uhR8zH+/tCrl3FFPDjYvfGJiIiIyL1vzx7o0we++878fEgITJ4M7dtrjXQRERE3UtNSvI/dDitXwrBhsHev+RirFV56CWJjoXx5t4YnIiIiInnAhQsQFwfTpxsXy29WoAD07w8jRkDRou6PT0REJJ9T01K8y/r1MGQI/Pij8zGtWhmb7FSv7rawRERERCSPsNmM2TyDBhkbPJpp0sRoZlap4t7YRERExEFNS/EOu3YZd1Z+9ZXzMU88ARMmQL167otLRERERPKOHTugd2/YsMH8fPnyMHUqREdrKriIiIiHabs78axff4VOnSAiwnnDslYtWLXKWGdIDUsRERERyamzZ+G11+CRR8wblv7+MHIkJCTAc8+pYSkiIuIFdKeleMbp08Zu4B98AGlp5mMqVDCmgbdvb6xhKSIiIiKSEzYbzJ1rbNqYnGw+pkUL4+7KSpXcG5uIiIjckpqW4l4pKfDOOzBlCly6ZD6mdGnjSnf37uDn5974RERERCRv2LIFevWCrVvNz1eqBO++C82auTcuERERyRY1LcU9rl2DWbOMOyfPnDEfU7QoDBwI/fpBQIB74xMRERGRvCEpybizcu5c8/OFCsHw4TBgABQs6N7YREREJNvUtBTXSk+HxYvhzTfhyBHzMX5+xlXwoUOhVCn3xiciIiIiecP168ZF8pEj4fx58zGtWxuzfsqXd2toIiIiknNqWopr2O3GxjpDh8KePeZjLBbo3Bni4uCBB9wbn4iIiIjkHT/8YOwKvmuX+fkqVWD6dGjSxL1xiYiIyB3T7iaS+378EZ54wljU3FnD8tlnjaLy44/VsBQRERGRO3PyJHTqZNSeZg3LgAB4+234+Wc1LEVERO4xutNScs+ePTBsGKxc6XxMgwYwYQJERbkvLhERERHJW9LSjDsnY2Odb+744oswaRKUK+fW0ERERCR3qGkpd+/IEWPNyoULjWnhZsLDYfx4aNrUmBYuIiIiInIn1q2DPn0gIcH8fHg4zJxp3H0pIiIi9yyPTg//5z//SYsWLahduza1a9fm+eef5/vvv3ecv3btGnFxcdSpU4fIyEj69OnDmZt2nk5MTKR79+7UqlWLevXqMXHiRK5fv+7ut5I/JSXB669DaCh88ol5w/LBB41zO3ZAs2ZqWIqIiIjInTl2DNq1g8aNzRuWgYHG3Zfbt6thKSIikgd49E7LMmXK8MYbb/DAAw9gt9uJj4+nV69eLF++nIceeohx48bx/fffM23aNIoWLcqYMWPo3bs3S5YsASA9PZ0ePXoQHBzMkiVLOH36NIMHD8bX15f+/ft78q3lbRcvwtSpMHmy8b2ZUqWMnRu7dwd/f/fGJyIiIiJ5x7VrlJk3D+vHH8OVK+ZjXn7ZWIKodGm3hiYiIiKu49GmZaNGjTI97tevH//617/YuXMnZcqU4fPPP2fy5MnUq1cPgHHjxtG0aVN27txJREQE69ev5+DBg8yfP5/g4GCqVq1KTEwMkydPpnfv3vj5+XnibeVdqakwezaMGWPcZWkmIADeeAP694eiRd0bn4iIiIjkLatWYY2JodzBg+bna9eG996DunXdG5eIiIi4nNesaZmens7XX3/NlStXiIyMZM+ePaSlpRF1w4YtlSpVIiQkxNG03LlzJ6GhoQQHBzvG1K9fn9jYWA4ePEi1atXuKA75n/T0dLDZsC9ahD0uDsuvv5qOs/v6Yu/ZE/vQocZdlsaT3Ripe2V8TvR5yUx5Mae8OKfcmFNezCkv5pSP3PXhhx8yZcoUOnfuzPDhwwFjyaIJEyawatUqUlNTqV+/PqNGjcpUgyYmJhIbG8vmzZspXLgwrVq1YsCAARQo4DXl9r3l8GFjGaKVKzFdXKhkSWO99K5dwcfHzcGJiIiIO3i8itq/fz/t27fn2rVrFC5cmPfee4/KlSuTkJCAr68vxYoVyzQ+KCiIpD/v8jtz5kymYhFwPE5ydifgbezevfuOnpcn2e0U+/FHqr73HgV++cV8iMXC2aZNSezRg9SQEDhxwvjKJ/R5Mae8mFNenFNuzCkv5pQXcZVdu3axZMkSwsLCMh3XkkVudOUKTJxofF27lvW8xQI9esDYsRAU5P74RERExG083rSsUKEC8fHxXLx4kW+++YbBgwezaNEij8UTHh6Oj67WwsaNWIcNw/LDD06H2Js1wzZ2LMXDwynuvsi8Qnp6Ort379bn5SbKiznlxTnlxpzyYk55MZeRF7k7ly9fZuDAgYwdO5YPPvjAcfzixYtassgd7Hb44gvj7sojR8yH1KmD5f33jSnhIiIikud5vGnp5+fHAw88AECNGjXYvXs3n3zyCX//+99JS0sjJSUl092WycnJlPpz+nFwcDC7du3K9HoZu4tnjMkpHx+f/P0Pob17Yfhwo2h05vHHYcIELPXrk48zBejz4ozyYk55cU65Mae8mFNexBVGjx5Nw4YNiYqKytS09MSSRfluyv/+/Vj79cPyn/+YnraXLs1vr71GucGD8fH1zdNLEOWElsxwTrkxp7yYU17MKS/OKTfmXJEPjzctb2az2UhNTaVGjRr4+vqyceNGnn76aQAOHz5MYmIiERERAERERDBr1iySk5MJ+nN6yIYNGwgICKBy5cqeegv3pqNHYdQo+OQTsNnMx1Svbqwd1Ly5MTVHREREJA/46quv+O9//8vSpUuznDtz5ozblyzKL3fOWq9coezcuZRevBjL9etZztt9fDjdrh2JPXpgCwjg7N69HojS++WXz8udUG7MKS/mlBdzyotzyo3rebRpOWXKFJ544gnKli3L5cuX+fLLL9myZQtz586laNGitG7dmgkTJhAYGEhAQABjx44lMjLS0bSsX78+lStXZtCgQQwcOJCkpCSmTZtGhw4dNA0nu86cgXHjjF0XU1NNh1wrWxbfceOwduqkhc5FREQkTzl58iRvvfUW8+bNw9/f39PhAPlguSK7Hctnn2EZOBCLk7XQ7U88ge3ddwkOD6eEloYwpSUznFNuzCkv5pQXc8qLc8qNOVcsWeTRpmVycjKDBw/m9OnTFC1alLCwMObOncvjjz8OwLBhw7BarfTt2zfTTo0ZfHx8mDVrFrGxsTz//PMUKlSI6Oho+vbt66m3dO+4dAmmTYO334aUFPMxwcHYhg5lb7161HrsMTUsRUREJM/Zu3cvycnJPPfcc45j6enp/PTTTyxevJi5c+e6fcmiPL0Ewp490KcPfPed+fmQEJgyBcvzz+Nz08yePJ2Xu6C8OKfcmFNezCkv5pQX55Qb1/No03LcuHG3PO/v78+oUaMyNSpvVq5cOebMmZPboeVdqakwZw6MGQOnTpmPKVIEBgyAAQOwFymCfedOt4YoIiIi4i5169Zl5cqVmY4NHTqUihUr0q1bN8qWLasli3LDhQsQGwszZpivSenrC/36wYgRULSo28MTERER7+N1a1qKi9hssGQJjBwJhw+bj/H1hVdfNYrF0qWNY1pYVkRERPKwgIAAQkNDMx0rXLgwxYsXdxzXkkV3wWaDRYtg0CDnF8ybNIHp06FKFffGJiIiIl5NTcu8zm6Hb76BoUPB2R2TFgt06ABxcVCxolvDExEREfF2WrLoDu3YAb17w4YN5ufLlzeWK2rVSps8ioiISBZqWuZlmzbBkCHw/ffOxzRtamzEU6uW++ISERER8WILFy7M9FhLFuXQ2bPGzJ3Zs407LW/m72/ceTlkCBQu7P74RERE5J6gpmVelJAAw4fD8uXOx9SrBxMmwBNPuC8uEREREcm70tNh3jxjhk9ysvmYFi1g6lSoVMm9sYmIiMg9R03LvOTYMWOB848/Nr+qDVCtmnFn5bPPahqOiIiIiOSOzZuNqeBbt5qfr1QJ3n0XmjVzb1wiIiJyz1LTMi9ITjbumpwxA65dMx9z//0wejR06gQ+Pu6NT0RERETyptOnjTsr580zP1+okDEDaMAAKFjQvbGJiIjIPU1Ny3vZ5cvGFeuJEyElxXxMUJBRKPbsqUJRRERERHLH9eswaxaMHAnnz5uPadMGpkwxNtwRERERySE1Le9FaWnw0UfGnZO//24+pnBh6N8f3ngDAgPdG5+IiIiI5F0//GBMBd+1y/x8lSowfTo0aeLeuERERCRPUdPyXmKzwWefGbsxHjxoPqZAAejRwxhTpox74xMRERGRvOvkSRg4EBYvNj8fEACjRkHfvuDn597YREREJM9R0/JeYLfDmjXGekHbtzsf98ILMGaMdmMUERERkdyTlmbcORkbC5cumY/p0AEmTYKQELeGJiIiInmXmpbebssWGDIEvv3W+ZhnnoHx4yEiwm1hiYiIiEg+sG4d9OkDCQnm58PDYeZMeOIJ98YlIiIieZ7V0wGIE/v3G4uX16njvGGZcW71ajUsRURERCT3HD0K7dpB48bmDcvAQOPuy+3b1bAUERERl9Cdlt7mxAlj6s38+ZCebj6mShUYNw5atQKLxZ3RiYiIiEhedu2aseP3W2/BlSvmY155xZjlU7q0e2MTERGRfEVNS29x9ixMnGhcsb561XzMffdBXBx07mxsuCMiIiIikltWrYKYGOcbPj78sDEVvG5d98YlIiIi+ZI6X5525YrRqJw4Ec6fNx9TogQMGwa9ekGhQm4NT0RERETyuMOH4fXXYeVK8/MlSxp3VnbtCj4+bg1NRERE8i81LT0lLQ3mzTPunDx50nxMoULQrx8MHAjFi7s1PBERERHJ465cMS6cT5xoTAu/mcUCr74KY8ZAUJD74xMREZF8TU1Ld7PbYelSGD4cfvnFfEyBAtCtG4wcCWXLujc+EREREcnb7HaIjzcujh85Yj6mXj1jKnjt2m4NTURERCSDmpbutHYtDBkC27Y5H/P88zB2LFSu7L64RERERCR/2L8f+vaF//zH/Hzp0jBpEnTqBFare2MTERERuYGalu5w8aLRjFy92vmYp54y1grS1WwRERERcYV33jEuoKelZT3n4wN9+kBsLAQGuj00ERERkZupaekOgwY5b1g++ihMmACNGrk3JhERERHJP378EQYMMD/XsCHMmAHh4e6NSUREROQWNOfDHczWCgoLM9a23LxZDUsRERERca2jR7MeCwmBf/0Lvv1WDUsRERHxOmpausPrr0PBgsb35crBnDmwZw+0bm3syigiIiIi4kotW0LVqsb3vr7GTKD9+6F9e9WjIiIi4pU0PdwdnnoKfvsNEhOhWjXw9/d0RCIiIiKSnxQuDLt2wdatxoyfEiU8HZGIiIjILalp6S5/+YvxJSIiIiLiCQUKQN26no5CREREJFs0PVxERERERERERES8ipqWIiIiIiIiIiIi4lXUtBQRERERERERERGvoqaliIiIiIiIiIiIeBU1LUVERERERERERMSrqGkpIiIiIiIiIiIiXkVNSxEREREREREREfEqalqKiIiIiIiIiIiIV1HTUkRERERERERERLyKmpYiIiIiIiIiIiLiVdS0FBEREREREREREa+ipqWIiIiIiIiIiIh4FTUtRURERERERERExKsU8HQA3sButzu+T09P92Ak3icjH8pLZsqLOeXFnPLinHJjTnkxp7yYuzEfN9Y0cm9RPeqc/u6bU16cU27MKS/mlBdzyotzyo05V9SkFruqW1JTU9m9e7enwxARERG5K+Hh4fj5+Xk6DLkDqkdFREQkr8itmlRNS8Bms3H9+nWsVisWi8XT4YiIiIjkiN1ux2azUaBAAaxWrf5zL1I9KiIiIve63K5J1bQUERERERERERERr6JL8SIiIiIiIiIiIuJV1LQUERERERERERERr6KmpYiIiIiIiIiIiHgVNS1FRERERERERETEq6hpKSIiIiIiIiIiIl5FTUsRERERERERERHxKmpaioiIiIiIiIiIiFdR01JERERERERERES8Sr5sWh4/fpxhw4bRqFEjatasSePGjZk+fTqpqam3fN61a9eIi4ujTp06REZG0qdPH86cOeOmqN3jgw8+oH379tSqVYtHHnkkW88ZMmQIYWFhmb66du3q4kjd705yY7fbeffdd6lfvz41a9akS5cu/Pbbb64N1M3Onz/PgAEDqF27No888gjDhg3j8uXLt3xOp06dsnxm3nzzTTdF7BqLFy+mUaNGhIeH07ZtW3bt2nXL8atXr+aZZ54hPDycFi1a8P3337spUvfLSW6WLVuW5bMRHh7uxmhd76effuLVV1+lfv36hIWFsXbt2ts+Z/PmzURHR1OjRg2aNGnCsmXL3BCp++U0N5s3b87yeQkLCyMpKclNEbve7Nmzad26NZGRkdSrV4/XXnuNw4cP3/Z5+el3zL1MNalzqknNqR41p3r0f1STmlM9mpVqUnOqR815qibNl03Lw4cPY7fbGT16NF999RVDhw5lyZIlTJ069ZbPGzduHN9++y3Tpk1j4cKFnD59mt69e7spavdIS0vjmWee4YUXXsjR8xo0aMD69esdX++8846LIvScO8nNnDlzWLhwIbGxsXz66acUKlSIrl27cu3aNRdG6l5vvPEGBw8eZP78+cyaNYutW7dmq+Br165dps/MoEGD3BCta6xatYrx48fTq1cvli9fTpUqVejatSvJycmm47dv386AAQNo06YN8fHx/O1vf6NXr14cOHDAzZG7Xk5zAxAQEJDps/Htt9+6MWLXu3LlCmFhYYwaNSpb448dO0aPHj2oU6cOX3zxBS+99BIjRozghx9+cHGk7pfT3GT4+uuvM31mgoKCXBSh+23ZsoUOHTrw6aefMn/+fK5fv07Xrl25cuWK0+fkp98x9zrVpM6pJjWnetSc6lGDalJzqkfNqSY1p3rUnMdqUrvY7Xa7fc6cOfZGjRo5PZ+SkmKvXr26ffXq1Y5jBw8etIeGhtp37Njhhgjd6/PPP7c//PDD2Ro7ePBge8+ePV0ckffIbm5sNpv98ccft3/00UeOYykpKfYaNWrYv/zyS1eG6DYZfwd27drlOPb999/bw8LC7L///rvT53Xs2NE+duxYd4ToFm3atLHHxcU5Hqenp9vr169vnz17tun4mJgYe/fu3TMda9u2rX3kyJEujdMTcpqbnPzuyQtCQ0Pta9asueWYSZMm2Zs1a5bp2Ouvv25/5ZVXXBmax2UnN5s2bbKHhobaL1y44KaoPC85OdkeGhpq37Jli9Mx+el3TF6kmjQz1aTmVI/+j+rR/1FNak716O2pJjWnetQ5d9Wk+fJOSzMXL14kMDDQ6fk9e/aQlpZGVFSU41ilSpUICQlh586dbojQu23ZsoV69erx9NNPM2rUKM6dO+fpkDzu+PHjJCUlZfrMFC1alFq1arFjxw4PRpZ7duzYQbFixTJNl4iKisJqtd52KsrKlSupU6cOzZs3Z8qUKfzxxx+uDtclUlNT2bt3b6Y/Z6vVSlRUlNM/5507d1KvXr1Mx+rXr5/nfpfcSW7AuLr55JNP0rBhQ3r27Mkvv/zijnC9Vn75vNyNVq1aUb9+fV5++WW2bdvm6XBc6uLFiwC3rFn0mbm3qSa9O6pJM1M9mj/qUVBN6ozq0dyTHz4vdyM/1aPgvpq0wB1Fl8ccOXKERYsWMXjwYKdjzpw5g6+vL8WKFct0PCgoKM+tVZBTDRo0oEmTJtx3330cO3aMd955h27duvHvf/8bHx8fT4fnMRmfi5tvCw8KCsoz606dOXOGkiVLZjpWoEABAgMDb/n3onnz5oSEhFC6dGn279/P5MmT+fXXX5k5c6arQ851586dIz093fTP2dkaH2fOnCE4ODjL+LzyuchwJ7mpUKEC48aNIywsjIsXLzJv3jzat2/PV199RZkyZdwRttcx+7wEBwdz6dIlrl69SsGCBT0UmeeVKlWKuLg4atSoQWpqKp999hmdO3fm008/pXr16p4OL9fZbDbGjRtH7dq1CQ0NdTouv/yOyYtUk94d1aRZqR7NH/UoqCZ1RvVo7lFNai6/1aPg3po0TzUtJ0+ezJw5c245ZtWqVVSqVMnx+NSpU/zjH//gmWeeoV27dq4O0SPuJC850axZM8f3GYvONm7c2HGl25u5Ojf3quzm5U49//zzju/DwsIoVaoUXbp04ejRo5QvX/6OX1fufZGRkURGRmZ63LRpU5YsWcLrr7/uucDEK1WsWJGKFSs6HteuXZtjx47x8ccf8/bbb3swMteIi4vjl19+4Z///KenQ5HbUE1qTjWpOdWj5lSPiqeoHpWcyG/1KLi3Js1TTctXXnmF6OjoW465//77Hd+fOnWKzp07ExkZyZgxY275vODgYNLS0khJScl0ZTs5OZlSpUrdXeAultO83K3777+fEiVKcOTIEa8uEMG1ucn4XCQnJ1O6dGnH8eTkZKpUqXJHr+ku2c1LcHAwZ8+ezXT8+vXrXLhwIUd/L2rVqgUYd5jca0ViiRIl8PHxybKQd3JycparShmCg4OzXF261fh71Z3k5ma+vr5UrVqVo0ePuiLEe4LZ5+XMmTMEBATk2yvatxIeHs727ds9HUauGz16NN999x2LFi267V0e+eV3jDdTTWpONak51aPmVI/mjGpSc6pHc49q0uzLq/UouL8mzVNNy5IlS2aZGuBMRnFYvXp1xo8fj9V66+U9a9Soga+vLxs3buTpp58GjB0fExMTiYiIuNvQXSoneckNv//+O+fPn/f6whlcm5v77ruPUqVKsXHjRqpWrQrApUuX+Pnnn3O8E6a7ZTcvkZGRpKSksGfPHmrUqAHApk2bsNls1KxZM9s/LyEhAeCe+MzczM/Pj+rVq7Nx40YaN24MGLfLb9y4kY4dO5o+JyIigk2bNtGlSxfHsQ0bNnj975KcupPc3Cw9PZ0DBw7QsGFDV4bq1SIiIvi///u/TMfy4uclt+zbt++e/F3ijN1uZ8yYMaxZs4aFCxdmq3GRX37HeDPVpOZUk5pTPWpO9WjOqCY1p3o096gmzb68Vo+C52rSfLkRz6lTp+jUqRNly5Zl8ODBnD17lqSkpExrnpw6dYpnnnnGsXhz0aJFad26NRMmTGDTpk3s2bOHYcOGERkZmaf+kiYmJpKQkEBiYiLp6ekkJCSQkJDA5cuXHWOeeeYZ1qxZA8Dly5eZOHEiO3fu5Pjx42zcuJHXXnuNBx54gAYNGnjqbbhETnNjsVjo3LkzH3zwAevWrWP//v0MGjSI0qVLO/6Hea+rVKkSDRo0YOTIkezatYtt27YxZswYmjVrxl/+8hcg69+lo0eP8t5777Fnzx6OHz/OunXrGDx4MI8++qjXX/F35uWXX+bTTz9l+fLlHDp0iNjYWP744w+ee+45AAYNGsSUKVMc4zt37swPP/zAvHnzOHToEDNmzGDPnj3ZLpzuJTnNzcyZM1m/fj3Hjh1j7969DBw4kMTERNq2beupt5DrLl++7Pj9AcYmCRm/WwCmTJnCoEGDHOPbt2/PsWPHmDRpEocOHWLx4sWsXr060//884qc5ubjjz9m7dq1HDlyhAMHDvDWW2+xadMmOnTo4JH4XSEuLo4VK1YwZcoUihQp4qhXrl696hiTn3/H3OtUkzqnmtSc6tGsVI/+j2pSc6pHzakmNad61JynatI8dadldv34448cOXKEI0eO8MQTT2Q6t3//fgDS0tL49ddfM+0gN2zYMKxWK3379iU1NZX69eszatQot8buatOnT2f58uWOx61atQLgk08+oU6dOgD8+uuvjp2ifHx8OHDgAPHx8Vy8eJHSpUvz+OOPExMTg5+fn9vjd6Wc5gagW7du/PHHH7z55pukpKTw8MMP89FHH+Hv7+/W2F1p8uTJjBkzhpdeegmr1cpTTz3FiBEjHOdv/ruUcXfIJ598wpUrVyhbtixPPfUUr732mqfewl1r2rQpZ8+eZfr06SQlJVG1alU++ugjx23vJ0+ezHTnTO3atZk8eTLTpk3jnXfe4cEHH+S999675SLG96qc5iYlJYWRI0eSlJREYGAg1atXZ8mSJVSuXNlTbyHX7dmzh86dOzsejx8/HoDo6GgmTJhAUlISJ0+edJy///77mT17NuPHj+eTTz6hTJkyjB07Nk/9IzxDTnOTlpbGxIkTOXXqFIUKFSI0NJT58+dTt25dt8fuKv/6178A6NSpU6bj48ePd/xjKz//jrnXqSZ1TjWpOdWj5lSPGlSTmlM9ak41qTnVo+Y8VZNa7Ha7PRfiFxEREREREREREckV+XJ6uIiIiIiIiIiIiHgvNS1FRERERERERETEq6hpKSIiIiIiIiIiIl5FTUsRERERERERERHxKmpaioiIiIiIiIiIiFdR01JERERERERERES8ipqWIiIiIiIiIiIi4lXUtBQRERERERERERGvoqaliLjF5s2bCQsLIyUlxdOh5EhYWBhr167Ntddr1KgRH3/8ca69nrsdP36csLAwEhISgHv3z1VERETyp3u1dlFNmplqUpH8oYCnAxCRe19YWNgtz/fu3ZvHHnvMTdHcmRkzZrB27Vq++OKLTMfXr19PYGCgh6LyrCFDhpCSksL777/vOFa2bFnWr19PiRIlPBiZiIiISFaqSfMm1aQi+ZealiJy19avX+/4ftWqVUyfPp2vv/7acaxw4cLs2bPHE6GRmpqKn5/fHT+/VKlSuRjNvc/Hx0c5EREREa+kmjT/UE0qkj9oeriI3LVSpUo5vooWLYrFYsl0rEiRIo6xe/fu5bnnnqNWrVq0b9+ew4cPZ3qttWvXEh0dTXh4OH/729+YOXMm169fd5xPTEykZ8+eREZGUrt2bWJiYjhz5ozj/IwZM2jZsiWfffYZjRo1ombNmgCkpKQwfPhw6tatS+3atencuTP79u0DYNmyZcycOZN9+/YRFhZGWFgYy5YtA7JOxfn999/p378/jz32GBERETz33HP8/PPPABw9epSePXsSFRVFZGQkrVu3ZsOGDTnKZXp6OuPHj+eRRx6hTp06TJo0icGDB/Paa685xphN52nZsiUzZsxwPJ4/fz4tWrQgIiKChg0bEhsby+XLlx3nly1bxiOPPMIPP/zA3//+dyIjI+natSunT5925HH58uWsW7fOkZPNmzdnmYpjZuvWrbz44ovUrFmThg0bMnbsWK5cueI4v3jxYp566inCw8OJioqib9++OcqRiIiIiBnVpKpJb6SaVOTep6aliLjV1KlTGTJkCJ9//jk+Pj4MGzbMcW7r1q0MHjyYzp07s2rVKkaPHs2yZcuYNWsWADabjddee40LFy6wcOFC5s+fz7Fjx+jXr1+mn3H06FG++eYbZs6cSXx8PAAxMTEkJyczZ84cli1bRvXq1XnppZc4f/48TZs25ZVXXuGhhx5i/fr1rF+/nqZNm2aJ/fLly3Ts2JFTp07x/vvv88UXX/CPf/wDm80GwJUrV2jYsCEff/wxy5cvp0GDBrz66qskJiZmOz/z5s1j+fLljBs3jn/+859cuHCBNWvW5DTNWCwWhg8fzpdffsmECRPYtGkTb7/9dqYxV69eZd68eUyaNIlFixZx8uRJJk6cCMArr7zC3//+dxo0aODISWRk5G1/7tGjR+nWrRtPPfUUK1asYOrUqWzbto0xY8YAsHv3bt566y369u3L119/zUcffcQjjzyS4/cnIiIicjdUk96aalIR8QaaHi4ibtWvXz/HWkLdu3ene/fuXLt2DX9/f2bOnEn37t2Jjo4G4P777ycmJoa3336b3r17s3HjRg4cOMC6desoW7YsAJMmTaJZs2bs2rXLcQU7LS2NSZMmUbJkScAoPHft2sXGjRsd03IGDx7M2rVr+eabb3j++ecpXLjwbaeZfPnll5w9e5alS5dSvHhxAB544AHH+SpVqlClShXH49dff521a9fy//7f/6Njx47Zys+CBQvo3r07Tz31FABxcXGZpjplV5cuXRzf33fffbz++uuMGjWK2NhYx/G0tDTi4uIoX748AB06dHCsFVSkSBEKFixIampqjqbezJ49mxYtWjh+/oMPPsjw4cPp1KkTsbGxnDx5kkKFCvHXv/6VgIAAypUrR7Vq1XL8/kRERETuhmrSW1NNKiLeQE1LEXGrGxdIzyg8kpOTCQkJYd++fWzfvt1xFRuMqSnXrl3jjz/+4NChQ5QpU8ZRHAJUrlyZYsWKcfjwYUeBGBIS4igOAfbv38+VK1eoU6dOpliuXr3K0aNHsx17QkIC1apVcxSHN7t8+TIzZ87ku+++IykpifT0dK5evZrtq9oXL14kKSmJWrVqOY4VKFCAGjVqYLfbsx0nwIYNG5g9ezaHDx/m0qVLmfJYqFAhAAoVKuQoDgFKly5NcnJyjn7Ozfbt28f+/ftZuXKl45jdbsdms3H8+HGioqIICQmhcePGNGjQgAYNGtCkSRNHTCIiIiLuoJrUOdWkIuIt1LQUEbcqUOB/v3YsFgtApqksffr0cVzRvZG/v3+2f8bNxcbly5cpVaoUCxcuzDK2aNGi2X7dggUL3vL8xIkT2bBhA4MHD6Z8+fIULFiQvn37kpaWlu2fkR0ZebvRjWssHT9+nB49evDCCy/Qr18/AgMD2bZtG8OHDyctLc2Rnxv/LDJeN6eF6M2uXLlC+/bt6dSpU5ZzZcuWxc/Pj+XLl7NlyxbWr1/P9OnTmTlzJkuXLqVYsWJ39bNFREREsks16d1TTSoirqampYh4jWrVqvHrr79mmt5yo0qVKvH7779z8uRJx5XtgwcPkpKSQqVKlZy+bvXq1Tlz5gw+Pj7cd999pmN8fX0dhaozYWFhfPbZZ5w/f970yvaOHTuIjo6mSZMmgFGYnjhx4paveaOiRYtSqlQpfv75Zx599FHAKPz27t2babpKyZIlHYuTA1y6dInjx487Hu/duxe73c6QIUOwWo2li1evXp3tODJkJyc3q1atGgcPHnT6ZwhGYRoVFUVUVBS9e/fm0UcfZdOmTab/MBARERFxN9WkqklFxDtoIx4R8Rq9evXiiy++YObMmfzyyy8cOnSIr776iqlTpwIQFRVFaGgob7zxBnv37mXXrl0MGjSIxx57jPDwcKevGxUVRUREBL169WL9+vUcP36c7du3M3XqVHbv3g1AuXLlOH78OAkJCZw9e5bU1NQsr9OsWTOCg4Pp1asX27Zt49ixY3zzzTfs2LEDMNYSWrNmDQkJCezbt48BAwbkuMDq3Lkzc+bMYe3atRw6dIi4uDhSUlIyjalbty4rVqxg69at7N+/n8GDBzsKwYw40tLSWLhwIceOHSM+Pp4lS5bkKI6MnOzfv5/Dhw9z9uzZbF2d79atGzt27GD06NEkJCTw22+/sXbtWkaPHg3At99+yyeffEJCQgInTpwgPj4em81GhQoVchyfiIiIiCuoJlVNKiLeQU1LEfEaDRo0YNasWaxfv542bdrQrl07Pv74Y8qVKwcYU0Xef/99ihUrRseOHenSpQv333+/o4B0xmKx8OGHH/Loo48ydOhQnnnmGfr378+JEycIDg4G4Omnn6ZBgwZ07tyZevXq8eWXX2Z5HT8/P+bNm0dQUBDdu3enRYsWfPjhh/j4+AAwZMgQihUrRvv27Xn11Vdp0KAB1atXz1EOXnnlFZ599lkGDx5M+/btKVKkiOMqeYYePXrw6KOP0qNHD3r06EHjxo0zrQNUpUoVhg4dypw5c2jevDkrV66kf//+OYoDoF27dlSoUIHWrVtTr149tm/fftvnVKlShYULF/Lbb7/x4osvEh0dzfTp0yldujRgXLlfs2YNL730Ek2bNmXJkiVMmTKFhx56KMfxiYiIiLiCalLVpCLiHSz2u10sQkREXGrIkCGkpKQ4dlEUEREREXE31aQi4m6601JERERERERERES8ipqWIiIiIiIiIiIi4lU0PVxERERERERERES8iu60FBEREREREREREa+ipqWIiIiIiIiIiIh4FTUtRURERERERERExKuoaSkiIiIiIiIiIiJeRU1LERERERERERER8SpqWoqIiIiIiIiIiIhXUdNSREREREREREREvIqaliIiIiIiIiIiIuJV/j8Y8841KxQcJgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performaing Normality Tests\n",
            "\n",
            "lstm_rmse: \n",
            "NormaltestResult(statistic=10.987483624705483, pvalue=0.0041124274172276605)\n",
            "gru_rmse:\n",
            "NormaltestResult(statistic=3.485635068554623, pvalue=0.17502656190233887)\n",
            "\n",
            " Two-sample ttest between lstm_rmse and  gru_rmse\n",
            "TtestResult(statistic=-3.129449946337364, pvalue=0.003382108899154282, df=37.52785849516695)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "comparative_qq_plots(best_lstm_rmse.iloc[:,1], best_gru_rmse.iloc[:,1])\n",
        "perform_normality_test(best_lstm_rmse.iloc[:,1], best_gru_rmse.iloc[:,1])\n",
        "perform_pairwise_ttests(best_lstm_rmse.iloc[:,1], best_gru_rmse.iloc[:,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqqUdJfPjU1p"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}